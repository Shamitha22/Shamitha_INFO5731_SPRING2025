Title,Abstract,Cleaned Abstract,Year,Citation Count
Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms,"We present Fashion-MNIST, a new dataset comprising of 28x28 grayscale images of 70,000 fashion products from 10 categories, with 7,000 images per category. The training set has 60,000 images and the test set has 10,000 images. Fashion-MNIST is intended to serve as a direct drop-in replacement for the original MNIST dataset for benchmarking machine learning algorithms, as it shares the same image size, data format and the structure of training and testing splits. The dataset is freely available at this https URL",present fashionmnist new dataset compris x grayscal imag fashion product categori imag per categori train set imag test set imag fashionmnist intend serv direct dropin replac origin mnist dataset benchmark machin learn algorithm share imag size data format structur train test split dataset freeli avail http url,2017,8225
TensorFlow: A system for large-scale machine learning,"TensorFlow is a machine learning system that operates at large scale and in heterogeneous environments. Tensor-Flow uses dataflow graphs to represent computation, shared state, and the operations that mutate that state. It maps the nodes of a dataflow graph across many machines in a cluster, and within a machine across multiple computational devices, including multicore CPUs, general-purpose GPUs, and custom-designed ASICs known as Tensor Processing Units (TPUs). This architecture gives flexibility to the application developer: whereas in previous ""parameter server"" designs the management of shared state is built into the system, TensorFlow enables developers to experiment with novel optimizations and training algorithms. TensorFlow supports a variety of applications, with a focus on training and inference on deep neural networks. Several Google services use TensorFlow in production, we have released it as an open-source project, and it has become widely used for machine learning research. In this paper, we describe the TensorFlow dataflow model and demonstrate the compelling performance that TensorFlow achieves for several real-world applications.",tensorflow machin learn system oper larg scale heterogen environ tensorflow use dataflow graph repres comput share state oper mutat state map node dataflow graph across mani machin cluster within machin across multipl comput devic includ multicor cpu generalpurpos gpu customdesign asic known tensor process unit tpu architectur give flexibl applic develop wherea previou paramet server design manag share state built system tensorflow enabl develop experi novel optim train algorithm tensorflow support varieti applic focu train infer deep neural network sever googl servic use tensorflow product releas opensourc project becom wide use machin learn research paper describ tensorflow dataflow model demonstr compel perform tensorflow achiev sever realworld applic,2016,17847
TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems,"TensorFlow is an interface for expressing machine learning algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as GPU cards. The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields, including speech recognition, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery. This paper describes the TensorFlow interface and an implementation of that interface that we have built at Google. The TensorFlow API and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at www.tensorflow.org.",tensorflow interfac express machin learn algorithm implement execut algorithm comput express use tensorflow execut littl chang wide varieti heterogen system rang mobil devic phone tablet largescal distribut system hundr machin thousand comput devic gpu card system flexibl use express wide varieti algorithm includ train infer algorithm deep neural network model use conduct research deploy machin learn system product across dozen area comput scienc field includ speech recognit comput vision robot inform retriev natur languag process geograph inform extract comput drug discoveri paper describ tensorflow interfac implement interfac built googl tensorflow api refer implement releas opensourc packag apach licens novemb avail wwwtensorfloworg,2016,10921
Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead,,,2018,5488
An Introduction to Machine Learning,,,2017,4064
Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting,"The goal of precipitation nowcasting is to predict the future rainfall intensity in a local region over a relatively short period of time. Very few previous studies have examined this crucial and challenging weather forecasting problem from the machine learning perspective. In this paper, we formulate precipitation nowcasting as a spatiotemporal sequence forecasting problem in which both the input and the prediction target are spatiotemporal sequences. By extending the fully connected LSTM (FC-LSTM) to have convolutional structures in both the input-to-state and state-to-state transitions, we propose the convolutional LSTM (ConvLSTM) and use it to build an end-to-end trainable model for the precipitation nowcasting problem. Experiments show that our ConvLSTM network captures spatiotemporal correlations better and consistently outperforms FC-LSTM and the state-of-the-art operational ROVER algorithm for precipitation nowcasting.",goal precipit nowcast predict futur rainfal intens local region rel short period time previou studi examin crucial challeng weather forecast problem machin learn perspect paper formul precipit nowcast spatiotempor sequenc forecast problem input predict target spatiotempor sequenc extend fulli connect lstm fclstm convolut structur inputtost statetost transit propos convolut lstm convlstm use build endtoend trainabl model precipit nowcast problem experi show convlstm network captur spatiotempor correl better consist outperform fclstm stateoftheart oper rover algorithm precipit nowcast,2015,7526
Pattern Recognition And Machine Learning,,,2016,7546
Open Graph Benchmark: Datasets for Machine Learning on Graphs,"We present the Open Graph Benchmark (OGB), a diverse set of challenging and realistic benchmark datasets to facilitate scalable, robust, and reproducible graph machine learning (ML) research. OGB datasets are large-scale (up to 100+ million nodes and 1+ billion edges), encompass multiple important graph ML tasks, and cover a diverse range of domains, ranging from social and information networks to biological networks, molecular graphs, source code ASTs, and knowledge graphs. For each dataset, we provide a unified evaluation protocol using meaningful application-specific data splits and evaluation metrics. In addition to building the datasets, we also perform extensive benchmark experiments for each dataset. Our experiments suggest that OGB datasets present significant challenges of scalability to large-scale graphs and out-of-distribution generalization under realistic data splits, indicating fruitful opportunities for future research. Finally, OGB provides an automated end-to-end graph ML pipeline that simplifies and standardizes the process of graph data loading, experimental setup, and model evaluation. OGB will be regularly updated and welcomes inputs from the community. OGB datasets as well as data loaders, evaluation scripts, baseline code, and leaderboards are publicly available at this https URL .",present open graph benchmark ogb diver set challeng realist benchmark dataset facilit scalabl robust reproduc graph machin learn ml research ogb dataset largescal million node billion edg encompass multipl import graph ml task cover diver rang domain rang social inform network biolog network molecular graph sourc code ast knowledg graph dataset provid unifi evalu protocol use meaning applicationspecif data split evalu metric addit build dataset also perform extens benchmark experi dataset experi suggest ogb dataset present signific challeng scalabl largescal graph outofdistribut gener realist data split indic fruit opportun futur research final ogb provid autom endtoend graph ml pipelin simplifi standard process graph data load experiment setup model evalu ogb regularli updat welcom input commun ogb dataset well data loader evalu script baselin code leaderboard publicli avail http url,2020,2449
A Survey on Bias and Fairness in Machine Learning,"With the widespread use of artificial intelligence (AI) systems and applications in our everyday lives, accounting for fairness has gained significant importance in designing and engineering of such systems. AI systems can be used in many sensitive environments to make important and life-changing decisions; thus, it is crucial to ensure that these decisions do not reflect discriminatory behavior toward certain groups or populations. More recently some work has been developed in traditional machine learning and deep learning that address such challenges in different subdomains. With the commercialization of these systems, researchers are becoming more aware of the biases that these applications can contain and are attempting to address them. In this survey, we investigated different real-world applications that have shown biases in various ways, and we listed different sources of biases that can affect AI applications. We then created a taxonomy for fairness definitions that machine learning researchers have defined to avoid the existing bias in AI systems. In addition to that, we examined different domains and subdomains in AI showing what researchers have observed with regard to unfair outcomes in the state-of-the-art methods and ways they have tried to address them. There are still many future directions and solutions that can be taken to mitigate the problem of bias in AI systems. We are hoping that this survey will motivate researchers to tackle these issues in the near future by observing existing work in their respective fields.",widespread use artifici intellig ai system applic everyday live account fair gain signific import design engin system ai system use mani sensit environ make import lifechang decis thu crucial ensur decis reflect discriminatori behavior toward certain group popul recent work develop tradit machin learn deep learn address challeng differ subdomain commerci system research becom awar bias applic contain attempt address survey investig differ realworld applic shown bias variou way list differ sourc bias affect ai applic creat taxonomi fair definit machin learn research defin avoid exist bia ai system addit examin differ domain subdomain ai show research observ regard unfair outcom stateoftheart method way tri address still mani futur direct solut taken mitig problem bia ai system hope survey motiv research tackl issu near futur observ exist work respect field,2019,3783
Physics-informed machine learning,,,2021,3045
Membership Inference Attacks Against Machine Learning Models,"We quantitatively investigate how machine learning models leak information about the individual data records on which they were trained. We focus on the basic membership inference attack: given a data record and black-box access to a model, determine if the record was in the model's training dataset. To perform membership inference against a target model, we make adversarial use of machine learning and train our own inference model to recognize differences in the target model's predictions on the inputs that it trained on versus the inputs that it did not train on. We empirically evaluate our inference techniques on classification models trained by commercial ""machine learning as a service"" providers such as Google and Amazon. Using realistic datasets and classification tasks, including a hospital discharge dataset whose membership is sensitive from the privacy perspective, we show that these models can be vulnerable to membership inference attacks. We then investigate the factors that influence this leakage and evaluate mitigation strategies.",quantit investig machin learn model leak inform individu data record train focu basic membership infer attack given data record blackbox access model determin record model train dataset perform membership infer target model make adversari use machin learn train infer model recogn differ target model predict input train versu input train empir evalu infer techniqu classif model train commerci machin learn servic provid googl amazon use realist dataset classif task includ hospit discharg dataset whose membership sensit privaci perspect show model vulner membership infer attack investig factor influenc leakag evalu mitig strategi,2016,3771
Foundations of Machine Learning,,,2021,3011
"Machine Learning: Algorithms, Real-World Applications and Research Directions",,,2021,2483
Scikit-learn: Machine Learning in Python,"Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net.",scikitlearn python modul integr wide rang stateoftheart machin learn algorithm mediumscal supervis unsupervis problem packag focus bring machin learn nonspecialist use generalpurpos highlevel languag emphasi put ea use perform document api consist minim depend distribut simplifi bsd licens encourag use academ commerci set sourc code binari document download httpscikitlearnsourceforgenet,2011,72412
"Machine learning: Trends, perspectives, and prospects","Machine learning addresses the question of how to build computers that improve automatically through experience. It is one of today’s most rapidly growing technical fields, lying at the intersection of computer science and statistics, and at the core of artificial intelligence and data science. Recent progress in machine learning has been driven both by the development of new learning algorithms and theory and by the ongoing explosion in the availability of online data and low-cost computation. The adoption of data-intensive machine-learning methods can be found throughout science, technology and commerce, leading to more evidence-based decision-making across many walks of life, including health care, manufacturing, education, financial modeling, policing, and marketing.",machin learn address question build comput improv automat experi one today rapidli grow technic field lie intersect comput scienc statist core artifici intellig data scienc recent progress machin learn driven develop new learn algorithm theori ongo explos avail onlin data lowcost comput adopt dataintens machinelearn method found throughout scienc technolog commerc lead evidencebas decisionmak across mani walk life includ health care manufactur educ financi model polic market,2015,6278
Towards A Rigorous Science of Interpretable Machine Learning,"As machine learning systems become ubiquitous, there has been a surge of interest in interpretable machine learning: systems that provide explanation for their outputs. These explanations are often used to qualitatively assess other criteria such as safety or non-discrimination. However, despite the interest in interpretability, there is very little consensus on what interpretable machine learning is and how it should be measured. In this position paper, we first define interpretability and describe when interpretability is needed (and when it is not). Next, we suggest a taxonomy for rigorous evaluation and expose open questions towards a more rigorous science of interpretable machine learning.",machin learn system becom ubiquit surg interest interpret machin learn system provid explan output explan often use qualit assess criterion safeti nondiscrimin howev despit interest interpret littl consensu interpret machin learn measur posit paper first defin interpret describ interpret need next suggest taxonomi rigor evalu expo open question toward rigor scienc interpret machin learn,2017,3416
C4.5: Programs for Machine Learning,"From the Publisher: 
Classifier systems play a major role in machine learning and knowledge-based systems, and Ross Quinlan's work on ID3 and C4.5 is widely acknowledged to have made some of the most significant contributions to their development. This book is a complete guide to the C4.5 system as implemented in C for the UNIX environment. It contains a comprehensive guide to the system's use , the source code (about 8,800 lines), and implementation notes. The source code and sample datasets are also available on a 3.5-inch floppy diskette for a Sun workstation. 
 
C4.5 starts with large sets of cases belonging to known classes. The cases, described by any mixture of nominal and numeric properties, are scrutinized for patterns that allow the classes to be reliably discriminated. These patterns are then expressed as models, in the form of decision trees or sets of if-then rules, that can be used to classify new cases, with emphasis on making the models understandable as well as accurate. The system has been applied successfully to tasks involving tens of thousands of cases described by hundreds of properties. The book starts from simple core learning methods and shows how they can be elaborated and extended to deal with typical problems such as missing data and over hitting. Advantages and disadvantages of the C4.5 approach are discussed and illustrated with several case studies. 
 
This book and software should be of interest to developers of classification-based intelligent systems and to students in machine learning and expert systems courses.",publish classifi system play major role machin learn knowledgebas system ross quinlan work id c wide acknowledg made signific contribut develop book complet guid c system implement c unix environ contain comprehens guid system use sourc code line implement note sourc code sampl dataset also avail inch floppi diskett sun workstat c start larg set case belong known class case describ mixtur nomin numer properti scrutin pattern allow class reliabl discrimin pattern express model form decis tree set ifthen rule use classifi new case emphasi make model understand well accur system appli success task involv ten thousand case describ hundr properti book start simpl core learn method show elabor extend deal typic problem miss data hit advantag disadvantag c approach discus illustr sever case studi book softwar interest develop classificationbas intellig system student machin learn expert system cours,1992,23968
Data Mining Practical Machine Learning Tools and Techniques,,,2014,17214
Machine learning - a probabilistic perspective,"All rights reserved. No part of this book may be reproduced in any form by any electronic or mechanical means (including photocopying, recording, or information storage and retrieval) without permission in writing from the publisher. Machine learning : a probabilistic perspective / Kevin P. Murphy. p. cm. — (Adaptive computation and machine learning series) Includes bibliographical references and index. Contents Preface xxvii 1 Introduction 1 1.1 Machine learning: what and why? 1 1.1.1 Types of machine learning 2 1.2 Supervised learning 3 1.2.1 Classification 3 1.2.2 Regression 8 1.3 Unsupervised learning 9 1.3.1 Discovering clusters 10 1.3.2 Discovering latent factors 11 1.3.3 Discovering graph structure 13 1.3.4 Matrix completion 14 1.4 Some basic concepts in machine learning 16 1.4.1 Parametric vs non-parametric models 16 1.4.2 A simple non-parametric classifier: K-nearest neighbors 16 1.4.3 The curse of dimensionality 18 1.4.4 Parametric models for classification and regression 19 1.4.5",right reserv part book may reproduc form electron mechan mean includ photocopi record inform storag retriev without permiss write publish machin learn probabilist perspect kevin p murphi p cm adapt comput machin learn seri includ bibliograph refer index content prefac xxvii introduct machin learn type machin learn supervis learn classif regress unsupervis learn discov cluster discov latent factor discov graph structur matrix complet basic concept machin learn parametr v nonparametr model simpl nonparametr classifi knearest neighbor cur dimension parametr model classif regress,2012,9242
Genetic Algorithms in Search Optimization and Machine Learning,"From the Publisher: 
This book brings together - in an informal and tutorial fashion - the computer techniques, mathematical tools, and research results that will enable both students and practitioners to apply genetic algorithms to problems in many fields. 
 
Major concepts are illustrated with running examples, and major algorithms are illustrated by Pascal computer programs. No prior knowledge of GAs or genetics is assumed, and only a minimum of computer programming and mathematics background is required.",publish book bring togeth inform tutori fashion comput techniqu mathemat tool research result enabl student practition appli genet algorithm problem mani field major concept illustr run exampl major algorithm illustr pascal comput program prior knowledg ga genet assum minimum comput program mathemat background requir,1988,60314
Pattern Recognition and Machine Learning,Probability Distributions.- Linear Models for Regression.- Linear Models for Classification.- Neural Networks.- Kernel Methods.- Sparse Kernel Machines.- Graphical Models.- Mixture Models and EM.- Approximate Inference.- Sampling Methods.- Continuous Latent Variables.- Sequential Data.- Combining Models.,probabl distribut linear model regress linear model classif neural network kernel method spar kernel machin graphic model mixtur model em approxim infer sampl method continu latent variabl sequenti data combin model,2006,36731
Interpretable Machine Learning,"Interpretable machine learning has become a popular research direction as deep neural networks (DNNs) have become more powerful and their applications more mainstream, yet DNNs remain difficult to understand. Testing with Concept Activation Vectors, TCAV, (Kim et al. 2017) is an approach to interpreting DNNs in a human-friendly way and has recently received significant attention in the machine learning community. The TCAV algorithm achieves a degree of global interpretability for DNNs through human-defined concepts as explanations. This project introduces Robust TCAV, which builds on TCAV and experimentally determines best practices for this method. The objectives for Robust TCAV are 1) Making TCAV more consistent by reducing variance in the TCAV score distribution and 2) Increasing CAV and TCAV score resistance to perturbations. A difference of means method for CAV generation was determined to be the best practice to achieve both objectives. Many areas of the TCAV process are explored including CAV visualization in low dimensions, negative class selection, and activation perturbation in the direction of a CAV. Finally, a thresholding technique is considered to remove noise in TCAV scores. This project is a step in the direction of making TCAV, an already impactful algorithm in interpretability, more reliable and useful for practitioners.",interpret machin learn becom popular research direct deep neural network dnn becom power applic mainstream yet dnn remain difficult understand test concept activ vector tcav kim et al approach interpret dnn humanfriendli way recent receiv signific attent machin learn commun tcav algorithm achiev degre global interpret dnn humandefin concept explan project introduc robust tcav build tcav experiment determin best practic method object robust tcav make tcav consist reduc varianc tcav score distribut increas cav tcav score resist perturb differ mean method cav gener determin best practic achiev object mani area tcav process explor includ cav visual low dimens neg class select activ perturb direct cav final threshold techniqu consid remov nois tcav score project step direct make tcav alreadi impact algorithm interpret reliabl use practition,2019,2418
Practical Black-Box Attacks against Machine Learning,"Machine learning (ML) models, e.g., deep neural networks (DNNs), are vulnerable to adversarial examples: malicious inputs modified to yield erroneous model outputs, while appearing unmodified to human observers. Potential attacks include having malicious content like malware identified as legitimate or controlling vehicle behavior. Yet, all existing adversarial example attacks require knowledge of either the model internals or its training data. We introduce the first practical demonstration of an attacker controlling a remotely hosted DNN with no such knowledge. Indeed, the only capability of our black-box adversary is to observe labels given by the DNN to chosen inputs. Our attack strategy consists in training a local model to substitute for the target DNN, using inputs synthetically generated by an adversary and labeled by the target DNN. We use the local substitute to craft adversarial examples, and find that they are misclassified by the targeted DNN. To perform a real-world and properly-blinded evaluation, we attack a DNN hosted by MetaMind, an online deep learning API. We find that their DNN misclassifies 84.24% of the adversarial examples crafted with our substitute. We demonstrate the general applicability of our strategy to many ML techniques by conducting the same attack against models hosted by Amazon and Google, using logistic regression substitutes. They yield adversarial examples misclassified by Amazon and Google at rates of 96.19% and 88.94%. We also find that this black-box attack strategy is capable of evading defense strategies previously found to make adversarial example crafting harder.",machin learn ml model eg deep neural network dnn vulner adversari exampl malici input modifi yield erron model output appear unmodifi human observ potenti attack includ malici content like malwar identifi legitim control vehicl behavior yet exist adversari exampl attack requir knowledg either model intern train data introduc first practic demonstr attack control remot host dnn knowledg inde capabl blackbox adversari observ label given dnn chosen input attack strategi consist train local model substitut target dnn use input synthet gener adversari label target dnn use local substitut craft adversari exampl find misclassifi target dnn perform realworld properlyblind evalu attack dnn host metamind onlin deep learn api find dnn misclassifi adversari exampl craft substitut demonstr gener applic strategi mani ml techniqu conduct attack model host amazon googl use logist regress substitut yield adversari exampl misclassifi amazon googl rate also find blackbox attack strategi capabl evad defens strategi previous found make adversari exampl craft harder,2016,3533
Practical Bayesian Optimization of Machine Learning Algorithms,"The use of machine learning algorithms frequently involves careful tuning of learning parameters and model hyperparameters. Unfortunately, this tuning is often a ""black art"" requiring expert experience, rules of thumb, or sometimes brute-force search. There is therefore great appeal for automatic approaches that can optimize the performance of any given learning algorithm to the problem at hand. In this work, we consider this problem through the framework of Bayesian optimization, in which a learning algorithm's generalization performance is modeled as a sample from a Gaussian process (GP). We show that certain choices for the nature of the GP, such as the type of kernel and the treatment of its hyperparameters, can play a crucial role in obtaining a good optimizer that can achieve expertlevel performance. We describe new algorithms that take into account the variable cost (duration) of learning algorithm experiments and that can leverage the presence of multiple cores for parallel experimentation. We show that these proposed algorithms improve on previous automatic procedures and can reach or surpass human expert-level optimization for many algorithms including latent Dirichlet allocation, structured SVMs and convolutional neural networks.",use machin learn algorithm frequent involv care tune learn paramet model hyperparamet unfortun tune often black art requir expert experi rule thumb sometim bruteforc search therefor great appeal automat approach optim perform given learn algorithm problem hand work consid problem framework bayesian optim learn algorithm gener perform model sampl gaussian process gp show certain choic natur gp type kernel treatment hyperparamet play crucial role obtain good optim achiev expertlevel perform describ new algorithm take account variabl cost durat learn algorithm experi leverag presenc multipl core parallel experiment show propos algorithm improv previou automat procedur reach surpass human expertlevel optim mani algorithm includ latent dirichlet alloc structur svm convolut neural network,2012,7520
Machine Learning Algorithms: A Review,.,,2022,1423
Thumbs up? Sentiment Classification using Machine Learning Techniques,"We consider the problem of classifying documents not by topic, but by overall sentiment, e.g., determining whether a review is positive or negative. Using movie reviews as data, we find that standard machine learning techniques definitively outperform human-produced baselines. However, the three machine learning methods we employed (Naive Bayes, maximum entropy classification, and support vector machines) do not perform as well on sentiment classification as on traditional topic-based categorization. We conclude by examining factors that make the sentiment classification problem more challenging.",consid problem classifi document topic overal sentiment eg determin whether review posit neg use movi review data find standard machin learn techniqu definit outperform humanproduc baselin howev three machin learn method employ naiv bay maximum entropi classif support vector machin perform well sentiment classif tradit topicbas categor conclud examin factor make sentiment classif problem challeng,2002,9245
UCI Repository of machine learning databases,,,1998,14257
The MNIST Database of Handwritten Digit Images for Machine Learning Research [Best of the Web],"In this issue, “Best of the Web” presents the modified National Institute of Standards and Technology (MNIST) resources, consisting of a collection of handwritten digit images used extensively in optical character recognition and machine learning research.",issu best web present modifi nation institut standard technolog mnist resourc consist collect handwritten digit imag use extens optic charact recognit machin learn research,2012,4074
Machine learning for molecular and materials science,,,2018,2737
Programs for Machine Learning,"Algorithms for constructing decision trees are among the most well known and widely used of all machine learning methods. Among decision tree algorithms, J. Ross Quinlan's ID3 and its successor, C4.5, are probably the most popular in the machine learning community. These algorithms and variations on them have been the subject of numerous research papers since Quinlan introduced ID3. Until recently, most researchers looking for an introduction to decision trees turned to Quinlan's seminal 1986 Machine Learning journal article [Quinlan, 1986]. In his new book, C4.5: Programs for Machine Learning, Quinlan has put together a definitive, much needed description of his complete system, including the latest developments. As such, this book will be a welcome addition to the library of many researchers and students.",algorithm construct decis tree among well known wide use machin learn method among decis tree algorithm j ross quinlan id successor c probabl popular machin learn commun algorithm variat subject numer research paper sinc quinlan introduc id recent research look introduct decis tree turn quinlan semin machin learn journal articl quinlan new book c program machin learn quinlan put togeth definit much need descript complet system includ latest develop book welcom addit librari mani research student,1994,9091
"Hands-On Machine Learning with Scikit-Learn and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems","Through a series of recent breakthroughs, deep learning has boosted the entire field of machine learning. Now, even programmers who know close to nothing about this technology can use simple, efficient tools to implement programs capable of learning from data. This practical book shows you how. By using concrete examples, minimal theory, and two production-ready Python frameworks-scikit-learn and TensorFlow-author Aurelien Geron helps you gain an intuitive understanding of the concepts and tools for building intelligent systems. You'll learn a range of techniques, starting with simple linear regression and progressing to deep neural networks. With exercises in each chapter to help you apply what you've learned, all you need is programming experience to get started. Explore the machine learning landscape, particularly neural nets Use scikit-learn to track an example machine-learning project end-to-end Explore several training models, including support vector machines, decision trees, random forests, and ensemble methods Use the TensorFlow library to build and train neural nets Dive into neural net architectures, including convolutional nets, recurrent nets, and deep reinforcement learning Learn techniques for training and scaling deep neural nets Apply practical code examples without acquiring excessive machine learning theory or algorithm details",seri recent breakthrough deep learn boost entir field machin learn even programm know close noth technolog use simpl effici tool implement program capabl learn data practic book show use concret exampl minim theori two productionreadi python frameworksscikitlearn tensorflowauthor aurelien geron help gain intuit understand concept tool build intellig system youll learn rang techniqu start simpl linear regress progress deep neural network exercis chapter help appli youv learn need program experi get start explor machin learn landscap particularli neural net use scikitlearn track exampl machinelearn project endtoend explor sever train model includ support vector machin decis tree random forest ensembl method use tensorflow librari build train neural net dive neural net architectur includ convolut net recurr net deep reinforc learn learn techniqu train scale deep neural net appli practic code exampl without acquir excess machin learn theori algorithm detail,2017,2733
Machine learning in automated text categorization,"The automated categorization (or classification) of texts into predefined categories has witnessed a booming interest in the last 10 years, due to the increased availability of documents in digital form and the ensuing need to organize them. In the research community the dominant approach to this problem is based on machine learning techniques: a general inductive process automatically builds a classifier by learning, from a set of preclassified documents, the characteristics of the categories. The advantages of this approach over the knowledge engineering approach (consisting in the manual definition of a classifier by domain experts) are a very good effectiveness, considerable savings in terms of expert labor power, and straightforward portability to different domains. This survey discusses the main approaches to text categorization that fall within the machine learning paradigm. We will discuss in detail issues pertaining to three different problems, namely, document representation, classifier construction, and classifier evaluation.",autom categor classif text predefin categori wit boom interest last year due increas avail document digit form ensu need organ research commun domin approach problem base machin learn techniqu gener induct process automat build classifi learn set preclassifi document characterist categori advantag approach knowledg engin approach consist manual definit classifi domain expert good effect consider save term expert labor power straightforward portabl differ domain survey discus main approach text categor fall within machin learn paradigm discus detail issu pertain three differ problem name document represent classifi construct classifi evalu,2001,8829
Adversarial Machine Learning at Scale,"Adversarial examples are malicious inputs designed to fool machine learning models. They often transfer from one model to another, allowing attackers to mount black box attacks without knowledge of the target model's parameters. Adversarial training is the process of explicitly training a model on adversarial examples, in order to make it more robust to attack or to reduce its test error on clean inputs. So far, adversarial training has primarily been applied to small problems. In this research, we apply adversarial training to ImageNet. Our contributions include: (1) recommendations for how to succesfully scale adversarial training to large models and datasets, (2) the observation that adversarial training confers robustness to single-step attack methods, (3) the finding that multi-step attack methods are somewhat less transferable than single-step attack methods, so single-step attacks are the best for mounting black-box attacks, and (4) resolution of a ""label leaking"" effect that causes adversarially trained models to perform better on adversarial examples than on clean examples, because the adversarial example construction process uses the true label and the model can learn to exploit regularities in the construction process.",adversari exampl malici input design fool machin learn model often transfer one model anoth allow attack mount black box attack without knowledg target model paramet adversari train process explicitli train model adversari exampl order make robust attack reduc test error clean input far adversari train primarili appli small problem research appli adversari train imagenet contribut includ recommend succes scale adversari train larg model dataset observ adversari train confer robust singlestep attack method find multistep attack method somewhat less transfer singlestep attack method singlestep attack best mount blackbox attack resolut label leak effect caus adversari train model perform better adversari exampl clean exampl adversari exampl construct process use true label model learn exploit regular construct process,2016,3006
Large-Scale Machine Learning with Stochastic Gradient Descent,,,2010,5682
Small data machine learning in materials science,,,2023,211
Probabilistic Graphical Models: Principles and Techniques - Adaptive Computation and Machine Learning,"Most tasks require a person or an automated system to reasonto reach conclusions based on available information. The framework of probabilistic graphical models, presented in this book, provides a general approach for this task. The approach is model-based, allowing interpretable models to be constructed and then manipulated by reasoning algorithms. These models can also be learned automatically from data, allowing the approach to be used in cases where manually constructing a model is difficult or even impossible. Because uncertainty is an inescapable aspect of most real-world applications, the book focuses on probabilistic models, which make the uncertainty explicit and provide models that are more faithful to reality. Probabilistic Graphical Models discusses a variety of models, spanning Bayesian networks, undirected Markov networks, discrete and continuous models, and extensions to deal with dynamical systems and relational data. For each class of models, the text describes the three fundamental cornerstones: representation, inference, and learning, presenting both basic concepts and advanced techniques. Finally, the book considers the use of the proposed framework for causal reasoning and decision making under uncertainty. The main text in each chapter provides the detailed technical development of the key ideas. Most chapters also include boxes with additional material: skill boxes, which describe techniques; case study boxes, which discuss empirical cases related to the approach described in the text, including applications in computer vision, robotics, natural language understanding, and computational biology; and concept boxes, which present significant concepts drawn from the material in the chapter. Instructors (and readers) can group chapters in various combinations, from core topics to more technically advanced material, to suit their particular needs. Adaptive Computation and Machine Learning series",task requir person autom system reasonto reach conclus base avail inform framework probabilist graphic model present book provid gener approach task approach modelbas allow interpret model construct manipul reason algorithm model also learn automat data allow approach use case manual construct model difficult even imposs uncertainti inescap aspect realworld applic book focus probabilist model make uncertainti explicit provid model faith realiti probabilist graphic model discus varieti model span bayesian network undirect markov network discret continu model extens deal dynam system relat data class model text describ three fundament cornerston represent infer learn present basic concept advanc techniqu final book consid use propos framework causal reason decis make uncertainti main text chapter provid detail technic develop key idea chapter also includ box addit materi skill box describ techniqu case studi box discus empir case relat approach describ text includ applic comput vision robot natur languag understand comput biolog concept box present signific concept drawn materi chapter instructor reader group chapter variou combin core topic technic advanc materi suit particular need adapt comput machin learn seri,2009,7621
Optimization Methods for Large-Scale Machine Learning,"This paper provides a review and commentary on the past, present, and future of numerical optimization algorithms in the context of machine learning applications. Through case studies on text classification and the training of deep neural networks, we discuss how optimization problems arise in machine learning and what makes them challenging. A major theme of our study is that large-scale machine learning represents a distinctive setting in which the stochastic gradient (SG) method has traditionally played a central role while conventional gradient-based nonlinear optimization techniques typically falter. Based on this viewpoint, we present a comprehensive theory of a straightforward, yet versatile SG algorithm, discuss its practical behavior, and highlight opportunities for designing algorithms with improved performance. This leads to a discussion about the next generation of optimization methods for large-scale machine learning, including an investigation of two main streams of research on techniques that diminish noise in the stochastic directions and methods that make use of second-order derivative approximations.",paper provid review commentari past present futur numer optim algorithm context machin learn applic case studi text classif train deep neural network discus optim problem aris machin learn make challeng major theme studi largescal machin learn repres distinct set stochast gradient sg method tradit play central role convent gradientbas nonlinear optim techniqu typic falter base viewpoint present comprehens theori straightforward yet versatil sg algorithm discus practic behavior highlight opportun design algorithm improv perform lead discus next gener optim method largescal machin learn includ investig two main stream research techniqu diminish nois stochast direct method make use secondord deriv approxim,2016,3025
Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation,"In this paper, we propose a novel neural network model called RNN Encoder‐ Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixedlength vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder‐Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.",paper propos novel neural network model call rnn encod decod consist two recurr neural network rnn one rnn encod sequenc symbol fixedlength vector represent decod represent anoth sequenc symbol encod decod propos model jointli train maxim condit probabl target sequenc given sourc sequenc perform statist machin translat system empir found improv use condit probabl phrase pair comput rnn encoderdecod addit featur exist loglinear model qualit show propos model learn semant syntact meaning represent linguist phrase,2014,22292
Neural Machine Translation by Jointly Learning to Align and Translate,"Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.",neural machin translat recent propos approach machin translat unlik tradit statist machin translat neural machin translat aim build singl neural network jointli tune maxim translat perform model propos recent neural machin translat often belong famili encoderdecod consist encod encod sourc sentenc fixedlength vector decod gener translat paper conjectur use fixedlength vector bottleneck improv perform basic encoderdecod architectur propos extend allow model automat softsearch part sourc sentenc relev predict target word without form part hard segment explicitli new approach achiev translat perform compar exist stateoftheart phrasebas system task englishtofrench translat furthermor qualit analysi reveal softalign found model agre well intuit,2014,26478
Machine Learning for High-Speed Corner Detection,,,2006,4746
Explainable AI: A Review of Machine Learning Interpretability Methods,"Recent advances in artificial intelligence (AI) have led to its widespread industrial adoption, with machine learning systems demonstrating superhuman performance in a significant number of tasks. However, this surge in performance, has often been achieved through increased model complexity, turning such systems into “black box” approaches and causing uncertainty regarding the way they operate and, ultimately, the way that they come to decisions. This ambiguity has made it problematic for machine learning systems to be adopted in sensitive yet critical domains, where their value could be immense, such as healthcare. As a result, scientific interest in the field of Explainable Artificial Intelligence (XAI), a field that is concerned with the development of new methods that explain and interpret machine learning models, has been tremendously reignited over recent years. This study focuses on machine learning interpretability methods; more specifically, a literature review and taxonomy of these methods are presented, as well as links to their programming implementations, in the hope that this survey would serve as a reference point for both theorists and practitioners.",recent advanc artifici intellig ai led widespread industri adopt machin learn system demonstr superhuman perform signific number task howev surg perform often achiev increas model complex turn system black box approach caus uncertainti regard way oper ultim way come decis ambigu made problemat machin learn system adopt sensit yet critic domain valu could immens healthcar result scientif interest field explain artifici intellig xai field concern develop new method explain interpret machin learn model tremend reignit recent year studi focus machin learn interpret method specif literatur review taxonomi method present well link program implement hope survey would serv refer point theorist practition,2020,1719
Double/Debiased Machine Learning for Treatment and Structural Parameters,"We revisit the classic semiparametric problem of inference on a low dimensional parameter θ_0 in the presence of high-dimensional nuisance parameters η_0. We depart from the classical setting by allowing for η_0 to be so high-dimensional that the traditional assumptions, such as Donsker properties, that limit complexity of the parameter space for this object break down. To estimate η_0, we consider the use of statistical or machine learning (ML) methods which are particularly well-suited to estimation in modern, very high-dimensional cases. ML methods perform well by employing regularization to reduce variance and trading off regularization bias with overfitting in practice. However, both regularization bias and overfitting in estimating η_0 cause a heavy bias in estimators of θ_0 that are obtained by naively plugging ML estimators of η_0 into estimating equations for θ_0. This bias results in the naive estimator failing to be N^(-1/2) consistent, where N is the sample size. We show that the impact of regularization bias and overfitting on estimation of the parameter of interest θ_0 can be removed by using two simple, yet critical, ingredients: (1) using Neyman-orthogonal moments/scores that have reduced sensitivity with respect to nuisance parameters to estimate θ_0, and (2) making use of cross-fitting which provides an efficient form of data-splitting. We call the resulting set of methods double or debiased ML (DML). We verify that DML delivers point estimators that concentrate in a N^(-1/2)-neighborhood of the true parameter values and are approximately unbiased and normally distributed, which allows construction of valid confidence statements. The generic statistical theory of DML is elementary and simultaneously relies on only weak theoretical requirements which will admit the use of a broad array of modern ML methods for estimating the nuisance parameters such as random forests, lasso, ridge, deep neural nets, boosted trees, and various hybrids and ensembles of these methods. We illustrate the general theory by applying it to provide theoretical properties of DML applied to learn the main regression parameter in a partially linear regression model, DML applied to learn the coefficient on an endogenous variable in a partially linear instrumental variables model, DML applied to learn the average treatment effect and the average treatment effect on the treated under unconfoundedness, and DML applied to learn the local average treatment effect in an instrumental variables setting. In addition to these theoretical applications, we also illustrate the use of DML in three empirical examples.",revisit classic semiparametr problem infer low dimension paramet presenc highdimension nuisanc paramet depart classic set allow highdimension tradit assumpt donsker properti limit complex paramet space object break estim consid use statist machin learn ml method particularli wellsuit estim modern highdimension case ml method perform well employ regular reduc varianc trade regular bia overfit practic howev regular bia overfit estim caus heavi bia estim obtain naiv plug ml estim estim equat bia result naiv estim fail n consist n sampl size show impact regular bia overfit estim paramet interest remov use two simpl yet critic ingredi use neymanorthogon momentsscor reduc sensit respect nuisanc paramet estim make use crossfit provid effici form datasplit call result set method doubl debias ml dml verifi dml deliv point estim concentr nneighborhood true paramet valu approxim unbias normal distribut allow construct valid confid statement gener statist theori dml elementari simultan reli weak theoret requir admit use broad array modern ml method estim nuisanc paramet random forest lasso ridg deep neural net boost tree variou hybrid ensembl method illustr gener theori appli provid theoret properti dml appli learn main regress paramet partial linear regress model dml appli learn coeffici endogen variabl partial linear instrument variabl model dml appli learn averag treatment effect averag treatment effect treat unconfounded dml appli learn local averag treatment effect instrument variabl set addit theoret applic also illustr use dml three empir exampl,2017,2090
SoilGrids250m: Global gridded soil information based on machine learning,"This paper describes the technical development and accuracy assessment of the most recent and improved version of the SoilGrids system at 250m resolution (June 2016 update). SoilGrids provides global predictions for standard numeric soil properties (organic carbon, bulk density, Cation Exchange Capacity (CEC), pH, soil texture fractions and coarse fragments) at seven standard depths (0, 5, 15, 30, 60, 100 and 200 cm), in addition to predictions of depth to bedrock and distribution of soil classes based on the World Reference Base (WRB) and USDA classification systems (ca. 280 raster layers in total). Predictions were based on ca. 150,000 soil profiles used for training and a stack of 158 remote sensing-based soil covariates (primarily derived from MODIS land products, SRTM DEM derivatives, climatic images and global landform and lithology maps), which were used to fit an ensemble of machine learning methods—random forest and gradient boosting and/or multinomial logistic regression—as implemented in the R packages ranger, xgboost, nnet and caret. The results of 10–fold cross-validation show that the ensemble models explain between 56% (coarse fragments) and 83% (pH) of variation with an overall average of 61%. Improvements in the relative accuracy considering the amount of variation explained, in comparison to the previous version of SoilGrids at 1 km spatial resolution, range from 60 to 230%. Improvements can be attributed to: (1) the use of machine learning instead of linear regression, (2) to considerable investments in preparing finer resolution covariate layers and (3) to insertion of additional soil profiles. Further development of SoilGrids could include refinement of methods to incorporate input uncertainties and derivation of posterior probability distributions (per pixel), and further automation of spatial modeling so that soil maps can be generated for potentially hundreds of soil variables. Another area of future research is the development of methods for multiscale merging of SoilGrids predictions with local and/or national gridded soil products (e.g. up to 50 m spatial resolution) so that increasingly more accurate, complete and consistent global soil information can be produced. SoilGrids are available under the Open Data Base License.",paper describ technic develop accuraci assess recent improv version soilgrid system resolut june updat soilgrid provid global predict standard numer soil properti organ carbon bulk densiti cation exchang capac cec ph soil textur fraction coars fragment seven standard depth cm addit predict depth bedrock distribut soil class base world refer base wrb usda classif system ca raster layer total predict base ca soil profil use train stack remot sensingbas soil covari primarili deriv modi land product srtm dem deriv climat imag global landform litholog map use fit ensembl machin learn methodsrandom forest gradient boost andor multinomi logist regressiona implement r packag ranger xgboost nnet caret result fold crossvalid show ensembl model explain coars fragment ph variat overal averag improv rel accuraci consid amount variat explain comparison previou version soilgrid km spatial resolut rang improv attribut use machin learn instead linear regress consider invest prepar finer resolut covari layer insert addit soil profil develop soilgrid could includ refin method incorpor input uncertainti deriv posterior probabl distribut per pixel autom spatial model soil map gener potenti hundr soil variabl anoth area futur research develop method multiscal merg soilgrid predict local andor nation grid soil product eg spatial resolut increasingli accur complet consist global soil inform produc soilgrid avail open data base licens,2017,2807
"Federated Learning: Collaborative Machine Learning without
Centralized Training Data","Federated learning (also known as collaborative learning) is a machine learning technique that trains
an algorithm without transferring data samples across numerous decentralized edge devices or
servers. This strategy differs from standard centralized machine learning techniques in which all local
datasets are uploaded to a single server, as well as more traditional decentralized alternatives, which
frequently presume that local data samples are uniformly distributed.
Federated learning allows several actors to collaborate on the development of a single, robust
machine learning model without sharing data, allowing crucial issues such as data privacy, data
security, data access rights, and access to heterogeneous data to be addressed. Defence,
telecommunications, internet of things, and pharmaceutical industries are just a few of the sectors
where it has applications.",feder learn also known collabor learn machin learn techniqu train algorithm without transfer data sampl across numer decentr edg devic server strategi differ standard central machin learn techniqu local dataset upload singl server well tradit decentr altern frequent presum local data sampl uniformli distribut feder learn allow sever actor collabor develop singl robust machin learn model without share data allow crucial issu data privaci data secur data access right access heterogen data address defenc telecommun internet thing pharmaceut industri sector applic,2022,650
Practical Secure Aggregation for Privacy-Preserving Machine Learning,"We design a novel, communication-efficient, failure-robust protocol for secure aggregation of high-dimensional data. Our protocol allows a server to compute the sum of large, user-held data vectors from mobile devices in a secure manner (i.e. without learning each user's individual contribution), and can be used, for example, in a federated learning setting, to aggregate user-provided model updates for a deep neural network. We prove the security of our protocol in the honest-but-curious and active adversary settings, and show that security is maintained even if an arbitrarily chosen subset of users drop out at any time. We evaluate the efficiency of our protocol and show, by complexity analysis and a concrete implementation, that its runtime and communication overhead remain low even on large data sets and client pools. For 16-bit input values, our protocol offers $1.73 x communication expansion for 210 users and 220-dimensional vectors, and 1.98 x expansion for 214 users and 224-dimensional vectors over sending data in the clear.",design novel communicationeffici failurerobust protocol secur aggreg highdimension data protocol allow server comput sum larg userheld data vector mobil devic secur manner ie without learn user individu contribut use exampl feder learn set aggreg userprovid model updat deep neural network prove secur protocol honestbutcuri activ adversari set show secur maintain even arbitrarili chosen subset user drop time evalu effici protocol show complex analysi concret implement runtim commun overhead remain low even larg data set client pool bit input valu protocol offer x commun expans user dimension vector x expans user dimension vector send data clear,2017,2810
Multimodal Machine Learning: A Survey and Taxonomy,"Our experience of the world is multimodal - we see objects, hear sounds, feel texture, smell odors, and taste flavors. Modality refers to the way in which something happens or is experienced and a research problem is characterized as multimodal when it includes multiple such modalities. In order for Artificial Intelligence to make progress in understanding the world around us, it needs to be able to interpret such multimodal signals together. Multimodal machine learning aims to build models that can process and relate information from multiple modalities. It is a vibrant multi-disciplinary field of increasing importance and with extraordinary potential. Instead of focusing on specific multimodal applications, this paper surveys the recent advances in multimodal machine learning itself and presents them in a common taxonomy. We go beyond the typical early and late fusion categorization and identify broader challenges that are faced by multimodal machine learning, namely: representation, translation, alignment, fusion, and co-learning. This new taxonomy will enable researchers to better understand the state of the field and identify directions for future research.",experi world multimod see object hear sound feel textur smell odor tast flavor modal refer way someth happen experienc research problem character multimod includ multipl modal order artifici intellig make progress understand world around u need abl interpret multimod signal togeth multimod machin learn aim build model process relat inform multipl modal vibrant multidisciplinari field increas import extraordinari potenti instead focus specif multimod applic paper survey recent advanc multimod machin learn present common taxonomi go beyond typic earli late fusion categor identifi broader challeng face multimod machin learn name represent translat align fusion colearn new taxonomi enabl research better understand state field identifi direct futur research,2017,2628
Introduction to Machine Learning,,,2018,2503
ilastik: interactive machine learning for (bio)image analysis,,,2019,2080
On Hyperparameter Optimization of Machine Learning Algorithms: Theory and Practice,,,2020,1799
Correlation-based Feature Selection for Machine Learning,"A central problem in machine learning is identifying a representative set of features from which to construct a classification model for a particular task. This thesis addresses the problem of feature selection for machine learning through a correlation based approach. The central hypothesis is that good feature sets contain features that are highly correlated with the class, yet uncorrelated with each other. A feature evaluation formula, based on ideas from test theory, provides an operational definition of this hypothesis. CFS (Correlation based Feature Selection) is an algorithm that couples this evaluation formula with an appropriate correlation measure and a heuristic search strategy. CFS was evaluated by experiments on artificial and natural datasets. Three machine learning algorithms were used: C4.5 (a decision tree learner), IB1 (an instance based learner), and naive Bayes. Experiments on artificial datasets showed that CFS quickly identifies and screens irrelevant, redundant, and noisy features, and identifies relevant features as long as their relevance does not strongly depend on other features. On natural domains, CFS typically eliminated well over half the features. In most cases, classification accuracy using the reduced feature set equaled or bettered accuracy using the complete feature set. Feature selection degraded machine learning performance in cases where some features were eliminated which were highly predictive of very small areas of the instance space. Further experiments compared CFS with a wrapper—a well known approach to feature selection that employs the target learning algorithm to evaluate feature sets. In many cases CFS gave comparable results to the wrapper, and in general, outperformed the wrapper on small datasets. CFS executes many times faster than the wrapper, which allows it to scale to larger datasets. Two methods of extending CFS to handle feature interaction are presented and experimentally evaluated. The first considers pairs of features and the second incorporates iii feature weights calculated by the RELIEF algorithm. Experiments on artificial domains showed that both methods were able to identify interacting features. On natural domains, the pairwise method gave more reliable results than using weights provided by RELIEF.",central problem machin learn identifi repres set featur construct classif model particular task thesi address problem featur select machin learn correl base approach central hypothesi good featur set contain featur highli correl class yet uncorrel featur evalu formula base idea test theori provid oper definit hypothesi cf correl base featur select algorithm coupl evalu formula appropri correl measur heurist search strategi cf evalu experi artifici natur dataset three machin learn algorithm use c decis tree learner ib instanc base learner naiv bay experi artifici dataset show cf quickli identifi screen irrelev redund noisi featur identifi relev featur long relev strongli depend featur natur domain cf typic elimin well half featur case classif accuraci use reduc featur set equal better accuraci use complet featur set featur select degrad machin learn perform case featur elimin highli predict small area instanc space experi compar cf wrappera well known approach featur select employ target learn algorithm evalu featur set mani case cf gave compar result wrapper gener outperform wrapper small dataset cf execut mani time faster wrapper allow scale larger dataset two method extend cf handl featur interact present experiment evalu first consid pair featur second incorpor iii featur weight calcul relief algorithm experi artifici domain show method abl identifi interact featur natur domain pairwis method gave reliabl result use weight provid relief,2003,3900
CrypTen: Secure Multi-Party Computation Meets Machine Learning,"Secure multi-party computation (MPC) allows parties to perform computations on data while keeping that data private. This capability has great potential for machine-learning applications: it facilitates training of machine-learning models on private data sets owned by different parties, evaluation of one party's private model using another party's private data, etc. Although a range of studies implement machine-learning models via secure MPC, such implementations are not yet mainstream. Adoption of secure MPC is hampered by the absence of flexible software frameworks that""speak the language""of machine-learning researchers and engineers. To foster adoption of secure MPC in machine learning, we present CrypTen: a software framework that exposes popular secure MPC primitives via abstractions that are common in modern machine-learning frameworks, such as tensor computations, automatic differentiation, and modular neural networks. This paper describes the design of CrypTen and measure its performance on state-of-the-art models for text classification, speech recognition, and image classification. Our benchmarks show that CrypTen's GPU support and high-performance communication between (an arbitrary number of) parties allows it to perform efficient private evaluation of modern machine-learning models under a semi-honest threat model. For example, two parties using CrypTen can securely predict phonemes in speech recordings using Wav2Letter faster than real-time. We hope that CrypTen will spur adoption of secure MPC in the machine-learning community.",secur multiparti comput mpc allow parti perform comput data keep data privat capabl great potenti machinelearn applic facilit train machinelearn model privat data set own differ parti evalu one parti privat model use anoth parti privat data etc although rang studi implement machinelearn model via secur mpc implement yet mainstream adopt secur mpc hamper absenc flexibl softwar framework thatspeak languageof machinelearn research engin foster adopt secur mpc machin learn present crypten softwar framework expo popular secur mpc primit via abstract common modern machinelearn framework tensor comput automat differenti modular neural network paper describ design crypten measur perform stateoftheart model text classif speech recognit imag classif benchmark show crypten gpu support highperform commun arbitrari number parti allow perform effici privat evalu modern machinelearn model semihonest threat model exampl two parti use crypten secur predict phonem speech record use wavlett faster realtim hope crypten spur adopt secur mpc machinelearn commun,2021,313
Applications of machine learning to machine fault diagnosis: A review and roadmap,,,2020,1720
Machine learning–accelerated computational fluid dynamics,"Significance Accurate simulation of fluids is important for many science and engineering problems but is very computationally demanding. In contrast, machine-learning models can approximate physics very quickly but at the cost of accuracy. Here we show that using machine learning inside traditional fluid simulations can improve both accuracy and speed, even on examples very different from the training data. Our approach opens the door to applying machine learning to large-scale physical modeling tasks like airplane design and climate prediction. Numerical simulation of fluids plays an essential role in modeling many physical phenomena, such as weather, climate, aerodynamics, and plasma physics. Fluids are well described by the Navier–Stokes equations, but solving these equations at scale remains daunting, limited by the computational cost of resolving the smallest spatiotemporal features. This leads to unfavorable trade-offs between accuracy and tractability. Here we use end-to-end deep learning to improve approximations inside computational fluid dynamics for modeling two-dimensional turbulent flows. For both direct numerical simulation of turbulence and large-eddy simulation, our results are as accurate as baseline solvers with 8 to 10× finer resolution in each spatial dimension, resulting in 40- to 80-fold computational speedups. Our method remains stable during long simulations and generalizes to forcing functions and Reynolds numbers outside of the flows where it is trained, in contrast to black-box machine-learning approaches. Our approach exemplifies how scientific computing can leverage machine learning and hardware accelerators to improve simulations without sacrificing accuracy or generalization.",signific accur simul fluid import mani scienc engin problem comput demand contrast machinelearn model approxim physic quickli cost accuraci show use machin learn insid tradit fluid simul improv accuraci speed even exampl differ train data approach open door appli machin learn largescal physic model task like airplan design climat predict numer simul fluid play essenti role model mani physic phenomenon weather climat aerodynam plasma physic fluid well describ navierstok equat solv equat scale remain daunt limit comput cost resolv smallest spatiotempor featur lead unfavor tradeoff accuraci tractabl use endtoend deep learn improv approxim insid comput fluid dynam model twodimension turbul flow direct numer simul turbul largeeddi simul result accur baselin solver finer resolut spatial dimens result fold comput speedup method remain stabl long simul gener forc function reynold number outsid flow train contrast blackbox machinelearn approach approach exemplifi scientif comput leverag machin learn hardwar acceler improv simul without sacrif accuraci gener,2021,745
Supervised Machine Learning: A Review of Classification Techniques,"The goal of supervised learning is to build a concise model of the distribution of class labels in terms of predictor features. The resulting classifier is then used to assign class labels to the testing instances where the values of the predictor features are known, but the value of the class label is unknown. This paper describes various supervised machine learning classification techniques. Of course, a single chapter cannot be a complete review of all supervised machine learning classification algorithms (also known induction classification algorithms), yet we hope that the references cited will cover the major theoretical issues, guiding the researcher in interesting research directions and suggesting possible bias combinations that have yet to be explored.",goal supervis learn build concis model distribut class label term predictor featur result classifi use assign class label test instanc valu predictor featur known valu class label unknown paper describ variou supervis machin learn classif techniqu cours singl chapter cannot complet review supervis machin learn classif algorithm also known induct classif algorithm yet hope refer cite cover major theoret issu guid research interest research direct suggest possibl bia combin yet explor,2007,4724
Machine Learning with Adversaries: Byzantine Tolerant Gradient Descent,"We study the resilience to Byzantine failures of distributed implementations of Stochastic Gradient Descent (SGD). So far, distributed machine learning frameworks have largely ignored the possibility of failures, especially arbitrary (i.e., Byzantine) ones. Causes of failures include software bugs, network asynchrony, biases in local datasets, as well as attackers trying to compromise the entire system. Assuming a set of n workers, up to f being Byzantine, we ask how resilient can SGD be, without limiting the dimension, nor the size of the parameter space. We first show that no gradient aggregation rule based on a linear combination of the vectors proposed by the workers (i.e, current approaches) tolerates a single Byzantine failure. We then formulate a resilience property of the aggregation rule capturing the basic requirements to guarantee convergence despite f Byzantine workers. We propose Krum, an aggregation rule that satisfies our resilience property, which we argue is the first provably Byzantine-resilient algorithm for distributed SGD. We also report on experimental evaluations of Krum.",studi resili byzantin failur distribut implement stochast gradient descent sgd far distribut machin learn framework larg ignor possibl failur especi arbitrari ie byzantin one caus failur includ softwar bug network asynchroni bias local dataset well attack tri compromis entir system assum set n worker f byzantin ask resili sgd without limit dimens size paramet space first show gradient aggreg rule base linear combin vector propos worker ie current approach toler singl byzantin failur formul resili properti aggreg rule captur basic requir guarante converg despit f byzantin worker propos krum aggreg rule satisfi resili properti argu first provabl byzantineresili algorithm distribut sgd also report experiment evalu krum,2017,1564
The use of the area under the ROC curve in the evaluation of machine learning algorithms,,,1997,6319
Gaussian Processes for Machine Learning (Adaptive Computation and Machine Learning),,,2005,4239
Federated Machine Learning,"Today’s artificial intelligence still faces two major challenges. One is that, in most industries, data exists in the form of isolated islands. The other is the strengthening of data privacy and security. We propose a possible solution to these challenges: secure federated learning. Beyond the federated-learning framework first proposed by Google in 2016, we introduce a comprehensive secure federated-learning framework, which includes horizontal federated learning, vertical federated learning, and federated transfer learning. We provide definitions, architectures, and applications for the federated-learning framework, and provide a comprehensive survey of existing works on this subject. In addition, we propose building data networks among organizations based on federated mechanisms as an effective solution to allowing knowledge to be shared without compromising user privacy.",today artifici intellig still face two major challeng one industri data exist form isol island strengthen data privaci secur propos possibl solut challeng secur feder learn beyond federatedlearn framework first propos googl introduc comprehens secur federatedlearn framework includ horizont feder learn vertic feder learn feder transfer learn provid definit architectur applic federatedlearn framework provid comprehens survey exist work subject addit propos build data network among organ base feder mechan effect solut allow knowledg share without compromis user privaci,2019,2128
A Review on Fairness in Machine Learning,"An increasing number of decisions regarding the daily lives of human beings are being controlled by artificial intelligence and machine learning (ML) algorithms in spheres ranging from healthcare, transportation, and education to college admissions, recruitment, provision of loans, and many more realms. Since they now touch on many aspects of our lives, it is crucial to develop ML algorithms that are not only accurate but also objective and fair. Recent studies have shown that algorithmic decision making may be inherently prone to unfairness, even when there is no intention for it. This article presents an overview of the main concepts of identifying, measuring, and improving algorithmic fairness when using ML algorithms, focusing primarily on classification tasks. The article begins by discussing the causes of algorithmic bias and unfairness and the common definitions and measures for fairness. Fairness-enhancing mechanisms are then reviewed and divided into pre-process, in-process, and post-process mechanisms. A comprehensive comparison of the mechanisms is then conducted, toward a better understanding of which mechanisms should be used in different scenarios. The article ends by reviewing several emerging research sub-fields of algorithmic fairness, beyond classification.",increas number decis regard daili live human be control artifici intellig machin learn ml algorithm sphere rang healthcar transport educ colleg admiss recruit provis loan mani realm sinc touch mani aspect live crucial develop ml algorithm accur also object fair recent studi shown algorithm decis make may inher prone unfair even intent articl present overview main concept identifi measur improv algorithm fair use ml algorithm focus primarili classif task articl begin discus caus algorithm bia unfair common definit measur fair fairnessenhanc mechan review divid preprocess inprocess postprocess mechan comprehens comparison mechan conduct toward better understand mechan use differ scenario articl end review sever emerg research subfield algorithm fair beyond classif,2022,383
Educational data mining: prediction of students' academic performance using machine learning algorithms,,,2022,292
Challenges and opportunities in quantum machine learning,,,2022,316
Human-in-the-loop machine learning: a state of the art,,,2022,298
Classification Based on Decision Tree Algorithm for Machine Learning,"Decision tree classifiers are regarded to be a standout of the most well-known methods to data classification representation of classifiers. Different researchers from various fields and backgrounds have considered the problem of extending a decision tree from available data, such as machine study, pattern recognition, and statistics. In various fields such as medical disease analysis, text classification, user smartphone classification, images, and many more the employment of Decision tree classifiers has been proposed in many ways. This paper provides a detailed approach to the decision trees. Furthermore, paper specifics, such as algorithms/approaches used, datasets, and outcomes achieved, are evaluated and outlined comprehensively. In addition, all of the approaches analyzed were discussed to illustrate the themes of the authors and identify the most accurate classifiers. As a result, the uses of different types of datasets are discussed and their findings are analyzed.",decis tree classifi regard standout wellknown method data classif represent classifi differ research variou field background consid problem extend decis tree avail data machin studi pattern recognit statist variou field medic diseas analysi text classif user smartphon classif imag mani employ decis tree classifi propos mani way paper provid detail approach decis tree furthermor paper specif algorithmsapproach use dataset outcom achiev evalu outlin comprehens addit approach analyz discus illustr theme author identifi accur classifi result use differ type dataset discus find analyz,2021,1036
SecureML: A System for Scalable Privacy-Preserving Machine Learning,"Machine learning is widely used in practice to produce predictive models for applications such as image processing, speech and text recognition. These models are more accurate when trained on large amount of data collected from different sources. However, the massive data collection raises privacy concerns. In this paper, we present new and efficient protocols for privacy preserving machine learning for linear regression, logistic regression and neural network training using the stochastic gradient descent method. Our protocols fall in the two-server model where data owners distribute their private data among two non-colluding servers who train various models on the joint data using secure two-party computation (2PC). We develop new techniques to support secure arithmetic operations on shared decimal numbers, and propose MPC-friendly alternatives to non-linear functions such as sigmoid and softmax that are superior to prior work. We implement our system in C++. Our experiments validate that our protocols are several orders of magnitude faster than the state of the art implementations for privacy preserving linear and logistic regressions, and scale to millions of data samples with thousands of features. We also implement the first privacy preserving system for training neural networks.",machin learn wide use practic produc predict model applic imag process speech text recognit model accur train larg amount data collect differ sourc howev massiv data collect rais privaci concern paper present new effici protocol privaci preserv machin learn linear regress logist regress neural network train use stochast gradient descent method protocol fall twoserv model data owner distribut privat data among two noncollud server train variou model joint data use secur twoparti comput pc develop new techniqu support secur arithmet oper share decim number propos mpcfriendli altern nonlinear function sigmoid softmax superior prior work implement system c experi valid protocol sever order magnitud faster state art implement privaci preserv linear logist regress scale million data sampl thousand featur also implement first privaci preserv system train neural network,2017,1714
A guide to machine learning for biologists,,,2021,928
Machine learning and deep learning,,,2021,1006
Interpretable Machine Learning: Fundamental Principles and 10 Grand Challenges,"Interpretability in machine learning (ML) is crucial for high stakes decisions and troubleshooting. In this work, we provide fundamental principles for interpretable ML, and dispel common misunderstandings that dilute the importance of this crucial topic. We also identify 10 technical challenge areas in interpretable machine learning and provide history and background on each problem. Some of these problems are classically important, and some are recent problems that have arisen in the last few years. These problems are: (1) Optimizing sparse logical models such as decision trees; (2) Optimization of scoring systems; (3) Placing constraints into generalized additive models to encourage sparsity and better interpretability; (4) Modern case-based reasoning, including neural networks and matching for causal inference; (5) Complete supervised disentanglement of neural networks; (6) Complete or even partial unsupervised disentanglement of neural networks; (7) Dimensionality reduction for data visualization; (8) Machine learning models that can incorporate physics and other generative or causal constraints; (9) Characterization of the""Rashomon set""of good models; and (10) Interpretable reinforcement learning. This survey is suitable as a starting point for statisticians and computer scientists interested in working in interpretable machine learning.",interpret machin learn ml crucial high stake decis troubleshoot work provid fundament principl interpret ml dispel common misunderstand dilut import crucial topic also identifi technic challeng area interpret machin learn provid histori background problem problem classic import recent problem arisen last year problem optim spar logic model decis tree optim score system place constraint gener addit model encourag sparsiti better interpret modern casebas reason includ neural network match causal infer complet supervis disentangl neural network complet even partial unsupervis disentangl neural network dimension reduct data visual machin learn model incorpor physic gener causal constraint character therashomon setof good model interpret reinforc learn survey suitabl start point statistician comput scientist interest work interpret machin learn,2021,578
MRI-Based Brain Tumor Classification Using Ensemble of Deep Features and Machine Learning Classifiers,"Brain tumor classification plays an important role in clinical diagnosis and effective treatment. In this work, we propose a method for brain tumor classification using an ensemble of deep features and machine learning classifiers. In our proposed framework, we adopt the concept of transfer learning and uses several pre-trained deep convolutional neural networks to extract deep features from brain magnetic resonance (MR) images. The extracted deep features are then evaluated by several machine learning classifiers. The top three deep features which perform well on several machine learning classifiers are selected and concatenated as an ensemble of deep features which is then fed into several machine learning classifiers to predict the final output. To evaluate the different kinds of pre-trained models as a deep feature extractor, machine learning classifiers, and the effectiveness of an ensemble of deep feature for brain tumor classification, we use three different brain magnetic resonance imaging (MRI) datasets that are openly accessible from the web. Experimental results demonstrate that an ensemble of deep features can help improving performance significantly, and in most cases, support vector machine (SVM) with radial basis function (RBF) kernel outperforms other machine learning classifiers, especially for large datasets.",brain tumor classif play import role clinic diagnosi effect treatment work propos method brain tumor classif use ensembl deep featur machin learn classifi propos framework adopt concept transfer learn use sever pretrain deep convolut neural network extract deep featur brain magnet reson mr imag extract deep featur evalu sever machin learn classifi top three deep featur perform well sever machin learn classifi select concaten ensembl deep featur fed sever machin learn classifi predict final output evalu differ kind pretrain model deep featur extractor machin learn classifi effect ensembl deep featur brain tumor classif use three differ brain magnet reson imag mri dataset openli access web experiment result demonstr ensembl deep featur help improv perform significantli case support vector machin svm radial basi function rbf kernel outperform machin learn classifi especi larg dataset,2021,341
MoleculeNet: a benchmark for molecular machine learning,"A large scale benchmark for molecular machine learning consisting of multiple public datasets, metrics, featurizations and learning algorithms.",larg scale benchmark molecular machin learn consist multipl public dataset metric featur learn algorithm,2017,1631
Understanding Machine Learning - From Theory to Algorithms,"Machine learning is one of the fastest growing areas of computer science, with far-reaching applications. The aim of this textbook is to introduce machine learning, and the algorithmic paradigms it offers, in a principled way. The book provides an extensive theoretical account of the fundamental ideas underlying machine learning and the mathematical derivations that transform these principles into practical algorithms. Following a presentation of the basics of the field, the book covers a wide array of central topics that have not been addressed by previous textbooks. These include a discussion of the computational complexity of learning and the concepts of convexity and stability ; important algorithmic paradigms including stochastic gradient descent, neural networks, and structured output learning ; and emerging theoretical concepts such as the PACBayes approach and compression-based bounds. Designed for an advanced undergraduate or beginning graduate course, the text makes the fundamentals and algorithms of machine learning accessible to students and nonexpert readers in statistics, computer science, mathematics, and engineering.",machin learn one fastest grow area comput scienc farreach applic aim textbook introduc machin learn algorithm paradigm offer principl way book provid extens theoret account fundament idea underli machin learn mathemat deriv transform principl practic algorithm follow present basic field book cover wide array central topic address previou textbook includ discus comput complex learn concept convex stabil import algorithm paradigm includ stochast gradient descent neural network structur output learn emerg theoret concept pacbay approach compressionbas bound design advanc undergradu begin graduat cours text make fundament algorithm machin learn access student nonexpert reader statist comput scienc mathemat engin,2014,2863
Machine Learning With Python,,,2019,2111
FedML: A Research Library and Benchmark for Federated Machine Learning,"Federated learning is a rapidly growing research field in the machine learning domain. Although considerable research efforts have been made, existing libraries cannot adequately support diverse algorithmic development (e.g., diverse topology and flexible message exchange), and inconsistent dataset and model usage in experiments make fair comparisons difficult. In this work, we introduce FedML, an open research library and benchmark that facilitates the development of new federated learning algorithms and fair performance comparisons. FedML supports three computing paradigms (distributed training, mobile on-device training, and standalone simulation) for users to conduct experiments in different system environments. FedML also promotes diverse algorithmic research with flexible and generic API design and reference baseline implementations. A curated and comprehensive benchmark dataset for the non-I.I.D setting aims at making a fair comparison. We believe FedML can provide an efficient and reproducible means of developing and evaluating algorithms for the federated learning research community. We maintain the source code, documents, and user community at this https URL.",feder learn rapidli grow research field machin learn domain although consider research effort made exist librari cannot adequ support diver algorithm develop eg diver topolog flexibl messag exchang inconsist dataset model usag experi make fair comparison difficult work introduc fedml open research librari benchmark facilit develop new feder learn algorithm fair perform comparison fedml support three comput paradigm distribut train mobil ondevic train standalon simul user conduct experi differ system environ fedml also promot diver algorithm research flexibl gener api design refer baselin implement curat comprehens benchmark dataset noniid set aim make fair comparison believ fedml provid effici reproduc mean develop evalu algorithm feder learn research commun maintain sourc code document user commun http url,2020,512
Adversarial machine learning,"In this paper (expanded from an invited talk at AISEC 2010), we discuss an emerging field of study: adversarial machine learning---the study of effective machine learning techniques against an adversarial opponent. In this paper, we: give a taxonomy for classifying attacks against online machine learning algorithms; discuss application-specific factors that limit an adversary's capabilities; introduce two models for modeling an adversary's capabilities; explore the limits of an adversary's knowledge about the algorithm, feature space, training, and input data; explore vulnerabilities in machine learning algorithms; discuss countermeasures against attacks; introduce the evasion challenge; and discuss privacy-preserving learning techniques.",paper expand invit talk aisec discus emerg field studi adversari machin learningth studi effect machin learn techniqu adversari oppon paper give taxonomi classifi attack onlin machin learn algorithm discus applicationspecif factor limit adversari capabl introduc two model model adversari capabl explor limit adversari knowledg algorithm featur space train input data explor vulner machin learn algorithm discus countermeasur attack introduc evas challeng discus privacypreserv learn techniqu,2019,1400
Power of data in quantum machine learning,,,2020,549
Reconciling modern machine-learning practice and the classical bias–variance trade-off,"Significance While breakthroughs in machine learning and artificial intelligence are changing society, our fundamental understanding has lagged behind. It is traditionally believed that fitting models to the training data exactly is to be avoided as it leads to poor performance on unseen data. However, powerful modern classifiers frequently have near-perfect fit in training, a disconnect that spurred recent intensive research and controversy on whether theory provides practical insights. In this work, we show how classical theory and modern practice can be reconciled within a single unified performance curve and propose a mechanism underlying its emergence. We believe this previously unknown pattern connecting the structure and performance of learning architectures will help shape design and understanding of learning algorithms. Breakthroughs in machine learning are rapidly changing science and society, yet our fundamental understanding of this technology has lagged far behind. Indeed, one of the central tenets of the field, the bias–variance trade-off, appears to be at odds with the observed behavior of methods used in modern machine-learning practice. The bias–variance trade-off implies that a model should balance underfitting and overfitting: Rich enough to express underlying structure in data and simple enough to avoid fitting spurious patterns. However, in modern practice, very rich models such as neural networks are trained to exactly fit (i.e., interpolate) the data. Classically, such models would be considered overfitted, and yet they often obtain high accuracy on test data. This apparent contradiction has raised questions about the mathematical foundations of machine learning and their relevance to practitioners. In this paper, we reconcile the classical understanding and the modern practice within a unified performance curve. This “double-descent” curve subsumes the textbook U-shaped bias–variance trade-off curve by showing how increasing model capacity beyond the point of interpolation results in improved performance. We provide evidence for the existence and ubiquity of double descent for a wide spectrum of models and datasets, and we posit a mechanism for its emergence. This connection between the performance and the structure of machine-learning models delineates the limits of classical analyses and has implications for both the theory and the practice of machine learning.",signific breakthrough machin learn artifici intellig chang societi fundament understand lag behind tradit believ fit model train data exactli avoid lead poor perform unseen data howev power modern classifi frequent nearperfect fit train disconnect spur recent intens research controversi whether theori provid practic insight work show classic theori modern practic reconcil within singl unifi perform curv propos mechan underli emerg believ previous unknown pattern connect structur perform learn architectur help shape design understand learn algorithm breakthrough machin learn rapidli chang scienc societi yet fundament understand technolog lag far behind inde one central tenet field biasvari tradeoff appear odd observ behavior method use modern machinelearn practic biasvari tradeoff impli model balanc underfit overfit rich enough express underli structur data simpl enough avoid fit spuriou pattern howev modern practic rich model neural network train exactli fit ie interpol data classic model would consid overfit yet often obtain high accuraci test data appar contradict rais question mathemat foundat machin learn relev practition paper reconcil classic understand modern practic within unifi perform curv doubledesc curv subsum textbook ushap biasvari tradeoff curv show increas model capac beyond point interpol result improv perform provid evid exist ubiqu doubl descent wide spectrum model dataset posit mechan emerg connect perform structur machinelearn model delin limit classic analys implic theori practic machin learn,2018,1514
Universal Differential Equations for Scientific Machine Learning,"
 In the context of science, the well-known adage “a picture is worth a thousand words” might well be “a model is worth a thousand datasets.” Scientific models, such as Newtonian physics or biological gene regulatory networks, are human-driven simplifications of complex phenomena that serve as surrogates for the countless experiments that validated the models. Recently, machine learning has been able to overcome the inaccuracies of approximate modeling by directly learning the entire set of nonlinear interactions from data. However, without any predetermined structure from the scientific basis behind the problem, machine learning approaches are flexible but data-expensive, requiring large databases of homogeneous labeled training data. A central challenge is reconciling data that is at odds with simplified models without requiring ""big data"". In this work demonstrate how a mathematical object, which we denote universal differential equations (UDEs), can be utilized as a theoretical underpinning to a diverse array of problems in scientific machine learning to yield efficient algorithms and generalized approaches. The UDE model augments scientific models with machine-learnable structures for scientifically-based learning. We show how UDEs can be utilized to discover previously unknown governing equations, accurately extrapolate beyond the original data, and accelerate model simulation, all in a time and data-efficient manner. This advance is coupled with open-source software that allows for training UDEs which incorporate physical constraints, delayed interactions, implicitly-defined events, and intrinsic stochasticity in the model. Our examples show how a diverse set of computationally-difficult modeling issues across scientific disciplines, from automatically discovering biological mechanisms to accelerating the training of physics-informed neural networks and large-eddy simulations, can all be transformed into UDE training problems that are efficiently solved by a single software methodology.",context scienc wellknown adag pictur worth thousand word might well model worth thousand dataset scientif model newtonian physic biolog gene regulatori network humandriven simplif complex phenomenon serv surrog countless experi valid model recent machin learn abl overcom inaccuraci approxim model directli learn entir set nonlinear interact data howev without predetermin structur scientif basi behind problem machin learn approach flexibl dataexpens requir larg databas homogen label train data central challeng reconcil data odd simplifi model without requir big data work demonstr mathemat object denot univers differenti equat ude util theoret underpin diver array problem scientif machin learn yield effici algorithm gener approach ude model augment scientif model machinelearn structur scientificallybas learn show ude util discov previous unknown govern equat accur extrapol beyond origin data acceler model simul time dataeffici manner advanc coupl opensourc softwar allow train ude incorpor physic constraint delay interact implicitlydefin event intrins stochast model exampl show diver set computationallydifficult model issu across scientif disciplin automat discov biolog mechan acceler train physicsinform neural network largeeddi simul transform ude train problem effici solv singl softwar methodolog,2020,523
Fairness in Machine Learning: A Survey,"When Machine Learning technologies are used in contexts that affect citizens, companies as well as researchers need to be confident that there will not be any unexpected social implications, such as bias towards gender, ethnicity, and/or people with disabilities. There is significant literature on approaches to mitigate bias and promote fairness, yet the area is complex and hard to penetrate for newcomers to the domain. This article seeks to provide an overview of the different schools of thought and approaches that aim to increase the fairness of Machine Learning. It organizes approaches into the widely accepted framework of pre-processing, in-processing, and post-processing methods, subcategorizing into a further 11 method areas. Although much of the literature emphasizes binary classification, a discussion of fairness in regression, recommender systems, and unsupervised learning is also provided along with a selection of currently available open source libraries. The article concludes by summarizing open challenges articulated as five dilemmas for fairness research.",machin learn technolog use context affect citizen compani well research need confid unexpect social implic bia toward gender ethnic andor peopl disabl signific literatur approach mitig bia promot fair yet area complex hard penetr newcom domain articl seek provid overview differ school thought approach aim increas fair machin learn organ approach wide accept framework preprocess inprocess postprocess method subcategor method area although much literatur emphas binari classif discus fair regress recommend system unsupervis learn also provid along select current avail open sourc librari articl conclud summar open challeng articul five dilemma fair research,2020,534
Heart Disease Prediction using Machine Learning Techniques,,,2020,510
Counterfactual Explanations for Machine Learning: A Review,"Machine learning plays a role in many deployed decision systems, often in ways that are difficult or impossible to understand by human stakeholders. Explaining, in a human-understandable way, the relationship between the input and output of machine learning models is essential to the development of trustworthy machine-learning-based systems. A burgeoning body of research seeks to define the goals and methods of explainability in machine learning. In this paper, we seek to review and categorize research on counterfactual explanations, a specific class of explanation that provides a link between what could have happened had input to a model been changed in a particular way. Modern approaches to counterfactual explainability in machine learning draw connections to the established legal doctrine in many countries, making them appealing to fielded systems in high-impact areas such as finance and healthcare. Thus, we design a rubric with desirable properties of counterfactual explanation algorithms and comprehensively evaluate all currently-proposed algorithms against that rubric. Our rubric provides easy comparison and comprehension of the advantages and disadvantages of different approaches and serves as an introduction to major research themes in this field. We also identify gaps and discuss promising research directions in the space of counterfactual explainability.",machin learn play role mani deploy decis system often way difficult imposs understand human stakehold explain humanunderstand way relationship input output machin learn model essenti develop trustworthi machinelearningbas system burgeon bodi research seek defin goal method explain machin learn paper seek review categor research counterfactu explan specif class explan provid link could happen input model chang particular way modern approach counterfactu explain machin learn draw connect establish legal doctrin mani countri make appeal field system highimpact area financ healthcar thu design rubric desir properti counterfactu explan algorithm comprehens evalu currentlypropos algorithm rubric rubric provid easi comparison comprehens advantag disadvantag differ approach serv introduct major research theme field also identifi gap discus promis research direct space counterfactu explain,2020,386
Stable learning establishes some common ground between causal inference and machine learning,,,2022,136
Gaussian Processes in Machine Learning,,,2003,3993
Machine Learning for Fluid Mechanics,"The field of fluid mechanics is rapidly advancing, driven by unprecedented volumes of data from experiments, field measurements, and large-scale simulations at multiple spatiotemporal scales. Machine learning (ML) offers a wealth of techniques to extract information from data that can be translated into knowledge about the underlying fluid mechanics. Moreover, ML algorithms can augment domain knowledge and automate tasks related to flow control and optimization. This article presents an overview of past history, current developments, and emerging opportunities of ML for fluid mechanics. We outline fundamental ML methodologies and discuss their uses for understanding, modeling, optimizing, and controlling fluid flows. The strengths and limitations of these methods are addressed from the perspective of scientific inquiry that considers data as an inherent part of modeling, experiments, and simulations. ML provides a powerful information-processing framework that can augment, and possibly even transform, current lines of fluid mechanics research and industrial applications.",field fluid mechan rapidli advanc driven unpreced volum data experi field measur largescal simul multipl spatiotempor scale machin learn ml offer wealth techniqu extract inform data translat knowledg underli fluid mechan moreov ml algorithm augment domain knowledg autom task relat flow control optim articl present overview past histori current develop emerg opportun ml fluid mechan outlin fundament ml methodolog discus use understand model optim control fluid flow strength limit method address perspect scientif inquiri consid data inher part model experi simul ml provid power informationprocess framework augment possibl even transform current line fluid mechan research industri applic,2019,1948
"Definitions, methods, and applications in interpretable machine learning","Significance The recent surge in interpretability research has led to confusion on numerous fronts. In particular, it is unclear what it means to be interpretable and how to select, evaluate, or even discuss methods for producing interpretations of machine-learning models. We aim to clarify these concerns by defining interpretable machine learning and constructing a unifying framework for existing methods which highlights the underappreciated role played by human audiences. Within this framework, methods are organized into 2 classes: model based and post hoc. To provide guidance in selecting and evaluating interpretation methods, we introduce 3 desiderata: predictive accuracy, descriptive accuracy, and relevancy. Using our framework, we review existing work, grounded in real-world studies which exemplify our desiderata, and suggest directions for future work. Machine-learning models have demonstrated great success in learning complex patterns that enable them to make predictions about unobserved data. In addition to using models for prediction, the ability to interpret what a model has learned is receiving an increasing amount of attention. However, this increased focus has led to considerable confusion about the notion of interpretability. In particular, it is unclear how the wide array of proposed interpretation methods are related and what common concepts can be used to evaluate them. We aim to address these concerns by defining interpretability in the context of machine learning and introducing the predictive, descriptive, relevant (PDR) framework for discussing interpretations. The PDR framework provides 3 overarching desiderata for evaluation: predictive accuracy, descriptive accuracy, and relevancy, with relevancy judged relative to a human audience. Moreover, to help manage the deluge of interpretation methods, we introduce a categorization of existing techniques into model-based and post hoc categories, with subgroups including sparsity, modularity, and simulatability. To demonstrate how practitioners can use the PDR framework to evaluate and understand interpretations, we provide numerous real-world examples. These examples highlight the often underappreciated role played by human audiences in discussions of interpretability. Finally, based on our framework, we discuss limitations of existing methods and directions for future work. We hope that this work will provide a common vocabulary that will make it easier for both practitioners and researchers to discuss and choose from the full range of interpretation methods.",signific recent surg interpret research led confus numer front particular unclear mean interpret select evalu even discus method produc interpret machinelearn model aim clarifi concern defin interpret machin learn construct unifi framework exist method highlight underappreci role play human audienc within framework method organ class model base post hoc provid guidanc select evalu interpret method introduc desideratum predict accuraci descript accuraci relev use framework review exist work ground realworld studi exemplifi desideratum suggest direct futur work machinelearn model demonstr great success learn complex pattern enabl make predict unobserv data addit use model predict abil interpret model learn receiv increas amount attent howev increas focu led consider confus notion interpret particular unclear wide array propos interpret method relat common concept use evalu aim address concern defin interpret context machin learn introduc predict descript relev pdr framework discus interpret pdr framework provid overarch desideratum evalu predict accuraci descript accuraci relev relev judg rel human audienc moreov help manag delug interpret method introduc categor exist techniqu modelbas post hoc categori subgroup includ sparsiti modular simulat demonstr practition use pdr framework evalu understand interpret provid numer realworld exampl exampl highlight often underappreci role play human audienc discus interpret final base framework discus limit exist method direct futur work hope work provid common vocabulari make easier practition research discus choos full rang interpret method,2019,1323
MXNet: A Flexible and Efficient Machine Learning Library for Heterogeneous Distributed Systems,"MXNet is a multi-language machine learning (ML) library to ease the development of ML algorithms, especially for deep neural networks. Embedded in the host language, it blends declarative symbolic expression with imperative tensor computation. It offers auto differentiation to derive gradients. MXNet is computation and memory efficient and runs on various heterogeneous systems, ranging from mobile devices to distributed GPU clusters. 
This paper describes both the API design and the system implementation of MXNet, and explains how embedding of both symbolic expression and tensor operation is handled in a unified fashion. Our preliminary experiments reveal promising results on large scale deep neural network applications using multiple GPU machines.",mxnet multilanguag machin learn ml librari ea develop ml algorithm especi deep neural network embed host languag blend declar symbol express imper tensor comput offer auto differenti deriv gradient mxnet comput memori effici run variou heterogen system rang mobil devic distribut gpu cluster paper describ api design system implement mxnet explain embed symbol express tensor oper handl unifi fashion preliminari experi reveal promis result larg scale deep neural network applic use multipl gpu machin,2015,2213
Applications of machine learning in drug discovery and development,,,2019,1623
Recent advances and applications of machine learning in solid-state materials science,,,2019,1564
"Predicting the Future - Big Data, Machine Learning, and Clinical Medicine.","The algorithms of machine learning, which can sift through vast numbers of variables looking for combinations that reliably predict outcomes, will improve prognosis, displace much of the work of radiologists and anatomical pathologists, and improve diagnostic accuracy.",algorithm machin learn sift vast number variabl look combin reliabl predict outcom improv prognosi displac much work radiologist anatom pathologist improv diagnost accuraci,2016,2125
Machine Learning Interpretability: A Survey on Methods and Metrics,"Machine learning systems are becoming increasingly ubiquitous. These systems’s adoption has been expanding, accelerating the shift towards a more algorithmic society, meaning that algorithmically informed decisions have greater potential for significant social impact. However, most of these accurate decision support systems remain complex black boxes, meaning their internal logic and inner workings are hidden to the user and even experts cannot fully understand the rationale behind their predictions. Moreover, new regulations and highly regulated domains have made the audit and verifiability of decisions mandatory, increasing the demand for the ability to question, understand, and trust machine learning systems, for which interpretability is indispensable. The research community has recognized this interpretability problem and focused on developing both interpretable models and explanation methods over the past few years. However, the emergence of these methods shows there is no consensus on how to assess the explanation quality. Which are the most suitable metrics to assess the quality of an explanation? The aim of this article is to provide a review of the current state of the research field on machine learning interpretability while focusing on the societal impact and on the developed methods and metrics. Furthermore, a complete literature review is presented in order to identify future directions of work on this field.",machin learn system becom increasingli ubiquit systemss adopt expand acceler shift toward algorithm societi mean algorithm inform decis greater potenti signific social impact howev accur decis support system remain complex black box mean intern logic inner work hidden user even expert cannot fulli understand rational behind predict moreov new regul highli regul domain made audit verifi decis mandatori increas demand abil question understand trust machin learn system interpret indispens research commun recogn interpret problem focus develop interpret model explan method past year howev emerg method show consensu assess explan qualiti suitabl metric assess qualiti explan aim articl provid review current state research field machin learn interpret focus societ impact develop method metric furthermor complet literatur review present order identifi futur direct work field,2019,1205
Swarm Learning for decentralized and confidential clinical machine learning,,,2021,551
A survey on missing data in machine learning,,,2021,477
Machine Learning in Agriculture: A Comprehensive Updated Review,"The digital transformation of agriculture has evolved various aspects of management into artificial intelligent systems for the sake of making value from the ever-increasing data originated from numerous sources. A subset of artificial intelligence, namely machine learning, has a considerable potential to handle numerous challenges in the establishment of knowledge-based farming systems. The present study aims at shedding light on machine learning in agriculture by thoroughly reviewing the recent scholarly literature based on keywords’ combinations of “machine learning” along with “crop management”, “water management”, “soil management”, and “livestock management”, and in accordance with PRISMA guidelines. Only journal papers were considered eligible that were published within 2018–2020. The results indicated that this topic pertains to different disciplines that favour convergence research at the international level. Furthermore, crop management was observed to be at the centre of attention. A plethora of machine learning algorithms were used, with those belonging to Artificial Neural Networks being more efficient. In addition, maize and wheat as well as cattle and sheep were the most investigated crops and animals, respectively. Finally, a variety of sensors, attached on satellites and unmanned ground and aerial vehicles, have been utilized as a means of getting reliable input data for the data analyses. It is anticipated that this study will constitute a beneficial guide to all stakeholders towards enhancing awareness of the potential advantages of using machine learning in agriculture and contributing to a more systematic research on this topic.",digit transform agricultur evolv variou aspect manag artifici intellig system sake make valu everincreas data origin numer sourc subset artifici intellig name machin learn consider potenti handl numer challeng establish knowledgebas farm system present studi aim shed light machin learn agricultur thoroughli review recent scholarli literatur base keyword combin machin learn along crop manag water manag soil manag livestock manag accord prisma guidelin journal paper consid elig publish within result indic topic pertain differ disciplin favour converg research intern level furthermor crop manag observ centr attent plethora machin learn algorithm use belong artifici neural network effici addit maiz wheat well cattl sheep investig crop anim respect final varieti sensor attach satellit unman ground aerial vehicl util mean get reliabl input data data analys anticip studi constitut benefici guid stakehold toward enhanc awar potenti advantag use machin learn agricultur contribut systemat research topic,2021,364
Perspectives in machine learning for wildlife conservation,,,2021,350
Machine learning and the physical sciences,"Machine learning (ML) encompasses a broad range of algorithms and modeling tools used for a vast array of data processing tasks, which has entered most scientific disciplines in recent years. This article reviews in a selective way the recent research on the interface between machine learning and the physical sciences. This includes conceptual developments in ML motivated by physical insights, applications of machine learning techniques to several domains in physics, and cross fertilization between the two fields. After giving a basic notion of machine learning methods and principles, examples are described of how statistical physics is used to understand methods in ML. This review then describes applications of ML methods in particle physics and cosmology, quantum many-body physics, quantum computing, and chemical and material physics. Research and development into novel computing architectures aimed at accelerating ML are also highlighted. Each of the sections describe recent successes as well as domain-specific methodology and challenges.",machin learn ml encompass broad rang algorithm model tool use vast array data process task enter scientif disciplin recent year articl review select way recent research interfac machin learn physic scienc includ conceptu develop ml motiv physic insight applic machin learn techniqu sever domain physic cross fertil two field give basic notion machin learn method principl exampl describ statist physic use understand method ml review describ applic ml method particl physic cosmolog quantum manybodi physic quantum comput chemic materi physic research develop novel comput architectur aim acceler ml also highlight section describ recent success well domainspecif methodolog challeng,2019,1418
Combining Machine Learning and Computational Chemistry for Predictive Insights Into Chemical Systems,"Machine learning models are poised to make a transformative impact on chemical sciences by dramatically accelerating computational algorithms and amplifying insights available from computational chemistry methods. However, achieving this requires a confluence and coaction of expertise in computer science and physical sciences. This Review is written for new and experienced researchers working at the intersection of both fields. We first provide concise tutorials of computational chemistry and machine learning methods, showing how insights involving both can be achieved. We follow with a critical review of noteworthy applications that demonstrate how computational chemistry and machine learning can be used together to provide insightful (and useful) predictions in molecular and materials modeling, retrosyntheses, catalysis, and drug design.",machin learn model poi make transform impact chemic scienc dramat acceler comput algorithm amplifi insight avail comput chemistri method howev achiev requir confluenc coaction expertis comput scienc physic scienc review written new experienc research work intersect field first provid concis tutori comput chemistri machin learn method show insight involv achiev follow critic review noteworthi applic demonstr comput chemistri machin learn use togeth provid insight use predict molecular materi model retrosynthes catalysi drug design,2021,412
Machine Learning Basics,"coined in 1959 by Arthur Samuel [Samuel 1959], Tom Mitchell [Mitchell 1997] provided a more formal definition: “A computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E.” ML has be applied to many real-world problems or tasks, like medical diagno­ sis, robotics, recommendation systems, facial recognition, stock prices prediction, and sentiment analysis, with great success. We can divide ML algorithms into three main categories (see Figure 4.1): Machine Learning Basics",coin arthur samuel samuel tom mitchel mitchel provid formal definit comput program said learn experi e respect task perform measur p perform measur p improv experi e ml appli mani realworld problem task like medic diagno si robot recommend system facial recognit stock price predict sentiment analysi great success divid ml algorithm three main categori see figur machin learn basic,2021,268
Common pitfalls and recommendations for using machine learning to detect and prognosticate for COVID-19 using chest radiographs and CT scans,,,2020,746
Enhancing computational fluid dynamics with machine learning,,,2021,308
Machine Learning in Drug Discovery: A Review,,,2021,345
Machine learning pipeline for battery state-of-health estimation,,,2021,333
Fairness in Machine Learning,,,2020,475
"Reproducible, interactive, scalable and extensible microbiome data science using QIIME 2",,,2019,13186
High-Dimensional Probability: An Introduction with Applications in Data Science,"© 2018, Cambridge University Press Let us summarize our findings. A random projection of a set T in R n onto an m-dimensional subspace approximately preserves the geometry of T if m ⪆ d ( T ) . For...",cambridg univers press let u summar find random project set r n onto mdimension subspac approxim preserv geometri,2020,2798
"BOOK REVIEW - Data Science and Big Data Analytics: Discovering, Analyzing, Visualizing and Presenting Data","Data Science and Big Data Analytics is about harnessing the power of data for new insights. The book covers the breadth of activities and methods and tools that Data Scientists use. The content focuses on concepts, principles and practical applications that are applicable to any industry and technology environment, and the learning is supported and explained with examples that you can replicate using open-source software. This book will help you:",data scienc big data analyt har power data new insight book cover breadth activ method tool data scientist use content focus concept principl practic applic applic industri technolog environ learn support explain exampl replic use opensourc softwar book help,2024,74
Data Interpreter: An LLM Agent For Data Science,"Large Language Model (LLM)-based agents have shown effectiveness across many applications. However, their use in data science scenarios requiring solving long-term interconnected tasks, dynamic data adjustments and domain expertise remains challenging. Previous approaches primarily focus on individual tasks, making it difficult to assess the complete data science workflow. Moreover, they struggle to handle real-time changes in intermediate data and fail to adapt dynamically to evolving task dependencies inherent to data science problems. In this paper, we present Data Interpreter, an LLM-based agent designed to automatically solve various data science problems end-to-end. Our Data Interpreter incorporates two key modules: 1) Hierarchical Graph Modeling, which breaks down complex problems into manageable subproblems, enabling dynamic node generation and graph optimization; and 2) Programmable Node Generation, a technique that refines and verifies each subproblem to iteratively improve code generation results and robustness. Extensive experiments consistently demonstrate the superiority of Data Interpreter. On InfiAgent-DABench, it achieves a 25% performance boost, raising accuracy from 75.9% to 94.9%. For machine learning and open-ended tasks, it improves performance from 88% to 95%, and from 60% to 97%, respectively. Moreover, on the MATH dataset, Data Interpreter achieves remarkable performance with a 26% improvement compared to state-of-the-art baselines. The code is available at https://github.com/geekan/MetaGPT.",larg languag model llmbase agent shown effect across mani applic howev use data scienc scenario requir solv longterm interconnect task dynam data adjust domain expertis remain challeng previou approach primarili focu individu task make difficult assess complet data scienc workflow moreov struggl handl realtim chang intermedi data fail adapt dynam evolv task depend inher data scienc problem paper present data interpret llmbase agent design automat solv variou data scienc problem endtoend data interpret incorpor two key modul hierarch graph model break complex problem manag subproblem enabl dynam node gener graph optim programm node gener techniqu refin verifi subproblem iter improv code gener result robust extens experi consist demonstr superior data interpret infiagentdabench achiev perform boost rais accuraci machin learn openend task improv perform respect moreov math dataset data interpret achiev remark perform improv compar stateoftheart baselin code avail httpsgithubcomgeekanmetagpt,2024,35
The Role of ChatGPT in Data Science: How AI-Assisted Conversational Interfaces Are Revolutionizing the Field,"ChatGPT, a conversational AI interface that utilizes natural language processing and machine learning algorithms, is taking the world by storm and is the buzzword across many sectors today. Given the likely impact of this model on data science, through this perspective article, we seek to provide an overview of the potential opportunities and challenges associated with using ChatGPT in data science, provide readers with a snapshot of its advantages, and stimulate interest in its use for data science projects. The paper discusses how ChatGPT can assist data scientists in automating various aspects of their workflow, including data cleaning and preprocessing, model training, and result interpretation. It also highlights how ChatGPT has the potential to provide new insights and improve decision-making processes by analyzing unstructured data. We then examine the advantages of ChatGPT’s architecture, including its ability to be fine-tuned for a wide range of language-related tasks and generate synthetic data. Limitations and issues are also addressed, particularly around concerns about bias and plagiarism when using ChatGPT. Overall, the paper concludes that the benefits outweigh the costs and ChatGPT has the potential to greatly enhance the productivity and accuracy of data science workflows and is likely to become an increasingly important tool for intelligence augmentation in the field of data science. ChatGPT can assist with a wide range of natural language processing tasks in data science, including language translation, sentiment analysis, and text classification. However, while ChatGPT can save time and resources compared to training a model from scratch, and can be fine-tuned for specific use cases, it may not perform well on certain tasks if it has not been specifically trained for them. Additionally, the output of ChatGPT may be difficult to interpret, which could pose challenges for decision-making in data science applications.",chatgpt convers ai interfac util natur languag process machin learn algorithm take world storm buzzword across mani sector today given like impact model data scienc perspect articl seek provid overview potenti opportun challeng associ use chatgpt data scienc provid reader snapshot advantag stimul interest use data scienc project paper discus chatgpt assist data scientist autom variou aspect workflow includ data clean preprocess model train result interpret also highlight chatgpt potenti provid new insight improv decisionmak process analyz unstructur data examin advantag chatgpt architectur includ abil finetun wide rang languagerel task gener synthet data limit issu also address particularli around concern bia plagiar use chatgpt overal paper conclud benefit outweigh cost chatgpt potenti greatli enhanc product accuraci data scienc workflow like becom increasingli import tool intellig augment field data scienc chatgpt assist wide rang natur languag process task data scienc includ languag translat sentiment analysi text classif howev chatgpt save time resourc compar train model scratch finetun specif use case may perform well certain task specif train addit output chatgpt may difficult interpret could pose challeng decisionmak data scienc applic,2023,214
Spatial Data Science,,,2023,243
DS-1000: A Natural and Reliable Benchmark for Data Science Code Generation,"We introduce DS-1000, a code generation benchmark with a thousand data science problems spanning seven Python libraries, such as NumPy and Pandas. Compared to prior works, DS-1000 incorporates three core features. First, our problems reflect diverse, realistic, and practical use cases since we collected them from StackOverflow. Second, our automatic evaluation is highly specific (reliable) -- across all Codex-002-predicted solutions that our evaluation accept, only 1.8% of them are incorrect; we achieve this with multi-criteria metrics, checking both functional correctness by running test cases and surface-form constraints by restricting API usages or keywords. Finally, we proactively defend against memorization by slightly modifying our problems to be different from the original StackOverflow source; consequently, models cannot answer them correctly by memorizing the solutions from pre-training. The current best public system (Codex-002) achieves 43.3% accuracy, leaving ample room for improvement. We release our benchmark at https://ds1000-code-gen.github.io.",introduc d code gener benchmark thousand data scienc problem span seven python librari numpi panda compar prior work d incorpor three core featur first problem reflect diver realist practic use case sinc collect stackoverflow second automat evalu highli specif reliabl across codexpredict solut evalu accept incorrect achiev multicriteria metric check function correct run test case surfaceform constraint restrict api usag keyword final proactiv defend memor slightli modifi problem differ origin stackoverflow sourc consequ model cannot answer correctli memor solut pretrain current best public system codex achiev accuraci leav ampl room improv releas benchmark httpsdscodegengithubio,2022,241
A New Era of Learning: Considerations for ChatGPT as a Tool to Enhance Statistics and Data Science Education,"Abstract ChatGPT is one of many generative artificial intelligence (AI) tools that has emerged recently, creating controversy in the education community with concerns about its potential to be used for plagiarism and to undermine students’ ability to think independently. Recent publications have criticized the use of ChatGPT and other generative AI tools in the classroom, with little focus on the potential benefits. This article focuses on the potential of ChatGPT as an educational tool for statistics and data science. It encourages readers to consider the history of trepidation surrounding introducing new technology in the classroom, such as the calculator. We explore the possibility of leveraging ChatGPT’s capabilities in statistics and data science education, providing examples of how ChatGPT can aid in developing course materials and suggestions for how educators can prompt students to interact with ChatGPT responsibly. As educators, we can guide the use of generative AI tools in statistics and data science classrooms so that students and educators can leverage the benefits of this technology.",abstract chatgpt one mani gener artifici intellig ai tool emerg recent creat controversi educ commun concern potenti use plagiar undermin student abil think independ recent public critic use chatgpt gener ai tool classroom littl focu potenti benefit articl focus potenti chatgpt educ tool statist data scienc encourag reader consid histori trepid surround introduc new technolog classroom calcul explor possibl leverag chatgpt capabl statist data scienc educ provid exampl chatgpt aid develop cours materi suggest educ prompt student interact chatgpt respons educ guid use gener ai tool statist data scienc classroom student educ leverag benefit technolog,2023,45
What is Data Science?,"The Communications website, https://cacm.acm.org, features more than a dozen bloggers in the BLOG@CACM community. In each issue of Communications, we'll publish selected posts or excerpts. twitter Follow us on Twitter at http://twitter.com/blogCACM https://cacm.acm.org/blogs/blog-cacm Koby Mike and Orit Hazzan consider why multiple definitions are needed to pin down data science.",commun websit httpscacmacmorg featur dozen blogger blogcacm commun issu commun well publish select post excerpt twitter follow u twitter httptwittercomblogcacm httpscacmacmorgblogsblogcacm kobi mike orit hazzan consid multipl definit need pin data scienc,2023,31
ChatGPT for Teaching and Learning: An Experience from Data Science Education,"ChatGPT, an implementation and application of large language models, has gained significant popularity since its initial release. Researchers have been exploring ways to harness the practical benefits of ChatGPT in real-world scenarios. Educational researchers have investigated its potential in various subjects, e.g., programming, mathematics, finance, clinical decision support, etc. However, there has been limited attention given to its application in data science education. This paper aims to bridge that gap by utilizing ChatGPT in a data science course, gathering perspectives from students, and presenting our experiences and feedback on using ChatGPT for teaching and learning in data science education. The findings not only distinguish data science education from other disciplines but also uncover new opportunities and challenges associated with incorporating ChatGPT into the data science curriculum.",chatgpt implement applic larg languag model gain signific popular sinc initi releas research explor way har practic benefit chatgpt realworld scenario educ research investig potenti variou subject eg program mathemat financ clinic decis support etc howev limit attent given applic data scienc educ paper aim bridg gap util chatgpt data scienc cours gather perspect student present experi feedback use chatgpt teach learn data scienc educ find distinguish data scienc educ disciplin also uncov new opportun challeng associ incorpor chatgpt data scienc curriculum,2023,30
Data Science and Applications,"This paper investigates the significance of data science as an indispensable instrument for decision-making across multiple domains. The study examines the history, concepts, methods, and applications of data science, as well as its impact on numerous industries, such as artificial intelligence, manufacturing, fintech, government, Astro-informatics, e-commerce, education, and biotechnology. ERP (Enterprise Resource Planning) software was first developed by SAP in the 1960s, with modern ERP systems emerging in the 1990s, according to the research. The paper highlights the significance of data science in enhancing the functionality of enterprise resource planning (ERP) systems, with AI-based solutions such as those offered by MahaaAi and other firms automating human tasks, chat-based ERP applications, and virtual assistant assistants support to avoid human efforts. The conclusion of the study emphasizes the significant benefits of data science in the ERP industry, including self-service analytics, predictions, and prescriptive analysis.",paper investig signific data scienc indispens instrument decisionmak across multipl domain studi examin histori concept method applic data scienc well impact numer industri artifici intellig manufactur fintech govern astroinformat ecommerc educ biotechnolog erp enterpris resourc plan softwar first develop sap modern erp system emerg accord research paper highlight signific data scienc enhanc function enterpris resourc plan erp system aibas solut offer mahaaai firm autom human task chatbas erp applic virtual assist assist support avoid human effort conclus studi emphas signific benefit data scienc erp industri includ selfservic analyt predict prescript analysi,2023,18
The R Language: An Engine for Bioinformatics and Data Science,"The R programming language is approaching its 30th birthday, and in the last three decades it has achieved a prominent role in statistics, bioinformatics, and data science in general. It currently ranks among the top 10 most popular languages worldwide, and its community has produced tens of thousands of extensions and packages, with scopes ranging from machine learning to transcriptome data analysis. In this review, we provide an historical chronicle of how R became what it is today, describing all its current features and capabilities. We also illustrate the major tools of R, such as the current R editors and integrated development environments (IDEs), the R Shiny web server, the R methods for machine learning, and its relationship with other programming languages. We also discuss the role of R in science in general as a driver for reproducibility. Overall, we hope to provide both a complete snapshot of R today and a practical compendium of the major features and applications of this programming language.",r program languag approach th birthday last three decad achiev promin role statist bioinformat data scienc gener current rank among top popular languag worldwid commun produc ten thousand extens packag scope rang machin learn transcriptom data analysi review provid histor chronicl r becam today describ current featur capabl also illustr major tool r current r editor integr develop environ ide r shini web server r method machin learn relationship program languag also discus role r scienc gener driver reproduc overal hope provid complet snapshot r today practic compendium major featur applic program languag,2022,105
Foundations of Data Science,"Computer science as an academic discipline began in the 1960’s. Emphasis was on programming languages, compilers, operating systems, and the mathematical theory that supported these areas. Courses in theoretical computer science covered finite automata, regular expressions, context-free languages, and computability. In the 1970’s, the study of algorithms was added as an important component of theory. The emphasis was on making computers useful. Today, a fundamental change is taking place and the focus is more on applications. There are many reasons for this change. The merging of computing and communications has played an important role. The enhanced ability to observe, collect, and store data in the natural sciences, in commerce, and in other fields calls for a change in our understanding of data and how to handle it in the modern setting. The emergence of the web and social networks as central aspects of daily life presents both opportunities and challenges for theory.",comput scienc academ disciplin began emphasi program languag compil oper system mathemat theori support area cours theoret comput scienc cover finit automaton regular express contextfre languag comput studi algorithm ad import compon theori emphasi make comput use today fundament chang take place focu applic mani reason chang merg comput commun play import role enhanc abil observ collect store data natur scienc commerc field call chang understand data handl modern set emerg web social network central aspect daili life present opportun challeng theori,2020,333
"Smart Health Intelligent Healthcare Systems in the Metaverse, Artificial Intelligence, and Data Science Era","In recent decades, healthcare organizations around the world have increasingly appreciated the value of information technologies for a variety of applications. Three of the new technological advancements that are impacting smart health are metaverse, artificial intelligence (AI), and data science. The metaverse is the intersection of three major technologies — AI, augmented reality (AR), and virtual reality (VR). Metaverse provides new possibilities and potential that are still emerging. The increased work efficiency enabled by artificial intelligence and data science in hospitals not only improves patient care but also cuts costs and workload for healthcare providers.The availability of big data enables data scientists to use the data for descriptive, predictive, and prescriptive analytics. This article reviews multiple case studies and the literature on AI and data science applications in hospital administration. The article also presents unresolved research questions and challenges in the applications of the metaverse, AI, and data science in the smart health context.",recent decad healthcar organ around world increasingli appreci valu inform technolog varieti applic three new technolog advanc impact smart health metavers artifici intellig ai data scienc metavers intersect three major technolog ai augment realiti ar virtual realiti vr metavers provid new possibl potenti still emerg increas work effici enabl artifici intellig data scienc hospit improv patient care also cut cost workload healthcar providersth avail big data enabl data scientist use data descript predict prescript analyt articl review multipl case studi literatur ai data scienc applic hospit administr articl also present unresolv research question challeng applic metavers ai data scienc smart health context,2022,52
The case for data science in experimental chemistry: examples and recommendations,,,2022,46
Training and Evaluating a Jupyter Notebook Data Science Assistant,"We study the feasibility of a Data Science assistant powered by a sequence-to-sequence transformer by training a new model JuPyT5 on all publicly available Jupyter Notebook GitHub repositories and developing a new metric: Data Science Problems (DSP). DSP is a collection of 1119 problems curated from 306 pedagogical notebooks with 92 dataset dependencies, natural language and Markdown problem descriptions, and assert-based unit tests. These notebooks were designed to test university students' mastery of various Python implementations of Math and Data Science, and we now leverage them to study the ability of JuPyT5 to understand and pass the tests. We analyze the content of DSP, validate its quality, and we find that given 100 sampling attempts JuPyT5 is able to solve 77.5\% of the DSP problems. We further present various ablation and statistical analyses and compare DSP to other recent natural language to code benchmarks.",studi feasibl data scienc assist power sequencetosequ transform train new model jupyt publicli avail jupyt notebook github repositori develop new metric data scienc problem dsp dsp collect problem curat pedagog notebook dataset depend natur languag markdown problem descript assertbas unit test notebook design test univers student masteri variou python implement math data scienc leverag studi abil jupyt understand pas test analyz content dsp valid qualiti find given sampl attempt jupyt abl solv dsp problem present variou ablat statist analys compar dsp recent natur languag code benchmark,2022,39
Eleven grand challenges in single-cell data science,,,2020,885
Epistemic injustice and data science technologies,,,2022,32
Data Science,,,2022,17
"Machine Learning in Python: Main developments and technology trends in data science, machine learning, and artificial intelligence","Smarter applications are making better use of the insights gleaned from data, having an impact on every industry and research discipline. At the core of this revolution lies the tools and the methods that are driving it, from processing the massive piles of data generated each day to learning from and taking useful action. Deep neural networks, along with advancements in classical machine learning and scalable general-purpose graphics processing unit (GPU) computing, have become critical components of artificial intelligence, enabling many of these astounding breakthroughs and lowering the barrier to adoption. Python continues to be the most preferred language for scientific computing, data science, and machine learning, boosting both performance and productivity by enabling the use of low-level libraries and clean high-level APIs. This survey offers insight into the field of machine learning with Python, taking a tour through important topics to identify some of the core hardware and software paradigms that have enabled it. We cover widely-used libraries and concepts, collected together for holistic comparison, with the goal of educating the reader and driving the field of Python machine learning forward.",smarter applic make better use insight glean data impact everi industri research disciplin core revolut lie tool method drive process massiv pile data gener day learn take use action deep neural network along advanc classic machin learn scalabl generalpurpos graphic process unit gpu comput becom critic compon artifici intellig enabl mani astound breakthrough lower barrier adopt python continu prefer languag scientif comput data scienc machin learn boost perform product enabl use lowlevel librari clean highlevel api survey offer insight field machin learn python take tour import topic identifi core hardwar softwar paradigm enabl cover widelyus librari concept collect togeth holist comparison goal educ reader drive field python machin learn forward,2020,431
Cybersecurity data science: an overview from machine learning perspective,,,2020,384
CRISP-DM Twenty Years Later: From Data Mining Processes to Data Science Trajectories,"CRISP-DM(CRoss-Industry Standard Process for Data Mining) has its origins in the second half of the nineties and is thus about two decades old. According to many surveys and user polls it is still the de facto standard for developing data mining and knowledge discovery projects. However, undoubtedly the field has moved on considerably in twenty years, with data science now the leading term being favoured over data mining. In this paper we investigate whether, and in what contexts, CRISP-DM is still fit for purpose for data science projects. We argue that if the project is goal-directed and process-driven the process model view still largely holds. On the other hand, when data science projects become more exploratory the paths that the project can take become more varied, and a more flexible model is called for. We suggest what the outlines of such a trajectory-based model might look like and how it can be used to categorise data science projects (goal-directed, exploratory or data management). We examine seven real-life exemplars where exploratory activities play an important role and compare them against 51 use cases extracted from the NIST Big Data Public Working Group. We anticipate this categorisation can help project planning in terms of time and cost characteristics.",crispdmcrossindustri standard process data mine origin second half nineti thu two decad old accord mani survey user poll still de facto standard develop data mine knowledg discoveri project howev undoubtedli field move consider twenti year data scienc lead term favour data mine paper investig whether context crispdm still fit purpos data scienc project argu project goaldirect processdriven process model view still larg hold hand data scienc project becom exploratori path project take becom vari flexibl model call suggest outlin trajectorybas model might look like use categoris data scienc project goaldirect exploratori data manag examin seven reallif exemplar exploratori activ play import role compar use case extract nist big data public work group anticip categoris help project plan term time cost characterist,2021,218
Big-Data Science in Porous Materials: Materials Genomics and Machine Learning,"By combining metal nodes with organic linkers we can potentially synthesize millions of possible metal–organic frameworks (MOFs). The fact that we have so many materials opens many exciting avenues but also create new challenges. We simply have too many materials to be processed using conventional, brute force, methods. In this review, we show that having so many materials allows us to use big-data methods as a powerful technique to study these materials and to discover complex correlations. The first part of the review gives an introduction to the principles of big-data science. We show how to select appropriate training sets, survey approaches that are used to represent these materials in feature space, and review different learning architectures, as well as evaluation and interpretation strategies. In the second part, we review how the different approaches of machine learning have been applied to porous materials. In particular, we discuss applications in the field of gas storage and separation, the stability of these materials, their electronic properties, and their synthesis. Given the increasing interest of the scientific community in machine learning, we expect this list to rapidly expand in the coming years.",combin metal node organ linker potenti synthes million possibl metalorgan framework mof fact mani materi open mani excit avenu also creat new challeng simpli mani materi process use convent brute forc method review show mani materi allow u use bigdata method power techniqu studi materi discov complex correl first part review give introduct principl bigdata scienc show select appropri train set survey approach use repres materi featur space review differ learn architectur well evalu interpret strategi second part review differ approach machin learn appli porou materi particular discus applic field ga storag separ stabil materi electron properti synthesi given increas interest scientif commun machin learn expect list rapidli expand come year,2020,327
"QIIME 2: Reproducible, interactive, scalable, and extensible microbiome data science","We present QIIME 2, an open-source microbiome data science platform accessible to users spanning the microbiome research ecosystem, from scientists and engineers to clinicians and policy makers. QIIME 2 provides new features that will drive the next generation of microbiome research. These include interactive spatial and temporal analysis and visualization tools, support for metabolomics and shotgun metagenomics analysis, and automated data provenance tracking to ensure reproducible, transparent microbiome data science.",present qiim opensourc microbiom data scienc platform access user span microbiom research ecosystem scientist engin clinician polici maker qiim provid new featur drive next gener microbiom research includ interact spatial tempor analysi visual tool support metabolom shotgun metagenom analysi autom data proven track ensur reproduc transpar microbiom data scienc,2018,1066
Spectral Methods for Data Science: A Statistical Perspective,"Spectral methods have emerged as a simple yet surprisingly effective approach for extracting information from massive, noisy and incomplete data. In a nutshell, spectral methods refer to a collection of algorithms built upon the eigenvalues (resp. singular values) and eigenvectors (resp. singular vectors) of some properly designed matrices constructed from data. A diverse array of applications have been found in machine learning, data science, and signal processing. Due to their simplicity and effectiveness, spectral methods are not only used as a stand-alone estimator, but also frequently employed to initialize other more sophisticated algorithms to improve performance. 
While the studies of spectral methods can be traced back to classical matrix perturbation theory and methods of moments, the past decade has witnessed tremendous theoretical advances in demystifying their efficacy through the lens of statistical modeling, with the aid of non-asymptotic random matrix theory. This monograph aims to present a systematic, comprehensive, yet accessible introduction to spectral methods from a modern statistical perspective, highlighting their algorithmic implications in diverse large-scale applications. In particular, our exposition gravitates around several central questions that span various applications: how to characterize the sample efficiency of spectral methods in reaching a target level of statistical accuracy, and how to assess their stability in the face of random noise, missing data, and adversarial corruptions? In addition to conventional $\ell_2$ perturbation analysis, we present a systematic $\ell_{\infty}$ and $\ell_{2,\infty}$ perturbation theory for eigenspace and singular subspaces, which has only recently become available owing to a powerful ""leave-one-out"" analysis framework.",spectral method emerg simpl yet surprisingli effect approach extract inform massiv noisi incomplet data nutshel spectral method refer collect algorithm built upon eigenvalu resp singular valu eigenvector resp singular vector properli design matric construct data diver array applic found machin learn data scienc signal process due simplic effect spectral method use standalon estim also frequent employ initi sophist algorithm improv perform studi spectral method trace back classic matrix perturb theori method moment past decad wit tremend theoret advanc demystifi efficaci len statist model aid nonasymptot random matrix theori monograph aim present systemat comprehens yet access introduct spectral method modern statist perspect highlight algorithm implic diver largescal applic particular exposit gravit around sever central question span variou applic character sampl effici spectral method reach target level statist accuraci assess stabil face random nois miss data adversari corrupt addit convent ell perturb analysi present systemat ellinfti ellinfti perturb theori eigenspac singular subspac recent becom avail owe power leaveoneout analysi framework,2020,148
Diversifying the genomic data science research community,"Over the past 20 years, the explosion of genomic data collection and the cloud computing revolution have made computational and data science research accessible to anyone with a web browser and an internet connection. However, students at institutions with limited resources have received relatively little exposure to curricula or professional development opportunities that lead to careers in genomic data science. To broaden participation in genomics research, the scientific community needs to support these programs in local education and research at underserved institutions (UIs). These include community colleges, historically Black colleges and universities, Hispanic-serving institutions, and tribal colleges and universities that support ethnically, racially, and socioeconomically underrepresented students in the United States. We have formed the Genomic Data Science Community Network to support students, faculty, and their networks to identify opportunities and broaden access to genomic data science. These opportunities include expanding access to infrastructure and data, providing UI faculty development opportunities, strengthening collaborations among faculty, recognizing UI teaching and research excellence, fostering student awareness, developing modular and open-source resources, expanding course-based undergraduate research experiences (CUREs), building curriculum, supporting student professional development and research, and removing financial barriers through funding programs and collaborator support.",past year explos genom data collect cloud comput revolut made comput data scienc research access anyon web browser internet connect howev student institut limit resourc receiv rel littl exposur curriculum profession develop opportun lead career genom data scienc broaden particip genom research scientif commun need support program local educ research underserv institut ui includ commun colleg histor black colleg univers hispanicserv institut tribal colleg univers support ethnic racial socioeconom underrepres student unit state form genom data scienc commun network support student faculti network identifi opportun broaden access genom data scienc opportun includ expand access infrastructur data provid ui faculti develop opportun strengthen collabor among faculti recogn ui teach research excel foster student awar develop modular opensourc resourc expand coursebas undergradu research experi cure build curriculum support student profession develop research remov financi barrier fund program collabor support,2022,6
Data science approach to stock prices forecasting in Indonesia during Covid-19 using Long Short-Term Memory (LSTM),,,2021,77
AutoDS: Towards Human-Centered Automation of Data Science,"Data science (DS) projects often follow a lifecycle that consists of laborious tasks for data scientists and domain experts (e.g., data exploration, model training, etc.). Only till recently, machine learning(ML) researchers have developed promising automation techniques to aid data workers in these tasks. This paper introduces AutoDS, an automated machine learning (AutoML) system that aims to leverage the latest ML automation techniques to support data science projects. Data workers only need to upload their dataset, then the system can automatically suggest ML configurations, preprocess data, select algorithm, and train the model. These suggestions are presented to the user via a web-based graphical user interface and a notebook-based programming user interface. Our goal is to offer a systematic investigation of user interaction and perceptions of using an AutoDS system in solving a data science task. We studied AutoDS with 30 professional data scientists, where one group used AutoDS, and the other did not, to complete a data science project. As expected, AutoDS improves productivity; Yet surprisingly, we find that the models produced by the AutoDS group have higher quality and less errors, but lower human confidence scores. We reflect on the findings by presenting design implications for incorporating automation techniques into human work in the data science lifecycle.",data scienc d project often follow lifecycl consist labori task data scientist domain expert eg data explor model train etc till recent machin learningml research develop promis autom techniqu aid data worker task paper introduc autod autom machin learn automl system aim leverag latest ml autom techniqu support data scienc project data worker need upload dataset system automat suggest ml configur preprocess data select algorithm train model suggest present user via webbas graphic user interfac notebookbas program user interfac goal offer systemat investig user interact percept use autod system solv data scienc task studi autod profession data scientist one group use autod complet data scienc project expect autod improv product yet surprisingli find model produc autod group higher qualiti less error lower human confid score reflect find present design implic incorpor autom techniqu human work data scienc lifecycl,2021,65
"The role of data science in healthcare advancements: applications, benefits, and future prospects",,,2021,78
Nucleus segmentation across imaging experiments: the 2018 Data Science Bowl,,,2019,574
"How do Data Science Workers Collaborate? Roles, Workflows, and Tools","Today, the prominence of data science within organizations has given rise to teams of data science workers collaborating on extracting insights from data, as opposed to individual data scientists working alone. However, we still lack a deep understanding of how data science workers collaborate in practice. In this work, we conducted an online survey with 183 participants who work in various aspects of data science. We focused on their reported interactions with each other (e.g., managers with engineers) and with different tools (e.g., Jupyter Notebook). We found that data science teams are extremely collaborative and work with a variety of stakeholders and tools during the six common steps of a data science workflow (e.g., clean data and train model). We also found that the collaborative practices workers employ, such as documentation, vary according to the kinds of tools they use. Based on these findings, we discuss design implications for supporting data science team collaborations and future research directions.",today promin data scienc within organ given rise team data scienc worker collabor extract insight data oppos individu data scientist work alon howev still lack deep understand data scienc worker collabor practic work conduct onlin survey particip work variou aspect data scienc focus report interact eg manag engin differ tool eg jupyt notebook found data scienc team extrem collabor work varieti stakehold tool six common step data scienc workflow eg clean data train model also found collabor practic worker employ document vari accord kind tool use base find discus design implic support data scienc team collabor futur research direct,2020,238
"The Art and Practice of Data Science Pipelines: A Comprehensive Study of Data Science Pipelines In Theory, In-The-Small, and In-The-Large","Increasingly larger number of software systems today are including data science components for descriptive, predictive, and prescriptive analytics. The collection of data science stages from acquisition, to cleaning/curation, to modeling, and so on are referred to as data science pipelines. To facilitate research and practice on data science pipelines, it is essential to understand their nature. What are the typical stages of a data science pipeline? How are they connected? Do the pipelines differ in the theoretical representations and that in the practice? Today we do not fully understand these architectural characteristics of data science pipelines. In this work, we present a three-pronged comprehensive study to answer this for the state-of-the-art, data science in-the-small, and data science in-the-large, Our study analyzes three datasets: a collection of 71 proposals for data science pipelines and related concepts in theory, a collection of over 105 implementations of curated data science pipelines from Kaggle competitions to understand data science in-the-small, and a collection of 21 mature data science projects from GitHub to understand data science in-the-large. Our study has led to three representations of data science pipelines that capture the essence of our subjects in theory, in-the-small, and in-the-large.",increasingli larger number softwar system today includ data scienc compon descript predict prescript analyt collect data scienc stage acquisit cleaningcur model refer data scienc pipelin facilit research practic data scienc pipelin essenti understand natur typic stage data scienc pipelin connect pipelin differ theoret represent practic today fulli understand architectur characterist data scienc pipelin work present threeprong comprehens studi answer stateoftheart data scienc inthesmal data scienc inthelarg studi analyz three dataset collect propos data scienc pipelin relat concept theori collect implement curat data scienc pipelin kaggl competit understand data scienc inthesmal collect matur data scienc project github understand data scienc inthelarg studi led three represent data scienc pipelin captur essenc subject theori inthesmal inthelarg,2021,50
Leveraging Data Science to Combat COVID-19: A Comprehensive Review,"COVID-19, an infectious disease caused by the SARS-CoV-2 virus, was declared a pandemic by the World Health Organisation (WHO) in March 2020. By mid-August 2020, more than 21 million people have tested positive worldwide. Infections have been growing rapidly and tremendous efforts are being made to fight the disease. In this paper, we attempt to systematise the various COVID-19 research activities leveraging data science, where we define data science broadly to encompass the various methods and tools—including those from artificial intelligence (AI), machine learning (ML), statistics, modeling, simulation, and data visualization—that can be used to store, process, and extract insights from data. In addition to reviewing the rapidly growing body of recent research, we survey public datasets and repositories that can be used for further work to track COVID-19 spread and mitigation strategies. As part of this, we present a bibliometric analysis of the papers produced in this short span of time. Finally, building on these insights, we highlight common challenges and pitfalls observed across the surveyed works. We also created a live resource repository at https://github.com/Data-Science-and-COVID-19/Leveraging-Data-Science-To-Combat-COVID-19-A-Comprehensive-Review that we intend to keep updated with the latest resources including new papers and datasets.",covid infecti diseas caus sarscov viru declar pandem world health organis march midaugust million peopl test posit worldwid infect grow rapidli tremend effort made fight diseas paper attempt systematis variou covid research activ leverag data scienc defin data scienc broadli encompass variou method toolsinclud artifici intellig ai machin learn ml statist model simul data visualizationthat use store process extract insight data addit review rapidli grow bodi recent research survey public dataset repositori use work track covid spread mitig strategi part present bibliometr analysi paper produc short span time final build insight highlight common challeng pitfal observ across survey work also creat live resourc repositori httpsgithubcomdatascienceandcovidleveragingdatasciencetocombatcovidacomprehensivereview intend keep updat latest resourc includ new paper dataset,2020,218
Surgical data science and artificial intelligence for surgical education,"Surgical data science (SDS) aims to improve the quality of interventional healthcare and its value through the capture, organization, analysis, and modeling of procedural data. As data capture has increased and artificial intelligence (AI) has advanced, SDS can help to unlock augmented and automated coaching, feedback, assessment, and decision support in surgery. We review major concepts in SDS and AI as applied to surgical education and surgical oncology.",surgic data scienc sd aim improv qualiti intervent healthcar valu captur organ analysi model procedur data data captur increas artifici intellig ai advanc sd help unlock augment autom coach feedback assess decis support surgeri review major concept sd ai appli surgic educ surgic oncolog,2021,52
Data Science,"Les information détaillées à propos de chaque cours sont disponibles en cliquant sur le code cours. En particulier, l’horaire précis, jour par jour, et les locaux correspondants sont accessibles via la rubrique “Horaire”. Detailed information about each course unit is available by clicking the course code. In particular, the detailed schedule, day by day, and the corresponding classrooms are provided under the “Schedule” sub-title.",le inform dtaill propo de chaqu cour sont dispon en cliquant sur le code cour en particuli lhorair prci jour par jour et le locaux correspond sont access via la rubriqu horair detail inform cours unit avail click cours code particular detail schedul day day correspond classroom provid schedul subtitl,2022,1
Data Science Methodologies: Current Challenges and Future Approaches,,,2021,60
Data Science,,,2022,0
Automating data science,"Given the complexity of data science projects and related demand for human expertise, automation has the potential to transform the data science process.",given complex data scienc project relat demand human expertis autom potenti transform data scienc process,2021,34
The challenges of explainable AI in biomedical data science,,,2021,32
Quantifying causality in data science with quasi-experiments,,,2021,29
"CRISP-DM for Data Science: Strengths, Weaknesses and Potential Next Steps","This paper explores the strengths and weaknesses of CRISP-DM when used for data science projects. The paper then explores what key actions data science teams using CRISP-DM should consider that addresses CRISP-DM’s weaknesses. In brief, CRISP-DM, which is the most popular framework teams use to execute data science projects, provides an easy to understand description of the data science project workflow (i.e., the data science life cycle). However, CRISP-DM’s project phases miss some key aspects of the data science project life cycle. In addition, CRISP-DM’s task-focused approach fails to address how a team should prioritize tasks, and in general, collaborate and communicate. Hence, this paper also describes how CRISP-DM could be combined with a team coordination framework, such as Scrum or Data Driven Scrum, which is a newer collaboration framework developed to address the unique data science coordination challenges.",paper explor strength weak crispdm use data scienc project paper explor key action data scienc team use crispdm consid address crispdm weak brief crispdm popular framework team use execut data scienc project provid easi understand descript data scienc project workflow ie data scienc life cycl howev crispdm project phase miss key aspect data scienc project life cycl addit crispdm taskfocus approach fail address team priorit task gener collabor commun henc paper also describ crispdm could combin team coordin framework scrum data driven scrum newer collabor framework develop address uniqu data scienc coordin challeng,2021,26
Data science: a game changer for science and innovation,,,2021,26
Supervised and Unsupervised Learning for Data Science,,,2020,147
Finding Related Tables in Data Lakes for Interactive Data Science,"Many modern data science applications build on data lakes, schema-agnostic repositories of data files and data products that offer limited organization and management capabilities. There is a need to build data lake search capabilities into data science environments, so scientists and analysts can find tables, schemas, workflows, and datasets useful to their task at hand. We develop search and management solutions for the Jupyter Notebook data science platform, to enable scientists to augment training data, find potential features to extract, clean data, and find joinable or linkable tables. Our core methods also generalize to other settings where computational tasks involve execution of programs or scripts.",mani modern data scienc applic build data lake schemaagnost repositori data file data product offer limit organ manag capabl need build data lake search capabl data scienc environ scientist analyst find tabl schema workflow dataset use task hand develop search manag solut jupyt notebook data scienc platform enabl scientist augment train data find potenti featur extract clean data find joinabl linkabl tabl core method also gener set comput task involv execut program script,2020,112
Big Earth Data science: an information framework for a sustainable planet,"ABSTRACT The digital transformation of our society coupled with the increasing exploitation of natural resources makes sustainability challenges more complex and dynamic than ever before. These changes will unlikely stop or even decelerate in the near future. There is an urgent need for a new scientific approach and an advanced form of evidence-based decision-making towards the benefit of society, the economy, and the environment. To understand the impacts and interrelationships between humans as a society and natural Earth system processes, we propose a new engineering discipline, Big Earth Data science. This science is called to provide the methodologies and tools to generate knowledge from diverse, numerous, and complex data sources necessary to ensure a sustainable human society essential for the preservation of planet Earth. Big Earth Data science aims at utilizing data from Earth observation and social sensing and develop theories for understanding the mechanisms of how such a social-physical system operates and evolves. The manuscript introduces the universe of discourse characterizing this new science, its foundational paradigms and methodologies, and a possible technological framework to be implemented by applying an ecosystem approach. CASEarth and GEOSS are presented as examples of international implementation attempts. Conclusions discuss important challenges and collaboration opportunities.",abstract digit transform societi coupl increas exploit natur resourc make sustain challeng complex dynam ever chang unlik stop even deceler near futur urgent need new scientif approach advanc form evidencebas decisionmak toward benefit societi economi environ understand impact interrelationship human societi natur earth system process propos new engin disciplin big earth data scienc scienc call provid methodolog tool gener knowledg diver numer complex data sourc necessari ensur sustain human societi essenti preserv planet earth big earth data scienc aim util data earth observ social sen develop theori understand mechan socialphys system oper evolv manuscript introduc univers discours character new scienc foundat paradigm methodolog possibl technolog framework implement appli ecosystem approach casearth geoss present exampl intern implement attempt conclus discus import challeng collabor opportun,2020,89
Data science in economics: comprehensive review of advanced machine learning and deep learning methods,"This paper provides a state-of-the-art investigation of advances in data science in emerging economic applications. The analysis was performed on novel data science methods in four individual classes of deep learning models, hybrid deep learning models, hybrid machine learning, and ensemble models. Application domains include a wide and diverse range of economics research from the stock market, marketing, and e-commerce to corporate banking and cryptocurrency. Prisma method, a systematic literature review methodology, was used to ensure the quality of the survey. The findings reveal that the trends follow the advancement of hybrid models, which, based on the accuracy metric, outperform other learning algorithms. It is further expected that the trends will converge toward the advancements of sophisticated hybrid deep learning models.",paper provid stateoftheart investig advanc data scienc emerg econom applic analysi perform novel data scienc method four individu class deep learn model hybrid deep learn model hybrid machin learn ensembl model applic domain includ wide diver rang econom research stock market market ecommerc corpor bank cryptocurr prisma method systemat literatur review methodolog use ensur qualiti survey find reveal trend follow advanc hybrid model base accuraci metric outperform learn algorithm expect trend converg toward advanc sophist hybrid deep learn model,2020,100
Fixed Point Strategies in Data Science,"The goal of this article is to promote the use of fixed point strategies in data science by showing that they provide a simplifying and unifying framework to model, analyze, and solve a great variety of problems. They are seen to constitute a natural environment to explain the behavior of advanced convex optimization methods as well as of recent nonlinear methods in data science which are formulated in terms of paradigms that go beyond minimization concepts and involve constructs such as Nash equilibria or monotone inclusions. We review the pertinent tools of fixed point theory and describe the main state-of-the-art algorithms for provenly convergent fixed point construction. We also incorporate additional ingredients such as stochasticity, block-implementations, and non-Euclidean metrics, which provide further enhancements. Applications to signal and image processing, machine learning, statistics, neural networks, and inverse problems are discussed.",goal articl promot use fix point strategi data scienc show provid simplifi unifi framework model analyz solv great varieti problem seen constitut natur environ explain behavior advanc convex optim method well recent nonlinear method data scienc formul term paradigm go beyond minim concept involv construct nash equilibrium monoton inclus review pertin tool fix point theori describ main stateoftheart algorithm provenli converg fix point construct also incorpor addit ingredi stochast blockimplement noneuclidean metric provid enhanc applic signal imag process machin learn statist neural network invers problem discus,2020,80
Heidelberg colorectal data set for surgical data science in the sensor operating room,,,2020,80
A new paradigm for accelerating clinical data science at Stanford Medicine,"Stanford Medicine is building a new data platform for our academic research community to do better clinical data science. Hospitals have a large amount of patient data and researchers have demonstrated the ability to reuse that data and AI approaches to derive novel insights, support patient care, and improve care quality. However, the traditional data warehouse and Honest Broker approaches that are in current use, are not scalable. We are establishing a new secure Big Data platform that aims to reduce time to access and analyze data. In this platform, data is anonymized to preserve patient data privacy and made available preparatory to Institutional Review Board (IRB) submission. Furthermore, the data is standardized such that analysis done at Stanford can be replicated elsewhere using the same analytical code and clinical concepts. Finally, the analytics data warehouse integrates with a secure data science computational facility to support large scale data analytics. The ecosystem is designed to bring the modern data science community to highly sensitive clinical data in a secure and collaborative big data analytics environment with a goal to enable bigger, better and faster science.",stanford medicin build new data platform academ research commun better clinic data scienc hospit larg amount patient data research demonstr abil reus data ai approach deriv novel insight support patient care improv care qualiti howev tradit data warehous honest broker approach current use scalabl establish new secur big data platform aim reduc time access analyz data platform data anonym preserv patient data privaci made avail preparatori institut review board irb submiss furthermor data standard analysi done stanford replic elsewher use analyt code clinic concept final analyt data warehous integr secur data scienc comput facil support larg scale data analyt ecosystem design bring modern data scienc commun highli sensit clinic data secur collabor big data analyt environ goal enabl bigger better faster scienc,2020,67
Making data science systems work,"How are data science systems made to work? It may seem that whether a system works is a function of its technical design, but it is also accomplished through ongoing forms of discretionary work by many actors. Based on six months of ethnographic fieldwork with a corporate data science team, we describe how actors involved in a corporate project negotiated what work the system should do, how it should work, and how to assess whether it works. These negotiations laid the foundation for how, why, and to what extent the system ultimately worked. We describe three main findings. First, how already-existing technologies are essential reference points to determine how and whether systems work. Second, how the situated resolution of development challenges continually reshapes the understanding of how and whether systems work. Third, how business goals, and especially their negotiated balance with data science imperatives, affect a system’s working. We conclude with takeaways for critical data studies, orienting researchers to focus on the organizational and cultural aspects of data science, the third-party platforms underlying data science systems, and ways to engage with practitioners’ imagination of how systems can and should work.",data scienc system made work may seem whether system work function technic design also accomplish ongo form discretionari work mani actor base six month ethnograph fieldwork corpor data scienc team describ actor involv corpor project negoti work system work assess whether work negoti laid foundat extent system ultim work describ three main find first alreadyexist technolog essenti refer point determin whether system work second situat resolut develop challeng continu reshap understand whether system work third busi goal especi negoti balanc data scienc imper affect system work conclud takeaway critic data studi orient research focu organiz cultur aspect data scienc thirdparti platform underli data scienc system way engag practition imagin system work,2020,62
Data science and AI in FinTech: an overview,,,2020,70
Vamsa: Automated Provenance Tracking in Data Science Scripts,"There has recently been a lot of ongoing research in the areas of fairness, bias and explainability of machine learning (ML) models due to the self-evident or regulatory requirements of various ML applications. We make the following observation: All of these approaches require a robust understanding of the relationship between ML models and the data used to train them. In this work, we introduce the ML provenance tracking problem: the fundamental idea is to automatically track which columns in a dataset have been used to derive the features/labels of an ML model. We discuss the challenges in capturing such information in the context of Python, the most common language used by data scientists. We then present Vamsa, a modular system that extracts provenance from Python scripts without requiring any changes to the users' code. Using 26K real data science scripts, we verify the effectiveness of Vamsa in terms of coverage, and performance. We also evaluate Vamsa's accuracy on a smaller subset of manually labeled data. Our analysis shows that Vamsa's precision and recall range from 90.4% to 99.1% and its latency is in the order of milliseconds for average size scripts. Drawing from our experience in deploying ML models in production, we also present an example in which Vamsa helps automatically identify models that are affected by data corruption issues.",recent lot ongo research area fair bia explain machin learn ml model due selfevid regulatori requir variou ml applic make follow observ approach requir robust understand relationship ml model data use train work introduc ml proven track problem fundament idea automat track column dataset use deriv featureslabel ml model discus challeng captur inform context python common languag use data scientist present vamsa modular system extract proven python script without requir chang user code use k real data scienc script verifi effect vamsa term coverag perform also evalu vamsa accuraci smaller subset manual label data analysi show vamsa precis recal rang latenc order millisecond averag size script draw experi deploy ml model product also present exampl vamsa help automat identifi model affect data corrupt issu,2020,50
Data Science in the Food Industry.,"Food safety is one of the main challenges of the agri-food industry that is expected to be addressed in the current environment of tremendous technological progress, where consumers' lifestyles and preferences are in a constant state of flux. Food chain transparency and trust are drivers for food integrity control and for improvements in efficiency and economic growth. Similarly, the circular economy has great potential to reduce wastage and improve the efficiency of operations in multi-stakeholder ecosystems. Throughout the food chain cycle, all food commodities are exposed to multiple hazards, resulting in a high likelihood of contamination. Such biological or chemical hazards may be naturally present at any stage of food production, whether accidentally introduced or fraudulently imposed, risking consumers' health and their faith in the food industry. Nowadays, a massive amount of data is generated, not only from the next generation of food safety monitoring systems and along the entire food chain (primary production included) but also from the Internet of things, media, and other devices. These data should be used for the benefit of society, and the scientific field of data science should be a vital player in helping to make this possible.",food safeti one main challeng agrifood industri expect address current environ tremend technolog progress consum lifestyl prefer constant state flux food chain transpar trust driver food integr control improv effici econom growth similarli circular economi great potenti reduc wastag improv effici oper multistakehold ecosystem throughout food chain cycl food commod expo multipl hazard result high likelihood contamin biolog chemic hazard may natur present stage food product whether accident introduc fraudul impos risk consum health faith food industri nowaday massiv amount data gener next gener food safeti monitor system along entir food chain primari product includ also internet thing medium devic data use benefit societi scientif field data scienc vital player help make possibl,2021,31
Opening practice: supporting reproducibility and critical spatial data science,,,2020,70
Statistical Foundations of Data Science,,,2020,137
What Is Data Science,,,2018,469
Re-Shape: A Method to Teach Data Ethics for Data Science Education,"Data has become central to the technologies and services that human-computer interaction (HCI) designers make, and the ethical use of data in and through these technologies should be given critical attention throughout the design process. However, there is little research on ethics education in computer science that explicitly addresses data ethics. We present and analyze Re-Shape, a method to teach students about the ethical implications of data collection and use. Re-Shape, as part of an educational environment, builds upon the idea of cultivating care and allows students to collect, process, and visualize their physical movement data in ways that support critical reflection and coordinated classroom activities about data, data privacy, and human-centered systems for data science. We also use a case study of Re-Shape in an undergraduate computer science course to explore prospects and limitations of instructional designs and educational technology such as Re-Shape that leverage personal data to teach data ethics.",data becom central technolog servic humancomput interact hci design make ethic use data technolog given critic attent throughout design process howev littl research ethic educ comput scienc explicitli address data ethic present analyz reshap method teach student ethic implic data collect use reshap part educ environ build upon idea cultiv care allow student collect process visual physic movement data way support critic reflect coordin classroom activ data data privaci humancent system data scienc also use case studi reshap undergradu comput scienc cours explor prospect limit instruct design educ technolog reshap leverag person data teach data ethic,2020,48
A Survey on Data Pricing: From Economics to Data Science,"Data are invaluable. How can we assess the value of data objectively, systematically and quantitatively? Pricing data, or information goods in general, has been studied and practiced in dispersed areas and principles, such as economics, marketing, electronic commerce, data management, data mining and machine learning. In this article, we present a unified, interdisciplinary and comprehensive overview of this important direction. We examine various motivations behind data pricing, understand the economics of data pricing and review the development and evolution of pricing models according to a series of fundamental principles. We discuss both digital products and data products. We also consider a series of challenges and directions for future work.",data invalu assess valu data object systemat quantit price data inform good gener studi practic dispers area principl econom market electron commerc data manag data mine machin learn articl present unifi interdisciplinari comprehens overview import direct examin variou motiv behind data price understand econom data price review develop evolut price model accord seri fundament principl discus digit product data product also consid seri challeng direct futur work,2020,96
Teaching Creative and Practical Data Science at Scale,"Abstract–Nolan and Temple Lang’s Computing in the Statistics Curricula (2010) advocated for a shift in statistical education to broadly include computing. In the time since, individuals with training in both computing and statistics have become increasingly employable in the burgeoning data science field. In response, universities have developed new courses and programs to meet the growing demand for data science education. To address this demand, we created Data Science in Practice, a large-enrollment undergraduate course. Here, we present our goals for teaching this course, including: (1) conceptualizing data science as creative problem solving, with a focus on project-based learning, (2) prioritizing practical application, teaching and using standardized tools and best practices, and (3) scaling education through coursework that enables hands-on and classroom learning in a large-enrollment course. Throughout this course we also emphasize social context and data ethics to best prepare students for the interdisciplinary and impactful nature of their work. We highlight creative problem solving and strategies for teaching automation-resilient skills, while providing students the opportunity to create a unique data science project that demonstrates their technical and creative capacities.",abstractnolan templ lang comput statist curriculum advoc shift statist educ broadli includ comput time sinc individu train comput statist becom increasingli employ burgeon data scienc field respons univers develop new cours program meet grow demand data scienc educ address demand creat data scienc practic largeenrol undergradu cours present goal teach cours includ conceptu data scienc creativ problem solv focu projectbas learn priorit practic applic teach use standard tool best practic scale educ coursework enabl handson classroom learn largeenrol cours throughout cours also emphas social context data ethic best prepar student interdisciplinari impact natur work highlight creativ problem solv strategi teach automationresili skill provid student opportun creat uniqu data scienc project demonstr technic creativ capac,2020,44
Passing the Data Baton: A Retrospective Analysis on Data Science Work and Workers,"Data science is a rapidly growing discipline and organizations increasingly depend on data science work. Yet the ambiguity around data science, what it is, and who data scientists are can make it difficult for visualization researchers to identify impactful research trajectories. We have conducted a retrospective analysis of data science work and workers as described within the data visualization, human computer interaction, and data science literature. From this analysis we synthesis a comprehensive model that describes data science work and breakdown to data scientists into nine distinct roles. We summarise and reflect on the role that visualization has throughout data science work and the varied needs of data scientists themselves for tooling support. Our findings are intended to arm visualization researchers with a more concrete framing of data science with the hope that it will help them surface innovative opportunities for impacting data science work.",data scienc rapidli grow disciplin organ increasingli depend data scienc work yet ambigu around data scienc data scientist make difficult visual research identifi impact research trajectori conduct retrospect analysi data scienc work worker describ within data visual human comput interact data scienc literatur analysi synthesi comprehens model describ data scienc work breakdown data scientist nine distinct role summaris reflect role visual throughout data scienc work vari need data scientist tool support find intend arm visual research concret frame data scienc hope help surfac innov opportun impact data scienc work,2020,40
The Role of Academia in Data Science Education,"As the demand for data scientists continues to grow, universities are trying to figure out how to best contribute to the training of a workforce. However, there does not appear to be a consensus on the fundamental principles, expertise, skills, or knowledge-base needed to define an academic discipline. We argue that data science is not a discipline but rather an umbrella term used to describe a complex process involving not one data scientist possessing all the necessary expertise, but a team of data scientists with nonoverlapping complementary skills. We provide some recommendations for how to take this into account when designing data science academic programs.",demand data scientist continu grow univers tri figur best contribut train workforc howev appear consensu fundament principl expertis skill knowledgebas need defin academ disciplin argu data scienc disciplin rather umbrella term use describ complex process involv one data scientist possess necessari expertis team data scientist nonoverlap complementari skill provid recommend take account design data scienc academ program,2020,40
The data science life cycle,A cycle that traces ways to define the landscape of data science.,cycl trace way defin landscap data scienc,2020,27
Glossary for public health surveillance in the age of data science,"Public health surveillance is the ongoing systematic collection, analysis and interpretation of data, closely integrated with the timely dissemination of the resulting information to those responsible for preventing and controlling disease and injury. With the rapid development of data science, encompassing big data and artificial intelligence, and with the exponential growth of accessible and highly heterogeneous health-related data, from healthcare providers to user-generated online content, the field of surveillance and health monitoring is changing rapidly. It is, therefore, the right time for a short glossary of key terms in public health surveillance, with an emphasis on new data-science developments in the field.",public health surveil ongo systemat collect analysi interpret data close integr time dissemin result inform respons prevent control diseas injuri rapid develop data scienc encompass big data artifici intellig exponenti growth access highli heterogen healthrel data healthcar provid usergener onlin content field surveil health monitor chang rapidli therefor right time short glossari key term public health surveil emphasi new datasci develop field,2020,39
Human Data Science,,,2020,92
A Fresh Look at Introductory Data Science,"ABSTRACT The proliferation of vast quantities of available datasets that are large and complex in nature has challenged universities to keep up with the demand for graduates trained in both the statistical and the computational set of skills required to effectively plan, acquire, manage, analyze, and communicate the findings of such data. To keep up with this demand, attracting students early on to data science as well as providing them a solid foray into the field becomes increasingly important. We present a case study of an introductory undergraduate course in data science that is designed to address these needs. Offered at Duke University, this course has no prerequisites and serves a wide audience of aspiring statistics and data science majors as well as humanities, social sciences, and natural sciences students. We discuss the unique set of challenges posed by offering such a course, and in light of these challenges, we present a detailed discussion into the pedagogical design elements, content, structure, computational infrastructure, and the assessment methodology of the course. We also offer a repository containing all teaching materials that are open-source, along with supplementary materials and the R code for reproducing the figures found in the article.",abstract prolifer vast quantiti avail dataset larg complex natur challeng univers keep demand graduat train statist comput set skill requir effect plan acquir manag analyz commun find data keep demand attract student earli data scienc well provid solid foray field becom increasingli import present case studi introductori undergradu cours data scienc design address need offer duke univers cours prerequisit serv wide audienc aspir statist data scienc major well human social scienc natur scienc student discus uniqu set challeng pose offer cours light challeng present detail discus pedagog design element content structur comput infrastructur assess methodolog cours also offer repositori contain teach materi opensourc along supplementari materi r code reproduc figur found articl,2020,36
Data science applications to string theory,,,2020,116
Human-AI Collaboration in Data Science,"The rapid advancement of artificial intelligence (AI) is changing our lives in many ways. One application domain is data science. New techniques in automating the creation of AI, known as AutoAI or AutoML, aim to automate the work practices of data scientists. AutoAI systems are capable of autonomously ingesting and pre-processing data, engineering new features, and creating and scoring models based on a target objectives (e.g. accuracy or run-time efficiency). Though not yet widely adopted, we are interested in understanding how AutoAI will impact the practice of data science. We conducted interviews with 20 data scientists who work at a large, multinational technology company and practice data science in various business settings. Our goal is to understand their current work practices and how these practices might change with AutoAI. Reactions were mixed: while informants expressed concerns about the trend of automating their jobs, they also strongly felt it was inevitable. Despite these concerns, they remained optimistic about their future job security due to a view that the future of data science work will be a collaboration between humans and AI systems, in which both automation and human expertise are indispensable.",rapid advanc artifici intellig ai chang live mani way one applic domain data scienc new techniqu autom creation ai known autoai automl aim autom work practic data scientist autoai system capabl autonom ingest preprocess data engin new featur creat score model base target object eg accuraci runtim effici though yet wide adopt interest understand autoai impact practic data scienc conduct interview data scientist work larg multin technolog compani practic data scienc variou busi set goal understand current work practic practic might chang autoai reaction mix inform express concern trend autom job also strongli felt inevit despit concern remain optimist futur job secur due view futur data scienc work collabor human ai system autom human expertis indispens,2019,294
Data-science driven autonomous process optimization,,,2020,113
"How Data Science Workers Work with Data: Discovery, Capture, Curation, Design, Creation","With the rise of big data, there has been an increasing need for practitioners in this space and an increasing opportunity for researchers to understand their workflows and design new tools to improve it. Data science is often described as data-driven, comprising unambiguous data and proceeding through regularized steps of analysis. However, this view focuses more on abstract processes, pipelines, and workflows, and less on how data science workers engage with the data. In this paper, we build on the work of other CSCW and HCI researchers in describing the ways that scientists, scholars, engineers, and others work with their data, through analyses of interviews with 21 data science professionals. We set five approaches to data along a dimension of interventions: Data as given; as captured; as curated; as designed; and as created. Data science workers develop an intuitive sense of their data and processes, and actively shape their data. We propose new ways to apply these interventions analytically, to make sense of the complex activities around data practices.",rise big data increas need practition space increas opportun research understand workflow design new tool improv data scienc often describ datadriven compris unambigu data proceed regular step analysi howev view focus abstract process pipelin workflow less data scienc worker engag data paper build work cscw hci research describ way scientist scholar engin other work data analys interview data scienc profession set five approach data along dimens intervent data given captur curat design creat data scienc worker develop intuit sen data process activ shape data propos new way appli intervent analyt make sen complex activ around data practic,2019,245
Process Mining for Python (PM4Py): Bridging the Gap Between Process- and Data Science,"Process mining, i.e., a sub-field of data science focusing on the analysis of event data generated during the execution of (business) processes, has seen a tremendous change over the past two decades. Starting off in the early 2000's, with limited to no tool support, nowadays, several software tools, i.e., both open-source, e.g., ProM and Apromore, and commercial, e.g., Disco, Celonis, ProcessGold, etc., exist. The commercial process mining tools provide limited support for implementing custom algorithms. Moreover, both commercial and open-source process mining tools are often only accessible through a graphical user interface, which hampers their usage in large-scale experimental settings. Initiatives such as RapidProM provide process mining support in the scientific workflow-based data science suite RapidMiner. However, these offer limited to no support for algorithmic customization. In the light of the aforementioned, in this paper, we present a novel process mining library, i.e. Process Mining for Python (PM4Py) that aims to bridge this gap, providing integration with state-of-the-art data science libraries, e.g., pandas, numpy, scipy and scikit-learn. We provide a global overview of the architecture and functionality of PM4Py, accompanied by some representative examples of its usage.",process mine ie subfield data scienc focus analysi event data gener execut busi process seen tremend chang past two decad start earli limit tool support nowaday sever softwar tool ie opensourc eg prom apromor commerci eg disco celoni processgold etc exist commerci process mine tool provid limit support implement custom algorithm moreov commerci opensourc process mine tool often access graphic user interfac hamper usag largescal experiment set initi rapidprom provid process mine support scientif workflowbas data scienc suit rapidmin howev offer limit support algorithm custom light aforement paper present novel process mine librari ie process mine python pmpi aim bridg gap provid integr stateoftheart data scienc librari eg panda numpi scipi scikitlearn provid global overview architectur function pmpi accompani repres exampl usag,2019,218
Data Management for Data Science - Towards Embedded Analytics,"textabstractThe rise of Data Science has caused an influx of new usersin need of data management solutions. However, insteadof utilizing existing RDBMS solutions they are opting touse a stack of independent solutions for data storage andprocessing glued together by scripting languages. This is notbecause they do not need the functionality that an integratedRDBMS provides, but rather because existing RDBMS im-plementations do not cater to their use case. To solve theseissues, we propose a new class of data management systems:embedded analytical systems. These systems are tightlyintegrated with analytical tools, and provide fast and effi-cient access to the data stored within them. In this work,we describe the unique challenges and opportunities w.r.tworkloads, resilience and cooperation that are faced by thisnew class of systems and the steps we have taken towardsaddressing them in the DuckDB system.",textabstractth rise data scienc caus influx new usersin need data manag solut howev insteadof util exist rdbm solut opt tous stack independ solut data storag andprocess glu togeth script languag notbecaus need function integratedrdbm provid rather exist rdbm implement cater use case solv theseissu propos new class data manag systemsembed analyt system system tightlyintegr analyt tool provid fast effici access data store within workw describ uniqu challeng opportun wrtworkload resili cooper face thisnew class system step taken towardsaddress duckdb system,2020,32
A Systematic Review on Supervised and Unsupervised Machine Learning Algorithms for Data Science,,,2019,306
"Data Science in 2020: Computing, Curricula, and Challenges for the Next 10 Years","Abstract In the past 10 years, new data science courses and programs have proliferated at the collegiate level. As faculty and administrators enter the race to provide data science training and attract new students, the road map for teaching data science remains elusive. In 2019, 69 college and university faculty teaching data science courses and developing data science curricula were surveyed to learn about their curricula, computing tools, and challenges they face in their classrooms. Faculty reported teaching a variety of computing skills in introductory data science (albeit fewer computing topics than statistics topics), and that one of the biggest challenges they face is teaching computing to a diverse audience with varying preparation. The ever-evolving nature of data science is a major hurdle for faculty teaching data science courses, and a call for more data science teaching resources was echoed in many responses.",abstract past year new data scienc cours program prolifer collegi level faculti administr enter race provid data scienc train attract new student road map teach data scienc remain elus colleg univers faculti teach data scienc cours develop data scienc curriculum survey learn curriculum comput tool challeng face classroom faculti report teach varieti comput skill introductori data scienc albeit fewer comput topic statist topic one biggest challeng face teach comput diver audienc vari prepar everevolv natur data scienc major hurdl faculti teach data scienc cours call data scienc teach resourc echo mani respons,2020,32
Web Scraping in the Statistics and Data Science Curriculum: Challenges and Opportunities,"Abstract Best practices in statistics and data science courses include the use of real and relevant data as well as teaching the entire data science cycle starting with importing data. A rich source of real and current data is the web, where data are often presented and stored in a structure that needs some wrangling and transforming before they can be ready for analysis. The web is a resource students naturally turn to for finding data for data analysis projects, but without formal instruction on how to get that data into a structured format, they often resort to copy-pasting or manual entry into a spreadsheet, which are both time consuming and error-prone. Teaching web scraping provides an opportunity to bring such data into the curriculum in an effective and efficient way. In this article, we explain how web scraping works and how it can be implemented in a pedagogically sound and technically executable way at various levels of statistics and data science curricula. We provide classroom activities where we connect this modern computing technique with traditional statistical topics. Finally, we share the opportunities web scraping brings to the classrooms as well as the challenges to instructors and tips for avoiding them.",abstract best practic statist data scienc cours includ use real relev data well teach entir data scienc cycl start import data rich sourc real current data web data often present store structur need wrangl transform readi analysi web resourc student natur turn find data data analysi project without formal instruct get data structur format often resort copypast manual entri spreadsheet time consum errorpron teach web scrape provid opportun bring data curriculum effect effici way articl explain web scrape work implement pedagog sound technic execut way variou level statist data scienc curriculum provid classroom activ connect modern comput techniqu tradit statist topic final share opportun web scrape bring classroom well challeng instructor tip avoid,2020,34
Interrogating Data Science,"Data science provides powerful tools and methods. CSCW researchers have contributed insightfulstudies of conventional work-practices in data science - and particularly machine learning. However,recent research has shown that human skills and collaborative decision-making, play important rolesin defining data, acquiring data, curating data, designing data, and creating data. This workshopgathers researchers and practitioners together to take a collective and critical look at data sciencework-practices, and at how those work-practices make crucial and often invisible impacts on theformal work of data science. When we understand the human and social contributions to data sciencepipelines, we can constructively redesign both work and technologies for new insights, theories, andchallenges.",data scienc provid power tool method cscw research contribut insightfulstudi convent workpractic data scienc particularli machin learn howeverrec research shown human skill collabor decisionmak play import rolesin defin data acquir data curat data design data creat data workshopgath research practition togeth take collect critic look data scienceworkpractic workpractic make crucial often invis impact theform work data scienc understand human social contribut data sciencepipelin construct redesign work technolog new insight theori andchalleng,2020,23
Big Data Science on COVID-19 Data,"In the current era of big data, high volume of big data can be generated and collected from a wide variety of rich data sources at a rapid rate. Embedded in these big data are useful information and valuable knowledge. Examples include healthcare and epidemiological data such as data related to patients who suffered from viral diseases like the coronavirus disease 2019 (COVID-19). Knowledge discovered from these epidemiological data via data science helps researchers, epidemiologists and policy makers to get a better understanding of the disease, which may inspire them to come up ways to detect, control and combat the disease. In this paper, we present a data science solution for analyzing big COVID-19 epidemiological data. The solution helps users to get a better understanding of information about the confirmed cases of COVID-19. Evaluation results show the benefits of our data science solution in discovering useful knowledge from big COVID-19 data.",current era big data high volum big data gener collect wide varieti rich data sourc rapid rate embed big data use inform valuabl knowledg exampl includ healthcar epidemiolog data data relat patient suffer viral diseas like coronaviru diseas covid knowledg discov epidemiolog data via data scienc help research epidemiologist polici maker get better understand diseas may inspir come way detect control combat diseas paper present data scienc solut analyz big covid epidemiolog data solut help user get better understand inform confirm case covid evalu result show benefit data scienc solut discov use knowledg big covid data,2020,21
Ten Research Challenge Areas in Data Science,"Although data science builds on knowledge from computer science, mathematics, statistics, and other disciplines, data science is a unique field with many mysteries to unlock: challenging scientific questions and pressing questions of societal importance. This article starts with meta-questions about data science as a discipline and then elaborates on ten ideas for the basis of a research agenda for data science.",although data scienc build knowledg comput scienc mathemat statist disciplin data scienc uniqu field mani mysteri unlock challeng scientif question press question societ import articl start metaquest data scienc disciplin elabor ten idea basi research agenda data scienc,2020,19
Theory-Guided Data Science: A New Paradigm for Scientific Discovery from Data,"Data science models, although successful in a number of commercial domains, have had limited applicability in scientific problems involving complex physical phenomena. Theory-guided data science (TGDS) is an emerging paradigm that aims to leverage the wealth of scientific knowledge for improving the effectiveness of data science models in enabling scientific discovery. The overarching vision of TGDS is to introduce scientific consistency as an essential component for learning generalizable models. Further, by producing scientifically interpretable models, TGDS aims to advance our scientific understanding by discovering novel domain insights. Indeed, the paradigm of TGDS has started to gain prominence in a number of scientific disciplines such as turbulence modeling, material discovery, quantum chemistry, bio-medical science, bio-marker discovery, climate science, and hydrology. In this paper, we formally conceptualize the paradigm of TGDS and present a taxonomy of research themes in TGDS. We describe several approaches for integrating domain knowledge in different research themes using illustrative examples from different disciplines. We also highlight some of the promising avenues of novel research for realizing the full potential of theory-guided data science.",data scienc model although success number commerci domain limit applic scientif problem involv complex physic phenomenon theoryguid data scienc tgd emerg paradigm aim leverag wealth scientif knowledg improv effect data scienc model enabl scientif discoveri overarch vision tgd introduc scientif consist essenti compon learn generaliz model produc scientif interpret model tgd aim advanc scientif understand discov novel domain insight inde paradigm tgd start gain promin number scientif disciplin turbul model materi discoveri quantum chemistri biomed scienc biomark discoveri climat scienc hydrolog paper formal conceptu paradigm tgd present taxonomi research theme tgd describ sever approach integr domain knowledg differ research theme use illustr exampl differ disciplin also highlight promis avenu novel research realiz full potenti theoryguid data scienc,2016,907
Veridical data science,"Significance Predictability, computability, and stability (PCS) are three core principles of data science. They embed the scientific principles of prediction and replication in data-driven decision making while recognizing the central role of computation. Based on these principles, we propose the PCS framework, including workflow and documentation (in R Markdown or Jupyter Notebook). The PCS framework aims at responsible, reliable, reproducible, and transparent analysis across fields of science, social science, engineering, business, and government. It can be used as a recommendation system for scientific hypothesis generation and experimental design. In particular, we propose (basic) PCS inference for reliability measures on data results, extending statistical inference to a much broader scope as current data science practice entails. Building and expanding on principles of statistics, machine learning, and scientific inquiry, we propose the predictability, computability, and stability (PCS) framework for veridical data science. Our framework, composed of both a workflow and documentation, aims to provide responsible, reliable, reproducible, and transparent results across the data science life cycle. The PCS workflow uses predictability as a reality check and considers the importance of computation in data collection/storage and algorithm design. It augments predictability and computability with an overarching stability principle. Stability expands on statistical uncertainty considerations to assess how human judgment calls impact data results through data and model/algorithm perturbations. As part of the PCS workflow, we develop PCS inference procedures, namely PCS perturbation intervals and PCS hypothesis testing, to investigate the stability of data results relative to problem formulation, data cleaning, modeling decisions, and interpretations. We illustrate PCS inference through neuroscience and genomics projects of our own and others. Moreover, we demonstrate its favorable performance over existing methods in terms of receiver operating characteristic (ROC) curves in high-dimensional, sparse linear model simulations, including a wide range of misspecified models. Finally, we propose PCS documentation based on R Markdown or Jupyter Notebook, with publicly available, reproducible codes and narratives to back up human choices made throughout an analysis. The PCS workflow and documentation are demonstrated in a genomics case study available on Zenodo.",signific predict comput stabil pc three core principl data scienc emb scientif principl predict replic datadriven decis make recogn central role comput base principl propos pc framework includ workflow document r markdown jupyt notebook pc framework aim respons reliabl reproduc transpar analysi across field scienc social scienc engin busi govern use recommend system scientif hypothesi gener experiment design particular propos basic pc infer reliabl measur data result extend statist infer much broader scope current data scienc practic entail build expand principl statist machin learn scientif inquiri propos predict comput stabil pc framework verid data scienc framework compos workflow document aim provid respons reliabl reproduc transpar result across data scienc life cycl pc workflow use predict realiti check consid import comput data collectionstorag algorithm design augment predict comput overarch stabil principl stabil expand statist uncertainti consider assess human judgment call impact data result data modelalgorithm perturb part pc workflow develop pc infer procedur name pc perturb interv pc hypothesi test investig stabil data result rel problem formul data clean model decis interpret illustr pc infer neurosci genom project other moreov demonstr favor perform exist method term receiv oper characterist roc curv highdimension spar linear model simul includ wide rang misspecifi model final propos pc document base r markdown jupyt notebook publicli avail reproduc code narr back human choic made throughout analysi pc workflow document demonstr genom case studi avail zenodo,2019,149
Algorithmic Government: Automating Public Services and Supporting Civil Servants in using Data Science Technologies,"The data science technologies of artificial intelligence (AI), Internet of Things (IoT), big data and behavioral/predictive analytics, and blockchain are poised to revolutionize government and create a new generation of GovTech start-ups. The impact from the ‘smartification’ of public services and the national infrastructure will be much more significant in comparison to any other sector given government's function and importance to every institution and individual. Potential GovTech systems include Chatbots and intelligent assistants for public engagement, Robo-advisors to support civil servants, real-time management of the national infrastructure using IoT and blockchain, automated compliance/regulation, public records securely stored in blockchain distributed ledgers, online judicial and dispute resolution systems, and laws/statutes encoded as blockchain smart contracts. Government is potentially the major ‘client’ and also ‘public champion’ for these new data technologies. This review paper uses our simple taxonomy of government services to provide an overview of data science automation being deployed by governments world-wide. The goal of this review paper is to encourage the Computer Science community to engage with government to develop these new systems to transform public services and support the work of civil servants.",data scienc technolog artifici intellig ai internet thing iot big data behavioralpredict analyt blockchain poi revolution govern creat new gener govtech startup impact smartif public servic nation infrastructur much signific comparison sector given govern function import everi institut individu potenti govtech system includ chatbot intellig assist public engag roboadvisor support civil servant realtim manag nation infrastructur use iot blockchain autom complianceregul public record secur store blockchain distribut ledger onlin judici disput resolut system lawsstatut encod blockchain smart contract govern potenti major client also public champion new data technolog review paper use simpl taxonomi govern servic provid overview data scienc autom deploy govern worldwid goal review paper encourag comput scienc commun engag govern develop new system transform public servic support work civil servant,2019,177
The State of the Art of Data Science and Engineering in Structural Health Monitoring,,,2019,339
50 Years of Data Science,"ABSTRACT More than 50 years ago, John Tukey called for a reformation of academic statistics. In “The Future of Data Analysis,” he pointed to the existence of an as-yet unrecognized science, whose subject of interest was learning from data, or “data analysis.” Ten to 20 years ago, John Chambers, Jeff Wu, Bill Cleveland, and Leo Breiman independently once again urged academic statistics to expand its boundaries beyond the classical domain of theoretical statistics; Chambers called for more emphasis on data preparation and presentation rather than statistical modeling; and Breiman called for emphasis on prediction rather than inference. Cleveland and Wu even suggested the catchy name “data science” for this envisioned field. A recent and growing phenomenon has been the emergence of “data science” programs at major universities, including UC Berkeley, NYU, MIT, and most prominently, the University of Michigan, which in September 2015 announced a $100M “Data Science Initiative” that aims to hire 35 new faculty. Teaching in these new programs has significant overlap in curricular subject matter with traditional statistics courses; yet many academic statisticians perceive the new programs as “cultural appropriation.” This article reviews some ingredients of the current “data science moment,” including recent commentary about data science in the popular media, and about how/whether data science is really different from statistics. The now-contemplated field of data science amounts to a superset of the fields of statistics and machine learning, which adds some technology for “scaling up” to “big data.” This chosen superset is motivated by commercial rather than intellectual developments. Choosing in this way is likely to miss out on the really important intellectual event of the next 50 years. Because all of science itself will soon become data that can be mined, the imminent revolution in data science is not about mere “scaling up,” but instead the emergence of scientific studies of data analysis science-wide. In the future, we will be able to predict how a proposal to change data analysis workflows would impact the validity of data analysis across all of science, even predicting the impacts field-by-field. Drawing on work by Tukey, Cleveland, Chambers, and Breiman, I present a vision of data science based on the activities of people who are “learning from data,” and I describe an academic field dedicated to improving that activity in an evidence-based manner. This new field is a better academic enlargement of statistics and machine learning than today’s data science initiatives, while being able to accommodate the same short-term goals. Based on a presentation at the Tukey Centennial Workshop, Princeton, NJ, September 18, 2015.",abstract year ago john tukey call reform academ statist futur data analysi point exist asyet unrecogn scienc whose subject interest learn data data analysi ten year ago john chamber jeff wu bill cleveland leo breiman independ urg academ statist expand boundari beyond classic domain theoret statist chamber call emphasi data prepar present rather statist model breiman call emphasi predict rather infer cleveland wu even suggest catchi name data scienc envis field recent grow phenomenon emerg data scienc program major univers includ uc berkeley nyu mit promin univers michigan septemb announc data scienc initi aim hire new faculti teach new program signific overlap curricular subject matter tradit statist cours yet mani academ statistician perceiv new program cultur appropri articl review ingredi current data scienc moment includ recent commentari data scienc popular medium howwheth data scienc realli differ statist nowcontempl field data scienc amount superset field statist machin learn add technolog scale big data chosen superset motiv commerci rather intellectu develop choos way like miss realli import intellectu event next year scienc soon becom data mine immin revolut data scienc mere scale instead emerg scientif studi data analysi sciencewid futur abl predict propos chang data analysi workflow would impact valid data analysi across scienc even predict impact fieldbyfield draw work tukey cleveland chamber breiman present vision data scienc base activ peopl learn data describ academ field dedic improv activ evidencebas manner new field better academ enlarg statist machin learn today data scienc initi abl accommod shortterm goal base present tukey centenni workshop princeton nj septemb,2017,612
A Second Chance to Get Causal Inference Right: A Classification of Data Science Tasks,"Causal inference from observational data is the goal of many data analyses in the health and social sciences. However, academic statistics has often frowned upon data analyses with a causal objective. The introduction of the term ""data science"" provides a historic opportunity to redefine data analysis in such a way that it naturally accommodates causal inference from observational data. Like others before, we organize the scientific contributions of data science into three classes of tasks: Description, prediction, and counterfactual prediction (which includes causal inference). An explicit classification of data science tasks is necessary to discuss the data, assumptions, and analytics required to successfully accomplish each task. We argue that a failure to adequately describe the role of subject-matter expert knowledge in data analysis is a source of widespread misunderstandings about data science. Specifically, causal analyses typically require not only good data and algorithms, but also domain expert knowledge. We discuss the implications for the use of data science to guide decision-making in the real world and to train data scientists.",causal infer observ data goal mani data analys health social scienc howev academ statist often frown upon data analys causal object introduct term data scienc provid histor opportun redefin data analysi way natur accommod causal infer observ data like other organ scientif contribut data scienc three class task descript predict counterfactu predict includ causal infer explicit classif data scienc task necessari discus data assumpt analyt requir success accomplish task argu failur adequ describ role subjectmatt expert knowledg data analysi sourc widespread misunderstand data scienc specif causal analys typic requir good data algorithm also domain expert knowledg discus implic use data scienc guid decisionmak real world train data scientist,2018,309
From hype to reality: data science enabling personalized medicine,,,2018,300
"Practitioners Teaching Data Science in Industry and Academia: Expectations, Workflows, and Challenges","Data science has been growing in prominence across both academia and industry, but there is still little formal consensus about how to teach it. Many people who currently teach data science are practitioners such as computational researchers in academia or data scientists in industry. To understand how these practitioner-instructors pass their knowledge onto novices and how that contrasts with teaching more traditional forms of programming, we interviewed 20 data scientists who teach in settings ranging from small-group workshops to large online courses. We found that: 1) they must empathize with a diverse array of student backgrounds and expectations, 2) they teach technical workflows that integrate authentic practices surrounding code, data, and communication, 3) they face challenges involving authenticity versus abstraction in software setup, finding and curating pedagogically-relevant datasets, and acclimating students to live with uncertainty in data analysis. These findings can point the way toward better tools for data science education and help bring data literacy to more people around the world.",data scienc grow promin across academia industri still littl formal consensu teach mani peopl current teach data scienc practition comput research academia data scientist industri understand practitionerinstructor pas knowledg onto novic contrast teach tradit form program interview data scientist teach set rang smallgroup workshop larg onlin cours found must empath diver array student background expect teach technic workflow integr authent practic surround code data commun face challeng involv authent versu abstract softwar setup find curat pedagogicallyrelev dataset acclim student live uncertainti data analysi find point way toward better tool data scienc educ help bring data literaci peopl around world,2019,78
Outbreak analytics: a developing data science for informing the response to emerging pathogens,"Despite continued efforts to improve health systems worldwide, emerging pathogen epidemics remain a major public health concern. Effective response to such outbreaks relies on timely intervention, ideally informed by all available sources of data. The collection, visualization and analysis of outbreak data are becoming increasingly complex, owing to the diversity in types of data, questions and available methods to address them. Recent advances have led to the rise of outbreak analytics, an emerging data science focused on the technological and methodological aspects of the outbreak data pipeline, from collection to analysis, modelling and reporting to inform outbreak response. In this article, we assess the current state of the field. After laying out the context of outbreak response, we critically review the most common analytics components, their inter-dependencies, data requirements and the type of information they can provide to inform operations in real time. We discuss some challenges and opportunities and conclude on the potential role of outbreak analytics for improving our understanding of, and response to outbreaks of emerging pathogens. This article is part of the theme issue ‘Modelling infectious disease outbreaks in humans, animals and plants: epidemic forecasting and control‘. This theme issue is linked with the earlier issue ‘Modelling infectious disease outbreaks in humans, animals and plants: approaches and important themes’.",despit continu effort improv health system worldwid emerg pathogen epidem remain major public health concern effect respons outbreak reli time intervent ideal inform avail sourc data collect visual analysi outbreak data becom increasingli complex owe diver type data question avail method address recent advanc led rise outbreak analyt emerg data scienc focus technolog methodolog aspect outbreak data pipelin collect analysi model report inform outbreak respons articl assess current state field lay context outbreak respons critic review common analyt compon interdepend data requir type inform provid inform oper real time discus challeng opportun conclud potenti role outbreak analyt improv understand respons outbreak emerg pathogen articl part theme issu model infecti diseas outbreak human anim plant epidem forecast control theme issu link earlier issu model infecti diseas outbreak human anim plant approach import theme,2019,148
SystemDS: A Declarative Machine Learning System for the End-to-End Data Science Lifecycle,"Machine learning (ML) applications become increasingly common in many domains. ML systems to execute these workloads include numerical computing frameworks and libraries, ML algorithm libraries, and specialized systems for deep neural networks and distributed ML. These systems focus primarily on efficient model training and scoring. However, the data science process is exploratory, and deals with underspecified objectives and a wide variety of heterogeneous data sources. Therefore, additional tools are employed for data engineering and debugging, which requires boundary crossing, unnecessary manual effort, and lacks optimization across the lifecycle. In this paper, we introduce SystemDS, an open source ML system for the end-to-end data science lifecycle from data integration, cleaning, and preparation, over local, distributed, and federated ML model training, to debugging and serving. To this end, we aim to provide a stack of declarative languages with R-like syntax for the different lifecycle tasks, and users with different expertise. We describe the overall system architecture, explain major design decisions (motivated by lessons learned from Apache SystemML), and discuss key features and research directions. Finally, we provide preliminary results that show the potential of end-to-end lifecycle optimization.",machin learn ml applic becom increasingli common mani domain ml system execut workload includ numer comput framework librari ml algorithm librari special system deep neural network distribut ml system focu primarili effici model train score howev data scienc process exploratori deal underspecifi object wide varieti heterogen data sourc therefor addit tool employ data engin debug requir boundari cross unnecessari manual effort lack optim across lifecycl paper introduc systemd open sourc ml system endtoend data scienc lifecycl data integr clean prepar local distribut feder ml model train debug serv end aim provid stack declar languag rlike syntax differ lifecycl task user differ expertis describ overal system architectur explain major design decis motiv lesson learn apach systemml discus key featur research direct final provid preliminari result show potenti endtoend lifecycl optim,2019,60
Data science ethical considerations: a systematic literature review and proposed project framework,,,2019,67
Data science for entrepreneurship research: studying demand dynamics for entrepreneurial skills in the Netherlands,,,2019,72
Data science in data librarianship: Core competencies of a data librarian,"Currently, data are stored in an always-on condition, and can be globally accessed at any point, by any user. Data librarianship has its origins in the social sciences. In particular, the creation of data services and data archives, in the United Kingdom (Data Archives Services) and in the United States and Canada (Data Library Services), is a key factor for the emergence of data librarianship. The focus of data librarianship nowadays is on the creation of new library services. Data librarians are concerned with the proposition of services for data management and curation in academic libraries and other research organizations. The purpose of this paper is to understand how the complexity of the data can serve as the basis for identifying the technical skills required by data librarians. This essay is systematically divided, first introducing the concepts of data and research data in data librarianship, followed by an overview of data science as a theory, method, and technology to assess data. Next, the identification of the competencies and skills required by data scientists and data librarians are discussed. Our final remarks highlight that data librarians should understand that the complexity and novelty associated with data science praxis. Data science provides new methods and practices for data librarianship. A data librarian need not become a programmer, statistician, or database manager, but should be interested in learning about the languages and programming logic of computers, databases, and information retrieval tools. We believe that numerous kinds of scientific data research provide opportunities for a data librarian to engage with data science.",current data store alwayson condit global access point user data librarianship origin social scienc particular creation data servic data archiv unit kingdom data archiv servic unit state canada data librari servic key factor emerg data librarianship focu data librarianship nowaday creation new librari servic data librarian concern proposit servic data manag curat academ librari research organ purpos paper understand complex data serv basi identifi technic skill requir data librarian essay systemat divid first introduc concept data research data data librarianship follow overview data scienc theori method technolog assess data next identif compet skill requir data scientist data librarian discus final remark highlight data librarian understand complex novelti associ data scienc praxi data scienc provid new method practic data librarianship data librarian need becom programm statistician databas manag interest learn languag program logic comput databas inform retriev tool believ numer kind scientif data research provid opportun data librarian engag data scienc,2019,64
Microbiome data science,,,2019,53
The Challenge of Big Data and Data Science,"Big data and data science are transforming the world in ways that spawn new concerns for social scientists, such as the impacts of the internet on citizens and the media, the repercussions of smart cities, the possibilities of cyber-warfare and cyber-terrorism, the implications of precision medicine, and the consequences of artificial intelligence and automation. Along with these changes in society, powerful new data science methods support research using administrative, internet, textual, and sensor-audio-video data. Burgeoning data and innovative methods facilitate answering previously hard-to-tackle questions about society by offering new ways to form concepts from data, to do descriptive inference, to make causal inferences, and to generate predictions. They also pose challenges as social scientists must grasp the meaning of concepts and predictions generated by convoluted algorithms, weigh the relative value of prediction versus causal inference, and cope with ethical challenges as their methods, such as algorithms for mobilizing voters or determining bail, are adopted by policy makers.",big data data scienc transform world way spawn new concern social scientist impact internet citizen medium repercuss smart citi possibl cyberwarfar cyberterror implic precis medicin consequ artifici intellig autom along chang societi power new data scienc method support research use administr internet textual sensoraudiovideo data burgeon data innov method facilit answer previous hardtotackl question societi offer new way form concept data descript infer make causal infer gener predict also pose challeng social scientist must grasp mean concept predict gener convolut algorithm weigh rel valu predict versu causal infer cope ethic challeng method algorithm mobil voter determin bail adopt polici maker,2019,57
Human-Centered Study of Data Science Work Practices,"With the rise of big data, there has been an increasing need to understand who is working in data science and how they are doing their work. HCI and CSCW researchers have begun to examine these questions. In this workshop, we invite researchers to share their observations, experiences, hypotheses, and insights, in the hopes of developing a taxonomy of work practices and open issues in the behavioral and social study of data science and data science workers.",rise big data increas need understand work data scienc work hci cscw research begun examin question workshop invit research share observ experi hypothes insight hope develop taxonomi work practic open issu behavior social studi data scienc data scienc worker,2019,53
Situating Data Science: Exploring How Relationships to Data Shape Learning,"The emerging field of Data Science has had a large impact on science and society. This has led to over a decade of calls to establish a corresponding field of Data Science Education. There is still a need, however, to more deeply conceptualize what a field of Data Science Education might entail in terms of scope, responsibility, and execution. This special issue explores how one distinguishing feature of Data Science—its focus on data collected from social and environmental contexts within which learners often find themselves deeply embedded—suggests serious implications for learning and education. The learning sciences is uniquely positioned to investigate how such contextual embeddings impact learners’ engagement with data including conceptual, experiential, communal, racialized, spatial, and political dimensions. This special issue demonstrates the richly layered relationships learners build with data and reveals them to be not merely utilitarian mechanisms for learning about data, but a critical part of navigating data as social text and understanding Data Science as a discipline. Together, the contributions offer a vision of how the learning sciences can contribute to a more expansive, agentive and socially aware Data Science Education.",emerg field data scienc larg impact scienc societi led decad call establish correspond field data scienc educ still need howev deepli conceptu field data scienc educ might entail term scope respons execut special issu explor one distinguish featur data scienceit focu data collect social environment context within learner often find deepli embeddedsuggest seriou implic learn educ learn scienc uniqu posit investig contextu embed impact learner engag data includ conceptu experienti commun racial spatial polit dimens special issu demonstr richli layer relationship learner build data reveal mere utilitarian mechan learn data critic part navig data social text understand data scienc disciplin togeth contribut offer vision learn scienc contribut expans agent social awar data scienc educ,2019,62
Data Science of the Natural Environment: A Research Roadmap,"Data science is the science of extracting meaning from potentially complex data. This is a fast moving field, drawing principles and techniques from a number of different disciplinary areas including computer science, statistics and complexity science. Data science is having a profound impact on a number of areas including commerce, health and smart cities. This paper argues that data science can have an equal if not greater impact in the area of earth and environmental sciences, offering a rich tapestry of new techniques to support both a deeper understanding of the natural environment in all its complexities, as well as the development of well-founded mitigation and adaptation strategies in the face of climate change. The paper argues that data science for the natural environment brings about new challenges for data science, particularly around complexity, spatial and temporal reasoning, and managing uncertainty. The paper also describes a case study in environmental data science which offers up insights into the promise of the area. The paper concludes with a research roadmap highlighting ten top challenges of environmental data science and also an invitation to become part of an international community working collaboratively on these problems.",data scienc scienc extract mean potenti complex data fast move field draw principl techniqu number differ disciplinari area includ comput scienc statist complex scienc data scienc profound impact number area includ commerc health smart citi paper argu data scienc equal greater impact area earth environment scienc offer rich tapestri new techniqu support deeper understand natur environ complex well develop wellfound mitig adapt strategi face climat chang paper argu data scienc natur environ bring new challeng data scienc particularli around complex spatial tempor reason manag uncertainti paper also describ case studi environment data scienc offer insight promis area paper conclud research roadmap highlight ten top challeng environment data scienc also invit becom part intern commun work collabor problem,2019,57
Big Data and data science: A critical review of issues for educational research,"Big Data refers to large and disparate volumes of data generated by people, applications and machines. It is gaining increasing attention from a variety of domains, including education. What are the challenges of engaging with Big Data research in education? This paper identifies a wide range of critical issues that researchers need to consider when working with Big Data in education. The issues identified include diversity in the conception and meaning of Big Data in education, ontological, epistemological disparity, technical challenges, ethics and privacy, digital divide and digital dividend, lack of expertise and academic development opportunities to prepare educational researchers to leverage opportunities afforded by Big Data. The goal of this paper is to raise awareness on these issues and initiate a dialogue. The paper was inspired partly by insights drawn from the literature but mostly informed by experience researching into Big Data in education. [ABSTRACT FROM AUTHOR]",big data refer larg dispar volum data gener peopl applic machin gain increas attent varieti domain includ educ challeng engag big data research educ paper identifi wide rang critic issu research need consid work big data educ issu identifi includ diver concept mean big data educ ontolog epistemolog dispar technic challeng ethic privaci digit divid digit dividend lack expertis academ develop opportun prepar educ research leverag opportun afford big data goal paper rais awar issu initi dialogu paper inspir partli insight drawn literatur mostli inform experi research big data educ abstract author,2019,147
Genomics and data science: an application within an umbrella,,,2019,50
Health Care and Precision Medicine Research: Analysis of a Scalable Data Science Platform,"Background Health care data are increasing in volume and complexity. Storing and analyzing these data to implement precision medicine initiatives and data-driven research has exceeded the capabilities of traditional computer systems. Modern big data platforms must be adapted to the specific demands of health care and designed for scalability and growth. Objective The objectives of our study were to (1) demonstrate the implementation of a data science platform built on open source technology within a large, academic health care system and (2) describe 2 computational health care applications built on such a platform. Methods We deployed a data science platform based on several open source technologies to support real-time, big data workloads. We developed data-acquisition workflows for Apache Storm and NiFi in Java and Python to capture patient monitoring and laboratory data for downstream analytics. Results Emerging data management approaches, along with open source technologies such as Hadoop, can be used to create integrated data lakes to store large, real-time datasets. This infrastructure also provides a robust analytics platform where health care and biomedical research data can be analyzed in near real time for precision medicine and computational health care use cases. Conclusions The implementation and use of integrated data science platforms offer organizations the opportunity to combine traditional datasets, including data from the electronic health record, with emerging big data sources, such as continuous patient monitoring and real-time laboratory results. These platforms can enable cost-effective and scalable analytics for the information that will be key to the delivery of precision medicine initiatives. Organizations that can take advantage of the technical advances found in data science platforms will have the opportunity to provide comprehensive access to health care data for computational health care and precision medicine research.",background health care data increas volum complex store analyz data implement precis medicin initi datadriven research exceed capabl tradit comput system modern big data platform must adapt specif demand health care design scalabl growth object object studi demonstr implement data scienc platform built open sourc technolog within larg academ health care system describ comput health care applic built platform method deploy data scienc platform base sever open sourc technolog support realtim big data workload develop dataacquisit workflow apach storm nifi java python captur patient monitor laboratori data downstream analyt result emerg data manag approach along open sourc technolog hadoop use creat integr data lake store larg realtim dataset infrastructur also provid robust analyt platform health care biomed research data analyz near real time precis medicin comput health care use case conclus implement use integr data scienc platform offer organ opportun combin tradit dataset includ data electron health record emerg big data sourc continu patient monitor realtim laboratori result platform enabl costeffect scalabl analyt inform key deliveri precis medicin initi organ take advantag technic advanc found data scienc platform opportun provid comprehens access health care data comput health care precis medicin research,2019,60
Bayesian Optimization and Data Science,,,2019,111
Applications of data science to game learning analytics data: A systematic literature review,,,2019,107
"Explainable Artificial Intelligence (XAI): Concepts, Taxonomies, Opportunities and Challenges toward Responsible AI",,,2019,5569
Explanation in Artificial Intelligence: Insights from the Social Sciences,,,2017,3948
Sparks of Artificial General Intelligence: Early experiments with GPT-4,"Artificial intelligence (AI) researchers have been developing and refining large language models (LLMs) that exhibit remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. The latest model developed by OpenAI, GPT-4, was trained using an unprecedented scale of compute and data. In this paper, we report on our investigation of an early version of GPT-4, when it was still in active development by OpenAI. We contend that (this early version of) GPT-4 is part of a new cohort of LLMs (along with ChatGPT and Google's PaLM for example) that exhibit more general intelligence than previous AI models. We discuss the rising capabilities and implications of these models. We demonstrate that, beyond its mastery of language, GPT-4 can solve novel and difficult tasks that span mathematics, coding, vision, medicine, law, psychology and more, without needing any special prompting. Moreover, in all of these tasks, GPT-4's performance is strikingly close to human-level performance, and often vastly surpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4's capabilities, we believe that it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system. In our exploration of GPT-4, we put special emphasis on discovering its limitations, and we discuss the challenges ahead for advancing towards deeper and more comprehensive versions of AGI, including the possible need for pursuing a new paradigm that moves beyond next-word prediction. We conclude with reflections on societal influences of the recent technological leap and future research directions.",artifici intellig ai research develop refin larg languag model llm exhibit remark capabl across varieti domain task challeng understand learn cognit latest model develop openai gpt train use unpreced scale comput data paper report investig earli version gpt still activ develop openai contend earli version gpt part new cohort llm along chatgpt googl palm exampl exhibit gener intellig previou ai model discus rise capabl implic model demonstr beyond masteri languag gpt solv novel difficult task span mathemat code vision medicin law psycholog without need special prompt moreov task gpt perform strikingli close humanlevel perform often vastli surpass prior model chatgpt given breadth depth gpt capabl believ could reason view earli yet still incomplet version artifici gener intellig agi system explor gpt put special emphasi discov limit discus challeng ahead advanc toward deeper comprehens version agi includ possibl need pursu new paradigm move beyond nextword predict conclud reflect societ influenc recent technolog leap futur research direct,2023,2633
Peeking Inside the Black-Box: A Survey on Explainable Artificial Intelligence (XAI),"At the dawn of the fourth industrial revolution, we are witnessing a fast and widespread adoption of artificial intelligence (AI) in our daily life, which contributes to accelerating the shift towards a more algorithmic society. However, even with such unprecedented advancements, a key impediment to the use of AI-based systems is that they often lack transparency. Indeed, the black-box nature of these systems allows powerful predictions, but it cannot be directly explained. This issue has triggered a new debate on explainable AI (XAI). A research field holds substantial promise for improving trust and transparency of AI-based systems. It is recognized as the sine qua non for AI to continue making steady progress without disruption. This survey provides an entry point for interested researchers and practitioners to learn key aspects of the young and rapidly growing body of research related to XAI. Through the lens of the literature, we review the existing approaches regarding the topic, discuss trends surrounding its sphere, and present major research trajectories.",dawn fourth industri revolut wit fast widespread adopt artifici intellig ai daili life contribut acceler shift toward algorithm societi howev even unpreced advanc key impedi use aibas system often lack transpar inde blackbox natur system allow power predict cannot directli explain issu trigger new debat explain ai xai research field hold substanti promis improv trust transpar aibas system recogn sine qua non ai continu make steadi progress without disrupt survey provid entri point interest research practition learn key aspect young rapidli grow bodi research relat xai len literatur review exist approach regard topic discus trend surround sphere present major research trajectori,2018,3558
High-performance medicine: the convergence of human and artificial intelligence,,,2019,3848
"Adaptation in Natural and Artificial Systems: An Introductory Analysis with Applications to Biology, Control, and Artificial Intelligence","From the Publisher: 
Genetic algorithms are playing an increasingly important role in studies of complex adaptive systems, ranging from adaptive agents in economic theory to the use of machine learning techniques in the design of complex devices such as aircraft turbines and integrated circuits. Adaptation in Natural and Artificial Systems is the book that initiated this field of study, presenting the theoretical foundations and exploring applications. 
In its most familiar form, adaptation is a biological process, whereby organisms evolve by rearranging genetic material to survive in environments confronting them. In this now classic work, Holland presents a mathematical model that allows for the nonlinearity of such complex interactions. He demonstrates the model's universality by applying it to economics, physiological psychology, game theory, and artificial intelligence and then outlines the way in which this approach modifies the traditional views of mathematical genetics. 
Initially applying his concepts to simply defined artificial systems with limited numbers of parameters, Holland goes on to explore their use in the study of a wide range of complex, naturally occuring processes, concentrating on systems having multiple factors that interact in nonlinear ways. Along the way he accounts for major effects of coadaptation and coevolution: the emergence of building blocks, or schemata, that are recombined and passed on to succeeding generations to provide, innovations and improvements. 
John H. Holland is Professor of Psychology and Professor of Electrical Engineering and Computer Science at the University of Michigan. He is also Maxwell Professor at the Santa Fe Institute and isDirector of the University of Michigan/Santa Fe Institute Advanced Research Program.",publish genet algorithm play increasingli import role studi complex adapt system rang adapt agent econom theori use machin learn techniqu design complex devic aircraft turbin integr circuit adapt natur artifici system book initi field studi present theoret foundat explor applic familiar form adapt biolog process wherebi organ evolv rearrang genet materi surviv environ confront classic work holland present mathemat model allow nonlinear complex interact demonstr model univers appli econom physiolog psycholog game theori artifici intellig outlin way approach modifi tradit view mathemat genet initi appli concept simpli defin artifici system limit number paramet holland goe explor use studi wide rang complex natur occur process concentr system multipl factor interact nonlinear way along way account major effect coadapt coevolut emerg build block schema recombin pas succeed gener provid innov improv john h holland professor psycholog professor electr engin comput scienc univers michigan also maxwel professor santa fe institut isdirector univers michigansanta fe institut advanc research program,1992,40065
Impact of Artificial Intelligence in Customer Journey,"The entire gamut of Customer journey is undergoing a massive transformation due to the rapid advancement of Artificial Intelligence (AI). Leveraging the power of AI , CRM & systems have refined the aspect of how businesses manage and optimize the customer journey. AI-powered systems have significant impact across various stages of the customer lifecycle by use of techniques such as machine learning to empower businesses to use systems that can analyse vast amounts of customer dataset in real-time, enabling them to gain deeper insights in customer behaviours, preferences, & sentiment. The AI-driven techniques help businesses to drive more personalized & targeted marketing campaigns, tailored recommendations, and extend efficient customer service leading ultimately to enhancing customer satisfaction and loyalty. Moreover, AI-powered systems have capabilities of offering predictive analytics which empower businesses to forecast customer behaviours and anticipate their needs. The capabilities help businesses in effective resource optimization and improve efficiency. For customer service AI-powered chatbots and virtual assistants are used to enhance engagement by providing instant responses and ability to handle resolving issues promptly.",entir gamut custom journey undergo massiv transform due rapid advanc artifici intellig ai leverag power ai crm system refin aspect busi manag optim custom journey aipow system signific impact across variou stage custom lifecycl use techniqu machin learn empow busi use system analys vast amount custom dataset realtim enabl gain deeper insight custom behaviour prefer sentiment aidriven techniqu help busi drive person target market campaign tailor recommend extend effici custom servic lead ultim enhanc custom satisfact loyalti moreov aipow system capabl offer predict analyt empow busi forecast custom behaviour anticip need capabl help busi effect resourc optim improv effici custom servic aipow chatbot virtual assist use enhanc engag provid instant respons abil handl resolv issu promptli,2024,207
ARTIFICIAL INTELLIGENCE FOR THE REAL WORLD,,,2023,630
Explainable Artificial Intelligence (XAI),"Explainable Artificial Intelligence (XAI) has emerged as a critical facet in the realm of machine learning and artificial intelligence, responding to the increasing complexity of models, particularly deep neural networks, and the subsequent need for transparent decision making processes. This research paper delves into the essence of XAI, unraveling its significance across diverse domains such as healthcare, finance, and criminal justice. As a countermeasure to the opacity of intricate models, the paper explores various XAI methods and techniques, including LIME and SHAP, weighing their interpretability against computational efficiency and accuracy. Through an examination of real-world applications, the research elucidates how XAI not only enhances decision-making processes but also influences user trust and acceptance in AI systems. However, the paper also scrutinizes the delicate balance between interpretability and performance, shedding light on instances where the pursuit of accuracy may compromise explain-ability. Additionally, it navigates through the current challenges and limitations in XAI, the regulatory landscape surrounding AI explain-ability, and offers insights into future trends and directions, fostering a comprehensive understanding of XAI's present state and future potential.",explain artifici intellig xai emerg critic facet realm machin learn artifici intellig respond increas complex model particularli deep neural network subsequ need transpar decis make process research paper delv essenc xai unravel signific across diver domain healthcar financ crimin justic countermeasur opac intric model paper explor variou xai method techniqu includ lime shap weigh interpret comput effici accuraci examin realworld applic research elucid xai enhanc decisionmak process also influenc user trust accept ai system howev paper also scrutin delic balanc interpret perform shed light instanc pursuit accuraci may compromis explain addit navig current challeng limit xai regulatori landscap surround ai explain offer insight futur trend direct foster comprehens understand xai present state futur potenti,2023,651
Artificial Intelligence and the Future of Work,,,2024,68
Foundation models for generalist medical artificial intelligence,,,2023,713
Revolutionizing healthcare: the role of artificial intelligence in clinical practice,,,2023,694
Scientific discovery in the age of artificial intelligence,,,2023,615
Artificial intelligence in higher education: the state of the field,,,2023,367
Examining Science Education in ChatGPT: An Exploratory Study of Generative Artificial Intelligence,,,2023,512
Can artificial intelligence help for scientific writing?,,,2023,457
"A Conversation on Artificial Intelligence, Chatbots, and Plagiarism in Higher Education",,,2023,372
Collaborating With ChatGPT: Considering the Implications of Generative Artificial Intelligence for Journalism and Media Education,Generative artificial intelligence (AI) is ushering in an era of potential transformation of journalism and media content. This essay considers one notable generative AI platform called ChatGPT made available to the public in 2022 for free use. ChatGPT allows users to enter text prompts and rapidly generates text responses drawn from its knowledge acquired via machine learning in engagement with the internet. This essay is coauthored by a human journalism and media professor in collaboration with ChatGPT. The essay demonstrates the capacity and limitations of ChatGPT and offers reflections on the implications of generative AI for journalism and media education.,gener artifici intellig ai usher era potenti transform journal medium content essay consid one notabl gener ai platform call chatgpt made avail public free use chatgpt allow user enter text prompt rapidli gener text respons drawn knowledg acquir via machin learn engag internet essay coauthor human journal medium professor collabor chatgpt essay demonstr capac limit chatgpt offer reflect implic gener ai journal medium educ,2023,517
Experimental evidence on the productivity effects of generative artificial intelligence,"We examined the productivity effects of a generative artificial intelligence (AI) technology, the assistive chatbot ChatGPT, in the context of midlevel professional writing tasks. In a preregistered online experiment, we assigned occupation-specific, incentivized writing tasks to 453 college-educated professionals and randomly exposed half of them to ChatGPT. Our results show that ChatGPT substantially raised productivity: The average time taken decreased by 40% and output quality rose by 18%. Inequality between workers decreased, and concern and excitement about AI temporarily rose. Workers exposed to ChatGPT during the experiment were 2 times as likely to report using it in their real job 2 weeks after the experiment and 1.6 times as likely 2 months after the experiment. Description Editor’s summary Automation has historically displaced human workers in factories (e.g., automotive manufacturing) or in performing routine computational tasks. Will generative artificial intelligence (AI) tools such as ChatGPT disrupt the labor market by making educated professionals obsolete, or will these tools complement their skills and enhance productivity? Noy and Zhang examined this issue in an experiment that recruited college-educated professionals to complete incentivized writing tasks. Participants assigned to use ChatGPT were more productive, efficient, and enjoyed the tasks more. Participants with weaker skills benefited the most from ChatGPT, which carries policy implications for efforts to reduce productivity inequality through AI. —EEU The assistive chatbot ChatGPT raises productivity in professional writing tasks and reduces productivity inequality.",examin product effect gener artifici intellig ai technolog assist chatbot chatgpt context midlevel profession write task preregist onlin experi assign occupationspecif incentiv write task collegeeduc profession randomli expo half chatgpt result show chatgpt substanti rais product averag time taken decreas output qualiti rose inequ worker decreas concern excit ai temporarili rose worker expo chatgpt experi time like report use real job week experi time like month experi descript editor summari autom histor displac human worker factori eg automot manufactur perform routin comput task gener artifici intellig ai tool chatgpt disrupt labor market make educ profession obsolet tool complement skill enhanc product noy zhang examin issu experi recruit collegeeduc profession complet incentiv write task particip assign use chatgpt product effici enjoy task particip weaker skill benefit chatgpt carri polici implic effort reduc product inequ ai eeu assist chatbot chatgpt rais product profession write task reduc product inequ,2023,472
Managing artificial intelligence,,,2023,241
Artificial Intelligence in Pharmaceutical Technology and Drug Delivery Design,"Artificial intelligence (AI) has emerged as a powerful tool that harnesses anthropomorphic knowledge and provides expedited solutions to complex challenges. Remarkable advancements in AI technology and machine learning present a transformative opportunity in the drug discovery, formulation, and testing of pharmaceutical dosage forms. By utilizing AI algorithms that analyze extensive biological data, including genomics and proteomics, researchers can identify disease-associated targets and predict their interactions with potential drug candidates. This enables a more efficient and targeted approach to drug discovery, thereby increasing the likelihood of successful drug approvals. Furthermore, AI can contribute to reducing development costs by optimizing research and development processes. Machine learning algorithms assist in experimental design and can predict the pharmacokinetics and toxicity of drug candidates. This capability enables the prioritization and optimization of lead compounds, reducing the need for extensive and costly animal testing. Personalized medicine approaches can be facilitated through AI algorithms that analyze real-world patient data, leading to more effective treatment outcomes and improved patient adherence. This comprehensive review explores the wide-ranging applications of AI in drug discovery, drug delivery dosage form designs, process optimization, testing, and pharmacokinetics/pharmacodynamics (PK/PD) studies. This review provides an overview of various AI-based approaches utilized in pharmaceutical technology, highlighting their benefits and drawbacks. Nevertheless, the continued investment in and exploration of AI in the pharmaceutical industry offer exciting prospects for enhancing drug development processes and patient care.",artifici intellig ai emerg power tool har anthropomorph knowledg provid expedit solut complex challeng remark advanc ai technolog machin learn present transform opportun drug discoveri formul test pharmaceut dosag form util ai algorithm analyz extens biolog data includ genom proteom research identifi diseaseassoci target predict interact potenti drug candid enabl effici target approach drug discoveri therebi increas likelihood success drug approv furthermor ai contribut reduc develop cost optim research develop process machin learn algorithm assist experiment design predict pharmacokinet toxic drug candid capabl enabl priorit optim lead compound reduc need extens costli anim test person medicin approach facilit ai algorithm analyz realworld patient data lead effect treatment outcom improv patient adher comprehens review explor widerang applic ai drug discoveri drug deliveri dosag form design process optim test pharmacokineticspharmacodynam pkpd studi review provid overview variou aibas approach util pharmaceut technolog highlight benefit drawback nevertheless continu invest explor ai pharmaceut industri offer excit prospect enhanc drug develop process patient care,2023,236
New Era of Artificial Intelligence in Education: Towards a Sustainable Multifaceted Revolution,"The recent high performance of ChatGPT on several standardized academic tests has thrust the topic of artificial intelligence (AI) into the mainstream conversation about the future of education. As deep learning is poised to shift the teaching paradigm, it is essential to have a clear understanding of its effects on the current education system to ensure sustainable development and deployment of AI-driven technologies at schools and universities. This research aims to investigate the potential impact of AI on education through review and analysis of the existing literature across three major axes: applications, advantages, and challenges. Our review focuses on the use of artificial intelligence in collaborative teacher–student learning, intelligent tutoring systems, automated assessment, and personalized learning. We also report on the potential negative aspects, ethical issues, and possible future routes for AI implementation in education. Ultimately, we find that the only way forward is to embrace the new technology, while implementing guardrails to prevent its abuse.",recent high perform chatgpt sever standard academ test thrust topic artifici intellig ai mainstream convers futur educ deep learn poi shift teach paradigm essenti clear understand effect current educ system ensur sustain develop deploy aidriven technolog school univers research aim investig potenti impact ai educ review analysi exist literatur across three major axe applic advantag challeng review focus use artifici intellig collabor teacherstud learn intellig tutor system autom assess person learn also report potenti neg aspect ethic issu possibl futur rout ai implement educ ultim find way forward embrac new technolog implement guardrail prevent abus,2023,254
Artificial Intelligence in Medicine,"Artificial Intelligence in Medicine is looking for novelty in the methodological and/or theoretical content of submitted papers. Such kind of novelty has to be mainly acknowledged in the area of AI and Computer Science. Methodological papers deal with the proposal of some strategy and related methods to solve some scientific issues in specific domains. They must show, usually through an experimental evaluation, how the proposed methodology can be applied to medicine, medicallyoriented human biology, and health care, respectively. They have also to provide a comparison with other proposals, and explicitly discuss elements of novelty. Theoretical papers focus on more fundamental, general and formal topics of AI and must show the novel expected effects of the proposed solution in some medical or healthcare field.",artifici intellig medicin look novelti methodolog andor theoret content submit paper kind novelti mainli acknowledg area ai comput scienc methodolog paper deal propos strategi relat method solv scientif issu specif domain must show usual experiment evalu propos methodolog appli medicin medicallyori human biolog health care respect also provid comparison propos explicitli discus element novelti theoret paper focu fundament gener formal topic ai must show novel expect effect propos solut medic healthcar field,2023,213
CS 188 Introduction to Artificial Intelligence Fall 2023,"In order to create a rational planning agent, we need a way to mathematically express the given environment in which the agent will exist. To do this, we must formally express a search problem given our agent’s current state (its configuration within its environment), how can we arrive at a new state that satisfies its goals in the best possible way? A search problem consists of the following elements:",order creat ration plan agent need way mathemat express given environ agent exist must formal express search problem given agent current state configur within environ arriv new state satisfi goal best possibl way search problem consist follow element,2023,935
Appropriateness of Cardiovascular Disease Prevention Recommendations Obtained From a Popular Online Chat-Based Artificial Intelligence Model.,"
 This study examines the appropriateness of artificial intelligence model responses to fundamental cardiovascular disease prevention questions.
",studi examin appropri artifici intellig model respons fundament cardiovascular diseas prevent question,2023,271
A Review of the Role of Artificial Intelligence in Healthcare,"Artificial intelligence (AI) applications have transformed healthcare. This study is based on a general literature review uncovering the role of AI in healthcare and focuses on the following key aspects: (i) medical imaging and diagnostics, (ii) virtual patient care, (iii) medical research and drug discovery, (iv) patient engagement and compliance, (v) rehabilitation, and (vi) other administrative applications. The impact of AI is observed in detecting clinical conditions in medical imaging and diagnostic services, controlling the outbreak of coronavirus disease 2019 (COVID-19) with early diagnosis, providing virtual patient care using AI-powered tools, managing electronic health records, augmenting patient engagement and compliance with the treatment plan, reducing the administrative workload of healthcare professionals (HCPs), discovering new drugs and vaccines, spotting medical prescription errors, extensive data storage and analysis, and technology-assisted rehabilitation. Nevertheless, this science pitch meets several technical, ethical, and social challenges, including privacy, safety, the right to decide and try, costs, information and consent, access, and efficacy, while integrating AI into healthcare. The governance of AI applications is crucial for patient safety and accountability and for raising HCPs’ belief in enhancing acceptance and boosting significant health consequences. Effective governance is a prerequisite to precisely address regulatory, ethical, and trust issues while advancing the acceptance and implementation of AI. Since COVID-19 hit the global health system, the concept of AI has created a revolution in healthcare, and such an uprising could be another step forward to meet future healthcare needs.",artifici intellig ai applic transform healthcar studi base gener literatur review uncov role ai healthcar focus follow key aspect medic imag diagnost ii virtual patient care iii medic research drug discoveri iv patient engag complianc v rehabilit vi administr applic impact ai observ detect clinic condit medic imag diagnost servic control outbreak coronaviru diseas covid earli diagnosi provid virtual patient care use aipow tool manag electron health record augment patient engag complianc treatment plan reduc administr workload healthcar profession hcp discov new drug vaccin spot medic prescript error extens data storag analysi technologyassist rehabilit nevertheless scienc pitch meet sever technic ethic social challeng includ privaci safeti right decid tri cost inform consent access efficaci integr ai healthcar govern ai applic crucial patient safeti account rais hcp belief enhanc accept boost signific health consequ effect govern prerequisit precis address regulatori ethic trust issu advanc accept implement ai sinc covid hit global health system concept ai creat revolut healthcar upris could anoth step forward meet futur healthcar need,2023,190
Data-centric Artificial Intelligence: A Survey,"
 Artificial Intelligence (AI) is making a profound impact in almost every domain. A vital enabler of its great success is the availability of abundant and high-quality data for building machine learning models. Recently, the role of data in AI has been significantly magnified, giving rise to the emerging concept of
 data-centric AI
 . The attention of researchers and practitioners has gradually shifted from advancing model design to enhancing the quality and quantity of the data. In this survey, we discuss the necessity of data-centric AI, followed by a holistic view of three general data-centric goals (training data development, inference data development, and data maintenance) and the representative methods. We also organize the existing literature from automation and collaboration perspectives, discuss the challenges, and tabulate the benchmarks for various tasks. We believe this is the first comprehensive survey that provides a global view of a spectrum of tasks across various stages of the data lifecycle. We hope it can help the readers efficiently grasp a broad picture of this field, and equip them with the techniques and further research ideas to systematically engineer data for building AI systems. A companion list of data-centric AI resources will be regularly updated on https://github.com/daochenzha/data-centric-AI
",artifici intellig ai make profound impact almost everi domain vital enabl great success avail abund highqual data build machin learn model recent role data ai significantli magnifi give rise emerg concept datacentr ai attent research practition gradual shift advanc model design enhanc qualiti quantiti data survey discus necess datacentr ai follow holist view three gener datacentr goal train data develop infer data develop data mainten repres method also organ exist literatur autom collabor perspect discus challeng tabul benchmark variou task believ first comprehens survey provid global view spectrum task across variou stage data lifecycl hope help reader effici grasp broad pictur field equip techniqu research idea systemat engin data build ai system companion list datacentr ai resourc regularli updat httpsgithubcomdaochenzhadatacentricai,2023,150
Explainable Artificial Intelligence (XAI): What we know and what is left to attain Trustworthy Artificial Intelligence,,,2023,490
"Artificial Intelligence and Machine Learning in Clinical Medicine, 2023.",,,2023,328
Artificial Intelligence,,,2017,2755
Trustworthy artificial intelligence,,,2023,159
Empowering Education with Generative Artificial Intelligence Tools: Approach with an Instructional Design Matrix,"This study focuses on the potential of generative artificial intelligence tools in education, particularly through the practical application of the 4PADAFE instructional design matrix. The objective was to evaluate how these tools, in combination with the matrix, can enhance education and improve the teaching–learning process. Through surveys conducted with teachers from the University of ESPE Armed Forces who participated in the MOOC course “Generative Artificial Intelligence Tools for Education: GPT Chat Techniques”, the study explores the impact of these tools on education. The findings reveal that generative artificial intelligence tools are crucial in developing massive MOOC virtual classrooms when integrated with an instructional design matrix. The results demonstrate the potential of generative artificial intelligence tools in university education. By utilizing these tools in conjunction with an instructional design matrix, educators can design and deliver personalized and enriching educational experiences. The devices offer opportunities to enhance the teaching–learning process and tailor educational materials to individual needs, ultimately preparing students for the demands of the 21st century. The study concludes that generative artificial intelligence tools have significant potential in education. They provide innovative ways to engage students, adapt content, and promote personalized learning. Implementing the 4PADAFE instructional design matrix further enhances the effectiveness and coherence of educational activities. By embracing these technological advancements, education can stay relevant and effectively meet the digital world’s challenges.",studi focus potenti gener artifici intellig tool educ particularli practic applic padaf instruct design matrix object evalu tool combin matrix enhanc educ improv teachinglearn process survey conduct teacher univers esp arm forc particip mooc cours gener artifici intellig tool educ gpt chat techniqu studi explor impact tool educ find reveal gener artifici intellig tool crucial develop massiv mooc virtual classroom integr instruct design matrix result demonstr potenti gener artifici intellig tool univers educ util tool conjunct instruct design matrix educ design deliv person enrich educ experi devic offer opportun enhanc teachinglearn process tailor educ materi individu need ultim prepar student demand st centuri studi conclud gener artifici intellig tool signific potenti educ provid innov way engag student adapt content promot person learn implement padaf instruct design matrix enhanc effect coher educ activ embrac technolog advanc educ stay relev effect meet digit world challeng,2023,110
Artificial intelligence in developing countries: The impact of generative artificial intelligence (AI) technologies for development,"This paper explores the potential impact of Generative Artificial Intelligence (Generative AI) on developing countries, considering both positive and negative effects across various domains of information, culture, and industry. Generative Artificial Intelligence refers to artificial intelligence (AI) systems that generate content, such as text, audio, or video, aiming to produce novel and creative outputs based on training data. Compared to conversational artificial intelligence, generative artificial intelligence systems have the unique capability of not only providing replies but also generating the content of those responses. Recent advancements in Artificial Intelligence during the Fourth Industrial Revolution, exemplified by tools like ChatGPT, have gained popularity and reshaped content production and creation. However, the benefits of generative artificial intelligence are not equally accessible to all, especially in developing countries, where limited access to cutting-edge technologies and inadequate infrastructure pose challenges. This paper seeks to understand the potential impact of generative AI technologies on developing countries, considering economic growth, access to technology, and the potential paradigm shift in education, healthcare, and the environment. The findings emphasize the importance of providing the necessary support and infrastructure to ensure that generative AI contributes to inclusive development rather than deepening existing inequalities. The study highlights the significance of integrating Generative AI into the context of the Fourth Industrial Revolution in developing countries, where technological change is a crucial determinant of progress and equitable growth.",paper explor potenti impact gener artifici intellig gener ai develop countri consid posit neg effect across variou domain inform cultur industri gener artifici intellig refer artifici intellig ai system gener content text audio video aim produc novel creativ output base train data compar convers artifici intellig gener artifici intellig system uniqu capabl provid repli also gener content respons recent advanc artifici intellig fourth industri revolut exemplifi tool like chatgpt gain popular reshap content product creation howev benefit gener artifici intellig equal access especi develop countri limit access cuttingedg technolog inadequ infrastructur pose challeng paper seek understand potenti impact gener ai technolog develop countri consid econom growth access technolog potenti paradigm shift educ healthcar environ find emphas import provid necessari support infrastructur ensur gener ai contribut inclus develop rather deepen exist inequ studi highlight signific integr gener ai context fourth industri revolut develop countri technolog chang crucial determin progress equit growth,2023,90
An Overview of Artificial Intelligence Ethics,"Artificial intelligence (AI) has profoundly changed and will continue to change our lives. AI is being applied in more and more fields and scenarios such as autonomous driving, medical care, media, finance, industrial robots, and internet services. The widespread application of AI and its deep integration with the economy and society have improved efficiency and produced benefits. At the same time, it will inevitably impact the existing social order and raise ethical concerns. Ethical issues, such as privacy leakage, discrimination, unemployment, and security risks, brought about by AI systems have caused great trouble to people. Therefore, AI ethics, which is a field related to the study of ethical issues in AI, has become not only an important research topic in academia, but also an important topic of common concern for individuals, organizations, countries, and society. This article will give a comprehensive overview of this field by summarizing and analyzing the ethical risks and issues raised by AI, ethical guidelines and principles issued by different organizations, approaches for addressing ethical issues in AI, and methods for evaluating the ethics of AI. Additionally, challenges in implementing ethics in AI and some future perspectives are pointed out. We hope our work will provide a systematic and comprehensive overview of AI ethics for researchers and practitioners in this field, especially the beginners of this research discipline.",artifici intellig ai profoundli chang continu chang live ai appli field scenario autonom drive medic care medium financ industri robot internet servic widespread applic ai deep integr economi societi improv effici produc benefit time inevit impact exist social order rais ethic concern ethic issu privaci leakag discrimin unemploy secur risk brought ai system caus great troubl peopl therefor ai ethic field relat studi ethic issu ai becom import research topic academia also import topic common concern individu organ countri societi articl give comprehens overview field summar analyz ethic risk issu rais ai ethic guidelin principl issu differ organ approach address ethic issu ai method evalu ethic ai addit challeng implement ethic ai futur perspect point hope work provid systemat comprehens overview ai ethic research practition field especi beginn research disciplin,2023,85
Smart farming using artificial intelligence: A review,,,2023,160
Evaluation of artificial intelligence techniques in disease diagnosis and prediction,,,2023,121
Artificial intelligence for waste management in smart cities: a review,,,2023,117
"From Artificial Intelligence to Explainable Artificial Intelligence in Industry 4.0: A Survey on What, How, and Where","Nowadays, Industry 4.0 can be considered a reality, a paradigm integrating modern technologies and innovations. Artificial intelligence (AI) can be considered the leading component of the industrial transformation enabling intelligent machines to execute tasks autonomously such as self-monitoring, interpretation, diagnosis, and analysis. AI-based methodologies (especially machine learning and deep learning support manufacturers and industries in predicting their maintenance needs and reducing downtime. Explainable artificial intelligence (XAI) studies and designs approaches, algorithms and tools producing human-understandable explanations of AI-based systems information and decisions. This article presents a comprehensive survey of AI and XAI-based methods adopted in the Industry 4.0 scenario. First, we briefly discuss different technologies enabling Industry 4.0. Then, we present an in-depth investigation of the main methods used in the literature: we also provide the details of what, how, why, and where these methods have been applied for Industry 4.0. Furthermore, we illustrate the opportunities and challenges that elicit future research directions toward responsible or human-centric AI and XAI systems, essential for adopting high-stakes industry applications.",nowaday industri consid realiti paradigm integr modern technolog innov artifici intellig ai consid lead compon industri transform enabl intellig machin execut task autonom selfmonitor interpret diagnosi analysi aibas methodolog especi machin learn deep learn support manufactur industri predict mainten need reduc downtim explain artifici intellig xai studi design approach algorithm tool produc humanunderstand explan aibas system inform decis articl present comprehens survey ai xaibas method adopt industri scenario first briefli discus differ technolog enabl industri present indepth investig main method use literatur also provid detail method appli industri furthermor illustr opportun challeng elicit futur research direct toward respons humancentr ai xai system essenti adopt highstak industri applic,2022,338
Artificial intelligence in radiology,,,2018,2353
"Generative artificial intelligence empowers educational reform: current status, issues, and prospects","The emergence of Chat GPT has once again sparked a wave of information revolution in generative artificial intelligence. This article provides a detailed overview of the development and technical support of generative artificial intelligence. It conducts an in-depth analysis of the current application of generative artificial intelligence in the field of education, and identifies problems in four aspects: opacity and unexplainability, data privacy and security, personalization and fairness, and effectiveness and reliability. Corresponding solutions are proposed, such as developing explainable and fair algorithms, upgrading encryption technology, and formulating relevant laws and regulations to protect data, as well as improving the quality and quantity of datasets. The article also looks ahead to the future development trends of generative artificial intelligence in education from four perspectives: personalized education, intelligent teaching, collaborative education, and virtual teaching. The aim of the study is to provide important reference value for research and practice in this field.",emerg chat gpt spark wave inform revolut gener artifici intellig articl provid detail overview develop technic support gener artifici intellig conduct indepth analysi current applic gener artifici intellig field educ identifi problem four aspect opac unexplain data privaci secur person fair effect reliabl correspond solut propos develop explain fair algorithm upgrad encrypt technolog formul relev law regul protect data well improv qualiti quantiti dataset articl also look ahead futur develop trend gener artifici intellig educ four perspect person educ intellig teach collabor educ virtual teach aim studi provid import refer valu research practic field,2023,96
Artificial intelligence-based solutions for climate change: a review,,,2023,92
Artificial Intelligence A Modern Approach 3rd,"Artificial IntelligenceArtificial Intelligence: A Modern Approach 2Nd Ed.Introduction to Machine LearningArtificial IntelligenceArtificial Intelligence: A Modern Approach, eBook, Global EditionIntroduction to Artificial IntelligenceModern Approaches in Machine Learning and Cognitive Science: A WalkthroughArtificial Intelligence: Pearson New International EditionArtificial IntelligenceArtificial IntelligenceArtificial IntelligenceArtificial Intelligence a Modern ApproachFundamentals of the New Artificial IntelligenceMultiagent SystemsArtificial IntelligenceArtificial IntelligenceThe Hundred-page Machine Learning BookArtificial IntelligenceArtificial IntelligenceArtificial IntelligenceDistributed Artificial IntelligenceArtificial Intelligence For BeginnersParadigms of Artificial Intelligence ProgrammingHuman CompatibleHuman CompatibleARTIFICIAL INTELLIGENCEArtificial IntelligenceArtificial IntelligenceArtificial Intelligence a Modern ApproachDo the Right ThingArtificial IntelligenceArtificial Intelligence : a Modern ApproachArtificial IntelligenceIntelligent Help Systems for UNIXArtificial IntelligenceArtificial IntelligenceArtificial Intelligence a Modern ApproachArtificial IntelligenceArtificial IntelligenceArtificial Intelligence for Human Computer Interaction: A Modern Approach",artifici intelligenceartifici intellig modern approach nd edintroduct machin learningartifici intelligenceartifici intellig modern approach ebook global editionintroduct artifici intelligencemodern approach machin learn cognit scienc walkthroughartifici intellig pearson new intern editionartifici intelligenceartifici intelligenceartifici intelligenceartifici intellig modern approachfundament new artifici intelligencemultiag systemsartifici intelligenceartifici intelligenceth hundredpag machin learn bookartifici intelligenceartifici intelligenceartifici intelligencedistribut artifici intelligenceartifici intellig beginnersparadigm artifici intellig programminghuman compatiblehuman compatibleartifici intelligenceartifici intelligenceartifici intelligenceartifici intellig modern approachdo right thingartifici intelligenceartifici intellig modern approachartifici intelligenceintellig help system unixartifici intelligenceartifici intelligenceartifici intellig modern approachartifici intelligenceartifici intelligenceartifici intellig human comput interact modern approach,2022,311
Empowering learners for the age of artificial intelligence,,,2023,95
"Artificial intelligence in healthcare: past, present and future","Artificial intelligence (AI) aims to mimic human cognitive functions. It is bringing a paradigm shift to healthcare, powered by increasing availability of healthcare data and rapid progress of analytics techniques. We survey the current status of AI applications in healthcare and discuss its future. AI can be applied to various types of healthcare data (structured and unstructured). Popular AI techniques include machine learning methods for structured data, such as the classical support vector machine and neural network, and the modern deep learning, as well as natural language processing for unstructured data. Major disease areas that use AI tools include cancer, neurology and cardiology. We then review in more details the AI applications in stroke, in the three major areas of early detection and diagnosis, treatment, as well as outcome prediction and prognosis evaluation. We conclude with discussion about pioneer AI systems, such as IBM Watson, and hurdles for real-life deployment of AI.",artifici intellig ai aim mimic human cognit function bring paradigm shift healthcar power increas avail healthcar data rapid progress analyt techniqu survey current statu ai applic healthcar discus futur ai appli variou type healthcar data structur unstructur popular ai techniqu includ machin learn method structur data classic support vector machin neural network modern deep learn well natur languag process unstructur data major diseas area use ai tool includ cancer neurolog cardiolog review detail ai applic stroke three major area earli detect diagnosi treatment well outcom predict prognosi evalu conclud discus pioneer ai system ibm watson hurdl reallif deploy ai,2017,2531
Legal and Ethical Consideration in Artificial Intelligence in Healthcare: Who Takes Responsibility?,"The legal and ethical issues that confront society due to Artificial Intelligence (AI) include privacy and surveillance, bias or discrimination, and potentially the philosophical challenge is the role of human judgment. Concerns about newer digital technologies becoming a new source of inaccuracy and data breaches have arisen as a result of its use. Mistakes in the procedure or protocol in the field of healthcare can have devastating consequences for the patient who is the victim of the error. Because patients come into contact with physicians at moments in their lives when they are most vulnerable, it is crucial to remember this. Currently, there are no well-defined regulations in place to address the legal and ethical issues that may arise due to the use of artificial intelligence in healthcare settings. This review attempts to address these pertinent issues highlighting the need for algorithmic transparency, privacy, and protection of all the beneficiaries involved and cybersecurity of associated vulnerabilities.",legal ethic issu confront societi due artifici intellig ai includ privaci surveil bia discrimin potenti philosoph challeng role human judgment concern newer digit technolog becom new sourc inaccuraci data breach arisen result use mistak procedur protocol field healthcar devast consequ patient victim error patient come contact physician moment live vulner crucial rememb current welldefin regul place address legal ethic issu may aris due use artifici intellig healthcar set review attempt address pertin issu highlight need algorithm transpar privaci protect beneficiari involv cybersecur associ vulner,2022,384
Trustworthy Artificial Intelligence: A Review,"Artificial intelligence (AI) and algorithmic decision making are having a profound impact on our daily lives. These systems are vastly used in different high-stakes applications like healthcare, business, government, education, and justice, moving us toward a more algorithmic society. However, despite so many advantages of these systems, they sometimes directly or indirectly cause harm to the users and society. Therefore, it has become essential to make these systems safe, reliable, and trustworthy. Several requirements, such as fairness, explainability, accountability, reliability, and acceptance, have been proposed in this direction to make these systems trustworthy. This survey analyzes all of these different requirements through the lens of the literature. It provides an overview of different approaches that can help mitigate AI risks and increase trust and acceptance of the systems by utilizing the users and society. It also discusses existing strategies for validating and verifying these systems and the current standardization efforts for trustworthy AI. Finally, we present a holistic view of the recent advancements in trustworthy AI to help the interested researchers grasp the crucial facets of the topic efficiently and offer possible future research directions.",artifici intellig ai algorithm decis make profound impact daili live system vastli use differ highstak applic like healthcar busi govern educ justic move u toward algorithm societi howev despit mani advantag system sometim directli indirectli caus harm user societi therefor becom essenti make system safe reliabl trustworthi sever requir fair explain account reliabl accept propos direct make system trustworthi survey analyz differ requir len literatur provid overview differ approach help mitig ai risk increas trust accept system util user societi also discus exist strategi valid verifi system current standard effort trustworthi ai final present holist view recent advanc trustworthi ai help interest research grasp crucial facet topic effici offer possibl futur research direct,2022,324
"Artificial intelligence in disease diagnosis: a systematic literature review, synthesizing framework and future research agenda",,,2022,454
Ethical principles for artificial intelligence in education,,,2022,273
Artificial Intelligence in Education: A Review,"The purpose of this study was to assess the impact of Artificial Intelligence (AI) on education. Premised on a narrative and framework for assessing AI identified from a preliminary analysis, the scope of the study was limited to the application and effects of AI in administration, instruction, and learning. A qualitative research approach, leveraging the use of literature review as a research design and approach was used and effectively facilitated the realization of the study purpose. Artificial intelligence is a field of study and the resulting innovations and developments that have culminated in computers, machines, and other artifacts having human-like intelligence characterized by cognitive abilities, learning, adaptability, and decision-making capabilities. The study ascertained that AI has extensively been adopted and used in education, particularly by education institutions, in different forms. AI initially took the form of computer and computer related technologies, transitioning to web-based and online intelligent education systems, and ultimately with the use of embedded computer systems, together with other technologies, the use of humanoid robots and web-based chatbots to perform instructors’ duties and functions independently or with instructors. Using these platforms, instructors have been able to perform different administrative functions, such as reviewing and grading students’ assignments more effectively and efficiently, and achieve higher quality in their teaching activities. On the other hand, because the systems leverage machine learning and adaptability, curriculum and content has been customized and personalized in line with students’ needs, which has fostered uptake and retention, thereby improving learners experience and overall quality of learning.",purpos studi assess impact artifici intellig ai educ premis narr framework assess ai identifi preliminari analysi scope studi limit applic effect ai administr instruct learn qualit research approach leverag use literatur review research design approach use effect facilit realiz studi purpos artifici intellig field studi result innov develop culmin comput machin artifact humanlik intellig character cognit abil learn adapt decisionmak capabl studi ascertain ai extens adopt use educ particularli educ institut differ form ai initi took form comput comput relat technolog transit webbas onlin intellig educ system ultim use embed comput system togeth technolog use humanoid robot webbas chatbot perform instructor duti function independ instructor use platform instructor abl perform differ administr function review grade student assign effect effici achiev higher qualiti teach activ hand system leverag machin learn adapt curriculum content custom person line student need foster uptak retent therebi improv learner experi overal qualiti learn,2020,1037
Artificial Intelligence and Jobs: Evidence from Online Vacancies,"We study the impact of artificial intelligence (AI) on labor markets using establishment-level data on the near universe of online vacancies in the United States from 2010 onward. There is rapid growth in AI-related vacancies over 2010–18 that is driven by establishments whose workers engage in tasks compatible with AI’s current capabilities. As these AI-exposed establishments adopt AI, they simultaneously reduce hiring in non-AI positions and change the skill requirements of remaining postings. While visible at the establishment level, the aggregate impacts of AI-labor substitution on employment and wage growth in more exposed occupations and industries is currently too small to be detectable.",studi impact artifici intellig ai labor market use establishmentlevel data near univers onlin vacanc unit state onward rapid growth airel vacanc driven establish whose worker engag task compat ai current capabl aiexpos establish adopt ai simultan reduc hire nonai posit chang skill requir remain post visibl establish level aggreg impact ailabor substitut employ wage growth expo occup industri current small detect,2022,227
Artificial Intelligence for Remote Sensing Data Analysis: A review of challenges and opportunities,"Artificial intelligence (AI) plays a growing role in remote sensing (RS). Applications of AI, particularly machine learning algorithms, range from initial image processing to high-level data understanding and knowledge discovery. AI techniques have emerged as a powerful strategy for analyzing RS data and led to remarkable breakthroughs in all RS fields. Given this period of breathtaking evolution, this work aims to provide a comprehensive review of the recent achievements of AI algorithms and applications in RS data analysis. The review includes more than 270 research papers, covering the following major aspects of AI innovation for RS: machine learning, computational intelligence, AI explicability, data mining, natural language processing (NLP), and AI security. We conclude this review by identifying promising directions for future research.",artifici intellig ai play grow role remot sen r applic ai particularli machin learn algorithm rang initi imag process highlevel data understand knowledg discoveri ai techniqu emerg power strategi analyz r data led remark breakthrough r field given period breathtak evolut work aim provid comprehens review recent achiev ai algorithm applic r data analysi review includ research paper cover follow major aspect ai innov r machin learn comput intellig ai explic data mine natur languag process nlp ai secur conclud review identifi promis direct futur research,2022,267
A Proposal For The Dartmouth Summer Research Project On Artificial Intelligence,"We propose that a 2 month, 10 man study of artificial intelligence be carried out during the summer of 1956 at Dartmouth College in Hanover, New Hampshire. The study is to proceed on the basis of the conjecture that every aspect of learning or any other feature of intelligence can in principle be so precisely described that a machine can be made to simulate it. An attempt will be made to find how to make machines use language, form abstractions and concepts, solve kinds of problems now reserved for humans",propos month man studi artifici intellig carri summer dartmouth colleg hanov new hampshir studi proceed basi conjectur everi aspect learn featur intellig principl precis describ machin made simul attempt made find make machin use languag form abstract concept solv kind problem reserv human,2022,237
Dual use of artificial-intelligence-powered drug discovery,,,2022,226
Artificial Intelligence A Modern Approach 3rd Edition,"Artificial IntelligenceArtificial Intelligence: A Modern Approach 2Nd Ed.Introduction to Machine LearningArtificial IntelligenceArtificial Intelligence: A Modern Approach, eBook, Global EditionIntroduction to Artificial IntelligenceModern Approaches in Machine Learning and Cognitive Science: A WalkthroughArtificial Intelligence: Pearson New International EditionArtificial IntelligenceArtificial IntelligenceArtificial IntelligenceArtificial Intelligence a Modern ApproachFundamentals of the New Artificial IntelligenceMultiagent SystemsArtificial IntelligenceArtificial IntelligenceThe Hundred-page Machine Learning BookArtificial IntelligenceArtificial IntelligenceArtificial IntelligenceDistributed Artificial IntelligenceArtificial Intelligence For BeginnersParadigms of Artificial Intelligence ProgrammingHuman CompatibleHuman CompatibleARTIFICIAL INTELLIGENCEArtificial IntelligenceArtificial IntelligenceArtificial Intelligence a Modern ApproachDo the Right ThingArtificial IntelligenceArtificial Intelligence : a Modern ApproachArtificial IntelligenceIntelligent Help Systems for UNIXArtificial IntelligenceArtificial IntelligenceArtificial Intelligence a Modern ApproachArtificial IntelligenceArtificial IntelligenceArtificial Intelligence for Human Computer Interaction: A Modern Approach",artifici intelligenceartifici intellig modern approach nd edintroduct machin learningartifici intelligenceartifici intellig modern approach ebook global editionintroduct artifici intelligencemodern approach machin learn cognit scienc walkthroughartifici intellig pearson new intern editionartifici intelligenceartifici intelligenceartifici intelligenceartifici intellig modern approachfundament new artifici intelligencemultiag systemsartifici intelligenceartifici intelligenceth hundredpag machin learn bookartifici intelligenceartifici intelligenceartifici intelligencedistribut artifici intelligenceartifici intellig beginnersparadigm artifici intellig programminghuman compatiblehuman compatibleartifici intelligenceartifici intelligenceartifici intelligenceartifici intellig modern approachdo right thingartifici intelligenceartifici intellig modern approachartifici intelligenceintellig help system unixartifici intelligenceartifici intelligenceartifici intellig modern approachartifici intelligenceartifici intelligenceartifici intellig human comput interact modern approach,2020,488
Artificial Intelligence for the Metaverse: A Survey,,,2022,358
"Definition, roles, and potential research issues of the metaverse in education: An artificial intelligence perspective",,,2022,347
Explainable Artificial Intelligence in education,,,2022,236
Human Trust in Artificial Intelligence: Review of Empirical Research,Artificial intelligence (AI) characterizes a new generation of technologies capable of interacting with the environment and aiming to simulate human intelligence. The success of integrating AI into...,artifici intellig ai character new gener technolog capabl interact environ aim simul human intellig success integr ai,2020,935
Systematic review of research on artificial intelligence applications in higher education – where are the educators?,,,2019,1578
The potential for artificial intelligence in healthcare,"ABSTRACT The complexity and rise of data in healthcare means that artificial intelligence (AI) will increasingly be applied within the field. Several types of AI are already being employed by payers and providers of care, and life sciences companies. The key categories of applications involve diagnosis and treatment recommendations, patient engagement and adherence, and administrative activities. Although there are many instances in which AI can perform healthcare tasks as well or better than humans, implementation factors will prevent large-scale automation of healthcare professional jobs for a considerable period. Ethical issues in the application of AI to healthcare are also discussed.",abstract complex rise data healthcar mean artifici intellig ai increasingli appli within field sever type ai alreadi employ payer provid care life scienc compani key categori applic involv diagnosi treatment recommend patient engag adher administr activ although mani instanc ai perform healthcar task well better human implement factor prevent largescal autom healthcar profession job consider period ethic issu applic ai healthcar also discus,2019,1920
Edge Intelligence: Paving the Last Mile of Artificial Intelligence With Edge Computing,"With the breakthroughs in deep learning, the recent years have witnessed a booming of artificial intelligence (AI) applications and services, spanning from personal assistant to recommendation systems to video/audio surveillance. More recently, with the proliferation of mobile computing and Internet of Things (IoT), billions of mobile and IoT devices are connected to the Internet, generating zillions bytes of data at the network edge. Driving by this trend, there is an urgent need to push the AI frontiers to the network edge so as to fully unleash the potential of the edge big data. To meet this demand, edge computing, an emerging paradigm that pushes computing tasks and services from the network core to the network edge, has been widely recognized as a promising solution. The resulted new interdiscipline, edge AI or edge intelligence (EI), is beginning to receive a tremendous amount of interest. However, research on EI is still in its infancy stage, and a dedicated venue for exchanging the recent advances of EI is highly desired by both the computer system and AI communities. To this end, we conduct a comprehensive survey of the recent research efforts on EI. Specifically, we first review the background and motivation for AI running at the network edge. We then provide an overview of the overarching architectures, frameworks, and emerging key technologies for deep learning model toward training/inference at the network edge. Finally, we discuss future research opportunities on EI. We believe that this survey will elicit escalating attentions, stimulate fruitful discussions, and inspire further research ideas on EI.",breakthrough deep learn recent year wit boom artifici intellig ai applic servic span person assist recommend system videoaudio surveil recent prolifer mobil comput internet thing iot billion mobil iot devic connect internet gener zillion byte data network edg drive trend urgent need push ai frontier network edg fulli unleash potenti edg big data meet demand edg comput emerg paradigm push comput task servic network core network edg wide recogn promis solut result new interdisciplin edg ai edg intellig ei begin receiv tremend amount interest howev research ei still infanc stage dedic venu exchang recent advanc ei highli desir comput system ai commun end conduct comprehens survey recent research effort ei specif first review background motiv ai run network edg provid overview overarch architectur framework emerg key technolog deep learn model toward traininginfer network edg final discus futur research opportun ei believ survey elicit escal attent stimul fruit discus inspir research idea ei,2019,1338
Human activity recognition in artificial intelligence framework: a narrative review,,,2022,171
Artificial Intelligence in Service,"Artificial intelligence (AI) is increasingly reshaping service by performing various tasks, constituting a major source of innovation, yet threatening human jobs. We develop a theory of AI job replacement to address this double-edged impact. The theory specifies four intelligences required for service tasks—mechanical, analytical, intuitive, and empathetic—and lays out the way firms should decide between humans and machines for accomplishing those tasks. AI is developing in a predictable order, with mechanical mostly preceding analytical, analytical mostly preceding intuitive, and intuitive mostly preceding empathetic intelligence. The theory asserts that AI job replacement occurs fundamentally at the task level, rather than the job level, and for “lower” (easier for AI) intelligence tasks first. AI first replaces some of a service job’s tasks, a transition stage seen as augmentation, and then progresses to replace human labor entirely when it has the ability to take over all of a job’s tasks. The progression of AI task replacement from lower to higher intelligences results in predictable shifts over time in the relative importance of the intelligences for service employees. An important implication from our theory is that analytical skills will become less important, as AI takes over more analytical tasks, giving the “softer” intuitive and empathetic skills even more importance for service employees. Eventually, AI will be capable of performing even the intuitive and empathetic tasks, which enables innovative ways of human–machine integration for providing service but also results in a fundamental threat for human employment.",artifici intellig ai increasingli reshap servic perform variou task constitut major sourc innov yet threaten human job develop theori ai job replac address doubleedg impact theori specifi four intellig requir servic tasksmechan analyt intuit empatheticand lay way firm decid human machin accomplish task ai develop predict order mechan mostli preced analyt analyt mostli preced intuit intuit mostli preced empathet intellig theori assert ai job replac occur fundament task level rather job level lower easier ai intellig task first ai first replac servic job task transit stage seen augment progress replac human labor entir abil take job task progress ai task replac lower higher intellig result predict shift time rel import intellig servic employe import implic theori analyt skill becom less import ai take analyt task give softer intuit empathet skill even import servic employe eventu ai capabl perform even intuit empathet task enabl innov way humanmachin integr provid servic also result fundament threat human employ,2018,1821
On scientific understanding with artificial intelligence,,,2022,167
The role of artificial intelligence in achieving the Sustainable Development Goals,,,2019,1306
Artificial Intelligence in Education: AIEd for Personalised Learning Pathways,"Artificial intelligence is the driving force of change focusing on the needs and demands of the student. The research explores Artificial Intelligence in Education (AIEd) for building personalised learning systems for students. The research investigates and proposes a framework for AIEd: social networking sites and chatbots, expert systems for education, intelligent mentors and agents, machine learning, personalised educational systems and virtual educational environments. These technologies help educators to develop and introduce personalised approaches to master new knowledge and develop professional competencies. The research presents a case study of AIEd implementation in education. The scholars conducted the experiment in educational establishments using artificial intelligence in the curriculum. The scholars surveyed 184 second-year students of the Institute of Pedagogy and Psychology at the Abay Kazakh National Pedagogical University and the Kuban State Technological University to collect the data. The scholars considered the collective group discussions regarding the application of artificial intelligence in education to improve the effectiveness of learning. The research identified key advantages to creating personalised learning pathways such as access to training in 24/7 mode, training in virtual contexts, adaptation of educational content to personal needs of students, real-time and regular feedback, improvements in the educational process and mental stimulations. The proposed education paradigm reflects the increasing role of artificial intelligence in socio-economic life, the social and ethical concerns artificial intelligence may pose to humanity and its role in the digitalisation of education. The current article may be used as a theoretical framework for many educational institutions planning to exploit the capabilities of artificial intelligence in their adaptation to personalized learning.",artifici intellig drive forc chang focus need demand student research explor artifici intellig educ aie build personalis learn system student research investig propos framework aie social network site chatbot expert system educ intellig mentor agent machin learn personalis educ system virtual educ environ technolog help educ develop introduc personalis approach master new knowledg develop profession compet research present case studi aie implement educ scholar conduct experi educ establish use artifici intellig curriculum scholar survey secondyear student institut pedagogi psycholog abay kazakh nation pedagog univers kuban state technolog univers collect data scholar consid collect group discus regard applic artifici intellig educ improv effect learn research identifi key advantag creat personalis learn pathway access train mode train virtual context adapt educ content person need student realtim regular feedback improv educ process mental stimul propos educ paradigm reflect increas role artifici intellig socioeconom life social ethic concern artifici intellig may pose human role digitalis educ current articl may use theoret framework mani educ institut plan exploit capabl artifici intellig adapt person learn,2022,161
Artificial Intelligence and Life in 2030: The One Hundred Year Study on Artificial Intelligence,"In September 2016, Stanford's""One Hundred Year Study on Artificial Intelligence""project (AI100) issued the first report of its planned long-term periodic assessment of artificial intelligence (AI) and its impact on society. It was written by a panel of 17 study authors, each of whom is deeply rooted in AI research, chaired by Peter Stone of the University of Texas at Austin. The report, entitled""Artificial Intelligence and Life in 2030,""examines eight domains of typical urban settings on which AI is likely to have impact over the coming years: transportation, home and service robots, healthcare, education, public safety and security, low-resource communities, employment and workplace, and entertainment. It aims to provide the general public with a scientifically and technologically accurate portrayal of the current state of AI and its potential and to help guide decisions in industry and governments, as well as to inform research and development in the field. The charge for this report was given to the panel by the AI100 Standing Committee, chaired by Barbara Grosz of Harvard University.",septemb stanfordson hundr year studi artifici intelligenceproject ai issu first report plan longterm period assess artifici intellig ai impact societi written panel studi author deepli root ai research chair peter stone univers texa austin report entitledartifici intellig life examin eight domain typic urban set ai like impact come year transport home servic robot healthcar educ public safeti secur lowresourc commun employ workplac entertain aim provid gener public scientif technolog accur portray current state ai potenti help guid decis industri govern well inform research develop field charg report given panel ai stand committe chair barbara grosz harvard univers,2022,149
The practical implementation of artificial intelligence technologies in medicine,,,2019,1339
"The Roles of Personality Traits, AI Anxiety, and Demographic Factors in Attitudes toward Artificial Intelligence","Abstract The present study adapted the General Attitudes toward Artificial Intelligence Scale (GAAIS) to Turkish and investigated the impact of personality traits, artificial intelligence anxiety, and demographics on attitudes toward artificial intelligence. The sample consisted of 259 female (74%) and 91 male (26%) individuals aged between 18 and 51 (Mean = 24.23). Measures taken were demographics, the Ten-Item Personality Inventory, the Artificial Intelligence Anxiety Scale, and the General Attitudes toward Artificial Intelligence Scale. The Turkish GAAIS had good validity and reliability. Hierarchical Multiple Linear Regression Analyses showed that positive attitudes toward artificial intelligence were significantly predicted by the level of computer use (β = 0.139, p = 0.013), level of knowledge about artificial intelligence (β = 0.119, p = 0.029), and AI learning anxiety (β = −0.172, p = 0.004). Negative attitudes toward artificial intelligence were significantly predicted by agreeableness (β = 0.120, p = 0.019), AI configuration anxiety (β = −0.379, p < 0.001), and AI learning anxiety (β = −0.211, p < 0.001). Personality traits, AI anxiety, and demographics play important roles in attitudes toward AI. Results are discussed in light of the previous research and theoretical explanations.",abstract present studi adapt gener attitud toward artifici intellig scale gaai turkish investig impact person trait artifici intellig anxieti demograph attitud toward artifici intellig sampl consist femal male individu age mean measur taken demograph tenitem person inventori artifici intellig anxieti scale gener attitud toward artifici intellig scale turkish gaai good valid reliabl hierarch multipl linear regress analys show posit attitud toward artifici intellig significantli predict level comput use p level knowledg artifici intellig p ai learn anxieti p neg attitud toward artifici intellig significantli predict agreeabl p ai configur anxieti p ai learn anxieti p person trait ai anxieti demograph play import role attitud toward ai result discus light previou research theoret explan,2022,138
Explainable Artificial Intelligence Applications in Cyber Security: State-of-the-Art in Research,"This survey presents a comprehensive review of current literature on Explainable Artificial Intelligence (XAI) methods for cyber security applications. Due to the rapid development of Internet-connected systems and Artificial Intelligence in recent years, Artificial Intelligence including Machine Learning (ML) and Deep Learning (DL) has been widely utilized in the fields of cyber security including intrusion detection, malware detection, and spam filtering. However, although Artificial Intelligence-based approaches for the detection and defense of cyber attacks and threats are more advanced and efficient compared to the conventional signature-based and rule-based cyber security strategies, most ML-based techniques and DL-based techniques are deployed in the “black-box” manner, meaning that security experts and customers are unable to explain how such procedures reach particular conclusions. The deficiencies of transparencies and interpretability of existing Artificial Intelligence techniques would decrease human users’ confidence in the models utilized for the defense against cyber attacks, especially in current situations where cyber attacks become increasingly diverse and complicated. Therefore, it is essential to apply XAI in the establishment of cyber security models to create more explainable models while maintaining high accuracy and allowing human users to comprehend, trust, and manage the next generation of cyber defense mechanisms. Although there are papers reviewing Artificial Intelligence applications in cyber security areas and the vast literature on applying XAI in many fields including healthcare, financial services, and criminal justice, the surprising fact is that there are currently no survey research articles that concentrate on XAI applications in cyber security. Therefore, the motivation behind the survey is to bridge the research gap by presenting a detailed and up-to-date survey of XAI approaches applicable to issues in the cyber security field. Our work is the first to propose a clear roadmap for navigating the XAI literature in the context of applications in cyber security.",survey present comprehens review current literatur explain artifici intellig xai method cyber secur applic due rapid develop internetconnect system artifici intellig recent year artifici intellig includ machin learn ml deep learn dl wide util field cyber secur includ intrus detect malwar detect spam filter howev although artifici intelligencebas approach detect defens cyber attack threat advanc effici compar convent signaturebas rulebas cyber secur strategi mlbase techniqu dlbase techniqu deploy blackbox manner mean secur expert custom unabl explain procedur reach particular conclus defici transpar interpret exist artifici intellig techniqu would decreas human user confid model util defens cyber attack especi current situat cyber attack becom increasingli diver complic therefor essenti appli xai establish cyber secur model creat explain model maintain high accuraci allow human user comprehend trust manag next gener cyber defens mechan although paper review artifici intellig applic cyber secur area vast literatur appli xai mani field includ healthcar financi servic crimin justic surpris fact current survey research articl concentr xai applic cyber secur therefor motiv behind survey bridg research gap present detail uptod survey xai approach applic issu cyber secur field work first propos clear roadmap navig xai literatur context applic cyber secur,2022,122
Quo vadis artificial intelligence?,,,2022,163
"Has the Future Started? The Current Growth of Artificial Intelligence, Machine Learning, and Deep Learning","In the modern era, many terms related to artificial intelligence, machine learning, and deep learning are widely used in domains such as business, healthcare, industries, and military. In these fields, the accurate prediction and analysis of data are crucial, regardless of how large the data are. However, using big data is confusing due to the rapid growth and massive development in public life, which requires a tremendous human effort in order to deal with such type of data and extract worthy information from it. Thus, the role of artificial intelligence begins in analyzing big data based on scientific techniques, especially in machine learning, whereby it can identify patterns of decision-making and reduce human intervention. In this regard, the significance role of artificial intelligence, machine learning and deep learning is growing rapidly. In this article, the authors decide to highlight these sciences by discussing how to develop and apply them in many decision-making domains. In addition, the influence of artificial intelligence in healthcare and the gains this science provides in the face of the COVID-19 pandemic are highlighted. This article concludes that these sciences have a significant impact, especially in healthcare, as well as the ability to grow and improve their methodology in decision-making. Additionally, artificial intelligence is a vital science, especially in the face of COVID-19.",modern era mani term relat artifici intellig machin learn deep learn wide use domain busi healthcar industri militari field accur predict analysi data crucial regardless larg data howev use big data confus due rapid growth massiv develop public life requir tremend human effort order deal type data extract worthi inform thu role artifici intellig begin analyz big data base scientif techniqu especi machin learn wherebi identifi pattern decisionmak reduc human intervent regard signific role artifici intellig machin learn deep learn grow rapidli articl author decid highlight scienc discus develop appli mani decisionmak domain addit influenc artifici intellig healthcar gain scienc provid face covid pandem highlight articl conclud scienc signific impact especi healthcar well abil grow improv methodolog decisionmak addit artifici intellig vital scienc especi face covid,2022,162
Artificial intelligence in cancer target identification and drug discovery,,,2022,145
Photonics for artificial intelligence and neuromorphic computing,,,2020,1068
Artificial Intelligence in Meta-optics,"Recent years have witnessed promising artificial intelligence (AI) applications in many disciplines, including optics, engineering, medicine, economics, and education. In particular, the synergy of AI and meta-optics has greatly benefited both fields. Meta-optics are advanced flat optics with novel functions and light-manipulation abilities. The optical properties can be engineered with a unique design to meet various optical demands. This review offers comprehensive coverage of meta-optics and artificial intelligence in synergy. After providing an overview of AI and meta-optics, we categorize and discuss the recent developments integrated by these two topics, namely AI for meta-optics and meta-optics for AI. The former describes how to apply AI to the research of meta-optics for design, simulation, optical information analysis, and application. The latter reports the development of the optical Al system and computation via meta-optics. This review will also provide an in-depth discussion of the challenges of this interdisciplinary field and indicate future directions. We expect that this review will inspire researchers in these fields and benefit the next generation of intelligent optical device design.",recent year wit promis artifici intellig ai applic mani disciplin includ optic engin medicin econom educ particular synergi ai metaopt greatli benefit field metaopt advanc flat optic novel function lightmanipul abil optic properti engin uniqu design meet variou optic demand review offer comprehens coverag metaopt artifici intellig synergi provid overview ai metaopt categor discus recent develop integr two topic name ai metaopt metaopt ai former describ appli ai research metaopt design simul optic inform analysi applic latter report develop optic al system comput via metaopt review also provid indepth discus challeng interdisciplinari field indic futur direct expect review inspir research field benefit next gener intellig optic devic design,2022,122
Principles of Artificial Intelligence,,,1980,3956
Artificial intelligence in medical education: a cross-sectional needs assessment,,,2022,117
Artificial Intelligence and Chatbots in Psychiatry,,,2022,114
Future paths for integer programming and links to artificial intelligence,,,1986,4328
Frontiers in Artificial Intelligence and Applications,"The industrial revolution has been the main cause ever since tremendous technological advancement was observed. The ubiquitous deployment of recent information and communication technologies (ICT), namely Artificial Intelligence (AI), Internet of Things (IoT), and Blockchain technology, is hastening the world’s industrial and technological transformation. This technical aggrandizement enhances the working culture and has a favorable impact on the workplace, as per the progressivist perspective. The breakneck pace of technological advancement, as well as AI, has enabled humans to replace manual labor in various industries. As being a domain of science and technology, AI develops machines and programs for computers that are intelligent and can accomplish tasks that would normally require human intelligence abilities. This paper mainly explores the frontiers of artificial intelligence and its applications in various fields. The AI Frontiers promulgate methodical concepts that are peer-reviewed cutting-edge research on the disruptive technological revolution of Artificial Intelligence. Additionally, some key viewpoints in the field of AI have been listed along with the main frontiers, including Machine Learning (ML), Deep Learning (DL), Fuzzy Logic (FL), Natural Language Processor (NLP), and Genetic Algorithm (GA). Furthermore, this paper discussed some common AI applications and a briefing about the current scenario in the worldwide market for artificial intelligence.",industri revolut main caus ever sinc tremend technolog advanc observ ubiquit deploy recent inform commun technolog ict name artifici intellig ai internet thing iot blockchain technolog hasten world industri technolog transform technic aggrandiz enhanc work cultur favor impact workplac per progressivist perspect breakneck pace technolog advanc well ai enabl human replac manual labor variou industri domain scienc technolog ai develop machin program comput intellig accomplish task would normal requir human intellig abil paper mainli explor frontier artifici intellig applic variou field ai frontier promulg method concept peerreview cuttingedg research disrupt technolog revolut artifici intellig addit key viewpoint field ai list along main frontier includ machin learn ml deep learn dl fuzzi logic fl natur languag processor nlp genet algorithm ga furthermor paper discus common ai applic brief current scenario worldwid market artifici intellig,2022,113
Explainable artificial intelligence: an analytical review,"This paper provides a brief analytical review of the current state‐of‐the‐art in relation to the explainability of artificial intelligence in the context of recent advances in machine learning and deep learning. The paper starts with a brief historical introduction and a taxonomy, and formulates the main challenges in terms of explainability building on the recently formulated National Institute of Standards four principles of explainability. Recently published methods related to the topic are then critically reviewed and analyzed. Finally, future directions for research are suggested.",paper provid brief analyt review current stateoftheart relat explain artifici intellig context recent advanc machin learn deep learn paper start brief histor introduct taxonomi formul main challeng term explain build recent formul nation institut standard four principl explain recent publish method relat topic critic review analyz final futur direct research suggest,2021,432
A Review of Artificial Intelligence (AI) in Education from 2010 to 2020,"This study provided a content analysis of studies aiming to disclose how artificial intelligence (AI) has been applied to the education sector and explore the potential research trends and challenges of AI in education. A total of 100 papers including 63 empirical papers (74 studies) and 37 analytic papers were selected from the education and educational research category of Social Sciences Citation Index database from 2010 to 2020. The content analysis showed that the research questions could be classified into development layer (classification, matching, recommendation, and deep learning), application layer (feedback, reasoning, and adaptive learning), and integration layer (affection computing, role-playing, immersive learning, and gamification). Moreover, four research trends, including Internet of Things, swarm intelligence, deep learning, and neuroscience, as well as an assessment of AI in education, were suggested for further investigation. However, we also proposed the challenges in education may be caused by AI with regard to inappropriate use of AI techniques, changing roles of teachers and students, as well as social and ethical issues. The results provide insights into an overview of the AI used for education domain, which helps to strengthen the theoretical foundation of AI in education and provides a promising channel for educators and AI engineers to carry out further collaborative research.",studi provid content analysi studi aim disclos artifici intellig ai appli educ sector explor potenti research trend challeng ai educ total paper includ empir paper studi analyt paper select educ educ research categori social scienc citat index databas content analysi show research question could classifi develop layer classif match recommend deep learn applic layer feedback reason adapt learn integr layer affect comput roleplay immers learn gamif moreov four research trend includ internet thing swarm intellig deep learn neurosci well assess ai educ suggest investig howev also propos challeng educ may caus ai regard inappropri use ai techniqu chang role teacher student well social ethic issu result provid insight overview ai use educ domain help strengthen theoret foundat ai educ provid promis channel educ ai engin carri collabor research,2021,436
"Artificial intelligence, robotics, advanced technologies and human resource management: a systematic review","Abstract Although academic production in intelligent automation (e.g. artificial intelligence, robotics) has grown rapidly, we still lack a comprehensive understanding of the impacts of the utilization of these technologies in human resource management (HRM) at an organizational (firms) and individual (employees) level. This study therefore aims to systematize the academic inputs on intelligent automation so far and to clarify what are its main contributions to and challenges for HRM. In a systematic search of 13,136 potentially relevant studies published in the top HRM, international business (IB), general management (GM) and information management (IM) journals, we found 45 articles studying artificial intelligence, robotics and other advanced technologies within HRM settings. Results show that intelligent automation technologies constitute a new approach to managing employees and enhancing firm performance, thus offering several opportunities for HRM but also considerable challenges at a technological and ethical level. The impact of these technologies has been identified to concentrate on HRM strategies, namely, job replacement, human-robot/AI collaboration, decision-making and learning opportunities, and HRM activities, namely, recruiting, training and job performance. This study discusses these shifts in detail, along with the main contributions to theory and practice and directions for future research.",abstract although academ product intellig autom eg artifici intellig robot grown rapidli still lack comprehens understand impact util technolog human resourc manag hrm organiz firm individu employe level studi therefor aim systemat academ input intellig autom far clarifi main contribut challeng hrm systemat search potenti relev studi publish top hrm intern busi ib gener manag gm inform manag im journal found articl studi artifici intellig robot advanc technolog within hrm set result show intellig autom technolog constitut new approach manag employe enhanc firm perform thu offer sever opportun hrm also consider challeng technolog ethic level impact technolog identifi concentr hrm strategi name job replac humanrobotai collabor decisionmak learn opportun hrm activ name recruit train job perform studi discus shift detail along main contribut theori practic direct futur research,2021,506
"Edge Artificial Intelligence for 6G: Vision, Enabling Technologies, and Applications","The thriving of artificial intelligence (AI) applications is driving the further evolution of wireless networks. It has been envisioned that 6G will be transformative and will revolutionize the evolution of wireless from “connected things” to “connected intelligence”. However, state-of-the-art deep learning and big data analytics based AI systems require tremendous computation and communication resources, causing significant latency, energy consumption, network congestion, and privacy leakage in both of the training and inference processes. By embedding model training and inference capabilities into the network edge, edge AI stands out as a disruptive technology for 6G to seamlessly integrate sensing, communication, computation, and intelligence, thereby improving the efficiency, effectiveness, privacy, and security of 6G networks. In this paper, we shall provide our vision for scalable and trustworthy edge AI systems with integrated design of wireless communication strategies and decentralized machine learning models. New design principles of wireless networks, service-driven resource allocation optimization methods, as well as a holistic end-to-end system architecture to support edge AI will be described. Standardization, software and hardware platforms, and application scenarios are also discussed to facilitate the industrialization and commercialization of edge AI systems.",thrive artifici intellig ai applic drive evolut wireless network envis g transform revolution evolut wireless connect thing connect intellig howev stateoftheart deep learn big data analyt base ai system requir tremend comput commun resourc caus signific latenc energi consumpt network congest privaci leakag train infer process embed model train infer capabl network edg edg ai stand disrupt technolog g seamlessli integr sen commun comput intellig therebi improv effici effect privaci secur g network paper shall provid vision scalabl trustworthi edg ai system integr design wireless commun strategi decentr machin learn model new design principl wireless network servicedriven resourc alloc optim method well holist endtoend system architectur support edg ai describ standard softwar hardwar platform applic scenario also discus facilit industri commerci edg ai system,2021,358
Explainable artificial intelligence: a comprehensive review,,,2021,374
Artificial intelligence-enhanced electrocardiography in cardiovascular disease management,,,2021,457
The role of artificial intelligence in healthcare: a structured literature review,,,2021,431
Artificial intelligence in healthcare: transforming the practice of medicine,"ABSTRACT Artificial intelligence (AI) is a powerful and disruptive area of computer science, with the potential to fundamentally transform the practice of medicine and the delivery of healthcare. In this review article, we outline recent breakthroughs in the application of AI in healthcare, describe a roadmap to building effective, reliable and safe AI systems, and discuss the possible future direction of AI augmented healthcare systems.",abstract artifici intellig ai power disrupt area comput scienc potenti fundament transform practic medicin deliveri healthcar review articl outlin recent breakthrough applic ai healthcar describ roadmap build effect reliabl safe ai system discus possibl futur direct ai augment healthcar system,2021,448
Attitudes and perception of artificial intelligence in healthcare: A cross-sectional survey among patients,"Objective The attitudes about the usage of artificial intelligence in healthcare are controversial. Unlike the perception of healthcare professionals, the attitudes of patients and their companions have been of less interest so far. In this study, we aimed to investigate the perception of artificial intelligence in healthcare among this highly relevant group along with the influence of digital affinity and sociodemographic factors. Methods We conducted a cross-sectional study using a paper-based questionnaire with patients and their companions at a German tertiary referral hospital from December 2019 to February 2020. The questionnaire consisted of three sections examining (a) the respondents’ technical affinity, (b) their perception of different aspects of artificial intelligence in healthcare and (c) sociodemographic characteristics. Results From a total of 452 participants, more than 90% already read or heard about artificial intelligence, but only 24% reported good or expert knowledge. Asked on their general perception, 53.18% of the respondents rated the use of artificial intelligence in medicine as positive or very positive, but only 4.77% negative or very negative. The respondents denied concerns about artificial intelligence, but strongly agreed that artificial intelligence must be controlled by a physician. Older patients, women, persons with lower education and technical affinity were more cautious on the healthcare-related artificial intelligence usage. Conclusions German patients and their companions are open towards the usage of artificial intelligence in healthcare. Although showing only a mediocre knowledge about artificial intelligence, a majority rated artificial intelligence in healthcare as positive. Particularly, patients insist that a physician supervises the artificial intelligence and keeps ultimate responsibility for diagnosis and therapy.",object attitud usag artifici intellig healthcar controversi unlik percept healthcar profession attitud patient companion less interest far studi aim investig percept artifici intellig healthcar among highli relev group along influenc digit affin sociodemograph factor method conduct crosssect studi use paperbas questionnair patient companion german tertiari referr hospit decemb februari questionnair consist three section examin respond technic affin b percept differ aspect artifici intellig healthcar c sociodemograph characterist result total particip alreadi read heard artifici intellig report good expert knowledg ask gener percept respond rate use artifici intellig medicin posit posit neg neg respond deni concern artifici intellig strongli agre artifici intellig must control physician older patient woman person lower educ technic affin cautiou healthcarerel artifici intellig usag conclus german patient companion open toward usag artifici intellig healthcar although show mediocr knowledg artifici intellig major rate artifici intellig healthcar posit particularli patient insist physician supervis artifici intellig keep ultim respons diagnosi therapi,2022,88
Artificial Intelligence and Business Value: a Literature Review,,,2021,340
A Survey on Explainable Artificial Intelligence (XAI): Toward Medical XAI,"Recently, artificial intelligence and machine learning in general have demonstrated remarkable performances in many tasks, from image processing to natural language processing, especially with the advent of deep learning (DL). Along with research progress, they have encroached upon many different fields and disciplines. Some of them require high level of accountability and thus transparency, for example, the medical sector. Explanations for machine decisions and predictions are thus needed to justify their reliability. This requires greater interpretability, which often means we need to understand the mechanism underlying the algorithms. Unfortunately, the blackbox nature of the DL is still unresolved, and many machine decisions are still poorly understood. We provide a review on interpretabilities suggested by different research works and categorize them. The different categories show different dimensions in interpretability research, from approaches that provide “obviously” interpretable information to the studies of complex patterns. By applying the same categorization to interpretability in medical research, it is hoped that: 1) clinicians and practitioners can subsequently approach these methods with caution; 2) insight into interpretability will be born with more considerations for medical practices; and 3) initiatives to push forward data-based, mathematically grounded, and technically grounded medical education are encouraged.",recent artifici intellig machin learn gener demonstr remark perform mani task imag process natur languag process especi advent deep learn dl along research progress encroach upon mani differ field disciplin requir high level account thu transpar exampl medic sector explan machin decis predict thu need justifi reliabl requir greater interpret often mean need understand mechan underli algorithm unfortun blackbox natur dl still unresolv mani machin decis still poorli understood provid review interpret suggest differ research work categor differ categori show differ dimens interpret research approach provid obvious interpret inform studi complex pattern appli categor interpret medic research hope clinician practition subsequ approach method caution insight interpret born consider medic practic initi push forward databas mathemat ground technic ground medic educ encourag,2019,1282
Artificial intelligence to deep learning: machine intelligence approach for drug discovery,,,2021,589
The Clinician and Dataset Shift in Artificial Intelligence.,"To the Editor: Artificial intelligence (AI) systems are now regularly being used in medical settings,1 although regulatory oversight is inconsistent and undeveloped.2,3 Safe deployment of clinical AI requires informed clinician-users, who are generally responsible for identifying and reporting emerging problems. Clinicians may also serve as administrators in governing the use of clinical AI. A natural question follows: are clinicians adequately prepared to identify circumstances in which AI systems fail to perform their intended function reliably? A major driver of AI system malfunction is known as “dataset shift.”4,5 Most clinical AI systems today use machine learning, algorithms that leverage statistical methods to learn key patterns from clinical data. Dataset shift occurs when a machine-learning system underperforms because of a mismatch between the data set with which it was developed and the data on which it is deployed.4 For example, the University of Michigan Hospital implemented the widely used sepsis-alerting model developed by Epic Systems; in April 2020, the model had to be deactivated because of spurious alerting owing to changes in patients’ demographic characteristics associated with the coronavirus disease 2019 pandemic. This was a case in which dataset shift fundamentally altered the relationship between fevers and bacterial sepsis, leading the hospital’s clinical AI governing committee (which one of the authors of this letter chairs) to decommission its use. This is an extreme example; many causes of dataset shift are more subtle. In Table 1, we present common causes of dataset shift, which we group into changes in technology (e.g., software vendors), changes in population and setting (e.g., new demographics), and changes in behavior (e.g., new reimbursement incentives); the list is not meant to be exhaustive. Successful recognition and mitigation of dataset shift require both vigilant clinicians and sound technical oversight through AI governance teams.4,5 When using an AI system, clinicians should note misalignment between the predictions of the model and their own clinical judgment, as in the sepsis example above. Clinicians who use AI systems must frequently consider whether relevant aspects of their own clinical practice are atypical or have recently changed. For their part, AI governance teams must be sure that it is easy for clinicians to report concerns about the function of AI systems and provide feedback so that the clinician who is reporting will understand that the registered concern has been noted and, if appropriate, actions to mitigate the concern have been taken. Teams must also establish AI monitoring and updating protocols that integrate technical solu-",editor artifici intellig ai system regularli use medic set although regulatori oversight inconsist undevelop safe deploy clinic ai requir inform clinicianus gener respons identifi report emerg problem clinician may also serv administr govern use clinic ai natur question follow clinician adequ prepar identifi circumst ai system fail perform intend function reliabl major driver ai system malfunct known dataset shift clinic ai system today use machin learn algorithm leverag statist method learn key pattern clinic data dataset shift occur machinelearn system underperform mismatch data set develop data deploy exampl univers michigan hospit implement wide use sepsisalert model develop epic system april model deactiv spuriou alert owe chang patient demograph characterist associ coronaviru diseas pandem case dataset shift fundament alter relationship fever bacteri sepsi lead hospit clinic ai govern committe one author letter chair decommiss use extrem exampl mani caus dataset shift subtl tabl present common caus dataset shift group chang technolog eg softwar vendor chang popul set eg new demograph chang behavior eg new reimburs incent list meant exhaust success recognit mitig dataset shift requir vigil clinician sound technic oversight ai govern team use ai system clinician note misalign predict model clinic judgment sepsi exampl clinician use ai system must frequent consid whether relev aspect clinic practic atyp recent chang part ai govern team must sure easi clinician report concern function ai system provid feedback clinician report understand regist concern note appropri action mitig concern taken team must also establish ai monitor updat protocol integr technic solu,2021,347
Artificial intelligence–enabled rapid diagnosis of patients with COVID-19,,,2020,781
Artificial Intelligence and Management: The Automation–Augmentation Paradox,"Taking three recent business books on artificial intelligence (AI) as a starting point, we explore the automation and augmentation concepts in the management domain. Whereas automation implies that...",take three recent busi book artifici intellig ai start point explor autom augment concept manag domain wherea autom impli,2020,694
Predicting cancer outcomes with radiomics and artificial intelligence in radiology,,,2021,378
"Human-Centered Artificial Intelligence: Reliable, Safe & Trustworthy","ABSTRACT Well-designed technologies that offer high levels of human control and high levels of computer automation can increase human performance, leading to wider adoption. The Human-Centered Artificial Intelligence (HCAI) framework clarifies how to (1) design for high levels of human control and high levels of computer automation so as to increase human performance, (2) understand the situations in which full human control or full computer control are necessary, and (3) avoid the dangers of excessive human control or excessive computer control. The methods of HCAI are more likely to produce designs that are Reliable, Safe & Trustworthy (RST). Achieving these goals will dramatically increase human performance, while supporting human self-efficacy, mastery, creativity, and responsibility.",abstract welldesign technolog offer high level human control high level comput autom increas human perform lead wider adopt humancent artifici intellig hcai framework clarifi design high level human control high level comput autom increas human perform understand situat full human control full comput control necessari avoid danger excess human control excess comput control method hcai like produc design reliabl safe trustworthi rst achiev goal dramat increas human perform support human selfefficaci masteri creativ respons,2020,590
Opportunities and Challenges in Explainable Artificial Intelligence (XAI): A Survey,"Nowadays, deep neural networks are widely used in mission critical systems such as healthcare, self-driving vehicles, and military which have direct impact on human lives. However, the black-box nature of deep neural networks challenges its use in mission critical applications, raising ethical and judicial concerns inducing lack of trust. Explainable Artificial Intelligence (XAI) is a field of Artificial Intelligence (AI) that promotes a set of tools, techniques, and algorithms that can generate high-quality interpretable, intuitive, human-understandable explanations of AI decisions. In addition to providing a holistic view of the current XAI landscape in deep learning, this paper provides mathematical summaries of seminal work. We start by proposing a taxonomy and categorizing the XAI techniques based on their scope of explanations, methodology behind the algorithms, and explanation level or usage which helps build trustworthy, interpretable, and self-explanatory deep learning models. We then describe the main principles used in XAI research and present the historical timeline for landmark studies in XAI from 2007 to 2020. After explaining each category of algorithms and approaches in detail, we then evaluate the explanation maps generated by eight XAI algorithms on image data, discuss the limitations of this approach, and provide potential future directions to improve XAI evaluation.",nowaday deep neural network wide use mission critic system healthcar selfdriv vehicl militari direct impact human live howev blackbox natur deep neural network challeng use mission critic applic rais ethic judici concern induc lack trust explain artifici intellig xai field artifici intellig ai promot set tool techniqu algorithm gener highqual interpret intuit humanunderstand explan ai decis addit provid holist view current xai landscap deep learn paper provid mathemat summari semin work start propos taxonomi categor xai techniqu base scope explan methodolog behind algorithm explan level usag help build trustworthi interpret selfexplanatori deep learn model describ main principl use xai research present histor timelin landmark studi xai explain categori algorithm approach detail evalu explan map gener eight xai algorithm imag data discus limit approach provid potenti futur direct improv xai evalu,2020,529
Artificial intelligence and machine learning in design of mechanical materials.,"Artificial intelligence, especially machine learning (ML) and deep learning (DL) algorithms, is becoming an important tool in the fields of materials and mechanical engineering, attributed to its power to predict materials properties, design de novo materials and discover new mechanisms beyond intuitions. As the structural complexity of novel materials soars, the material design problem to optimize mechanical behaviors can involve massive design spaces that are intractable for conventional methods. Addressing this challenge, ML models trained from large material datasets that relate structure, properties and function at multiple hierarchical levels have offered new avenues for fast exploration of the design spaces. The performance of a ML-based materials design approach relies on the collection or generation of a large dataset that is properly preprocessed using the domain knowledge of materials science underlying chemical and physical concepts, and a suitable selection of the applied ML model. Recent breakthroughs in ML techniques have created vast opportunities for not only overcoming long-standing mechanics problems but also for developing unprecedented materials design strategies. In this review, we first present a brief introduction of state-of-the-art ML models, algorithms and structures. Then, we discuss the importance of data collection, generation and preprocessing. The applications in mechanical property prediction, materials design and computational methods using ML-based approaches are summarized, followed by perspectives on opportunities and open challenges in this emerging and exciting field.",artifici intellig especi machin learn ml deep learn dl algorithm becom import tool field materi mechan engin attribut power predict materi properti design de novo materi discov new mechan beyond intuit structur complex novel materi soar materi design problem optim mechan behavior involv massiv design space intract convent method address challeng ml model train larg materi dataset relat structur properti function multipl hierarch level offer new avenu fast explor design space perform mlbase materi design approach reli collect gener larg dataset properli preprocess use domain knowledg materi scienc underli chemic physic concept suitabl select appli ml model recent breakthrough ml techniqu creat vast opportun overcom longstand mechan problem also develop unpreced materi design strategi review first present brief introduct stateoftheart ml model algorithm structur discus import data collect gener preprocess applic mechan properti predict materi design comput method use mlbase approach summar follow perspect opportun open challeng emerg excit field,2021,264
Artificial Intelligence: the global landscape of ethics guidelines,,,2019,1427
Structured information extraction from scientific text with large language models,,,2024,107
Unified Structure Generation for Universal Information Extraction,"Information extraction suffers from its varying targets, heterogeneous structures, and demand-specific schemas. In this paper, we propose a unified text-to-structure generation framework, namely UIE, which can universally model different IE tasks, adaptively generate targeted structures, and collaboratively learn general IE abilities from different knowledge sources. Specifically, UIE uniformly encodes different extraction structures via a structured extraction language, adaptively generates target extractions via a schema-based prompt mechanism – structural schema instructor, and captures the common IE abilities via a large-scale pretrained text-to-structure model. Experiments show that UIE achieved the state-of-the-art performance on 4 IE tasks, 13 datasets, and on all supervised, low-resource, and few-shot settings for a wide range of entity, relation, event and sentiment extraction tasks and their unification. These results verified the effectiveness, universality, and transferability of UIE.",inform extract suffer vari target heterogen structur demandspecif schema paper propos unifi texttostructur gener framework name uie univers model differ ie task adapt gener target structur collabor learn gener ie abil differ knowledg sourc specif uie uniformli encod differ extract structur via structur extract languag adapt gener target extract via schemabas prompt mechan structur schema instructor captur common ie abil via largescal pretrain texttostructur model experi show uie achiev stateoftheart perform ie task dataset supervis lowresourc fewshot set wide rang entiti relat event sentiment extract task unif result verifi effect univers transfer uie,2022,361
Zero-Shot Information Extraction via Chatting with ChatGPT,"Zero-shot information extraction (IE) aims to build IE systems from the unannotated text. It is challenging due to involving little human intervention. Challenging but worthwhile, zero-shot IE reduces the time and effort that data labeling takes. Recent efforts on large language models (LLMs, e.g., GPT-3, ChatGPT) show promising performance on zero-shot settings, thus inspiring us to explore prompt-based methods. In this work, we ask whether strong IE models can be constructed by directly prompting LLMs. Specifically, we transform the zero-shot IE task into a multi-turn question-answering problem with a two-stage framework (ChatIE). With the power of ChatGPT, we extensively evaluate our framework on three IE tasks: entity-relation triple extract, named entity recognition, and event extraction. Empirical results on six datasets across two languages show that ChatIE achieves impressive performance and even surpasses some full-shot models on several datasets (e.g., NYT11-HRL). We believe that our work could shed light on building IE models with limited resources.",zeroshot inform extract ie aim build ie system unannot text challeng due involv littl human intervent challeng worthwhil zeroshot ie reduc time effort data label take recent effort larg languag model llm eg gpt chatgpt show promis perform zeroshot set thu inspir u explor promptbas method work ask whether strong ie model construct directli prompt llm specif transform zeroshot ie task multiturn questionansw problem twostag framework chati power chatgpt extens evalu framework three ie task entityrel tripl extract name entiti recognit event extract empir result six dataset across two languag show chati achiev impress perform even surpass fullshot model sever dataset eg nythrl believ work could shed light build ie model limit resourc,2023,280
"Evaluating ChatGPT's Information Extraction Capabilities: An Assessment of Performance, Explainability, Calibration, and Faithfulness","The capability of Large Language Models (LLMs) like ChatGPT to comprehend user intent and provide reasonable responses has made them extremely popular lately. In this paper, we focus on assessing the overall ability of ChatGPT using 7 fine-grained information extraction (IE) tasks. Specially, we present the systematically analysis by measuring ChatGPT's performance, explainability, calibration, and faithfulness, and resulting in 15 keys from either the ChatGPT or domain experts. Our findings reveal that ChatGPT's performance in Standard-IE setting is poor, but it surprisingly exhibits excellent performance in the OpenIE setting, as evidenced by human evaluation. In addition, our research indicates that ChatGPT provides high-quality and trustworthy explanations for its decisions. However, there is an issue of ChatGPT being overconfident in its predictions, which resulting in low calibration. Furthermore, ChatGPT demonstrates a high level of faithfulness to the original text in the majority of cases. We manually annotate and release the test sets of 7 fine-grained IE tasks contains 14 datasets to further promote the research. The datasets and code are available at https://github.com/pkuserc/ChatGPT_for_IE.",capabl larg languag model llm like chatgpt comprehend user intent provid reason respons made extrem popular late paper focu assess overal abil chatgpt use finegrain inform extract ie task special present systemat analysi measur chatgpt perform explain calibr faith result key either chatgpt domain expert find reveal chatgpt perform standardi set poor surprisingli exhibit excel perform openi set evidenc human evalu addit research indic chatgpt provid highqual trustworthi explan decis howev issu chatgpt overconfid predict result low calibr furthermor chatgpt demonstr high level faith origin text major case manual annot releas test set finegrain ie task contain dataset promot research dataset code avail httpsgithubcompkusercchatgptfori,2023,136
InstructUIE: Multi-task Instruction Tuning for Unified Information Extraction,"Large language models have unlocked strong multi-task capabilities from reading instructive prompts. However, recent studies have shown that existing large models still have difficulty with information extraction tasks. For example, gpt-3.5-turbo achieved an F1 score of 18.22 on the Ontonotes dataset, which is significantly lower than the state-of-the-art performance. In this paper, we propose InstructUIE, a unified information extraction framework based on instruction tuning, which can uniformly model various information extraction tasks and capture the inter-task dependency. To validate the proposed method, we introduce IE INSTRUCTIONS, a benchmark of 32 diverse information extraction datasets in a unified text-to-text format with expert-written instructions. Experimental results demonstrate that our method achieves comparable performance to Bert in supervised settings and significantly outperforms the state-of-the-art and gpt3.5 in zero-shot settings.",larg languag model unlock strong multitask capabl read instruct prompt howev recent studi shown exist larg model still difficulti inform extract task exampl gptturbo achiev f score ontonot dataset significantli lower stateoftheart perform paper propos instructui unifi inform extract framework base instruct tune uniformli model variou inform extract task captur intertask depend valid propos method introduc ie instruct benchmark diver inform extract dataset unifi texttotext format expertwritten instruct experiment result demonstr method achiev compar perform bert supervis set significantli outperform stateoftheart gpt zeroshot set,2023,123
Large Language Models for Generative Information Extraction: A Survey,"Information Extraction (IE) aims to extract structural knowledge from plain natural language texts. Recently, generative Large Language Models (LLMs) have demonstrated remarkable capabilities in text understanding and generation. As a result, numerous works have been proposed to integrate LLMs for IE tasks based on a generative paradigm. To conduct a comprehensive systematic review and exploration of LLM efforts for IE tasks, in this study, we survey the most recent advancements in this field. We first present an extensive overview by categorizing these works in terms of various IE subtasks and techniques, and then we empirically analyze the most advanced methods and discover the emerging trend of IE tasks with LLMs. Based on a thorough review conducted, we identify several insights in technique and promising research directions that deserve further exploration in future studies. We maintain a public repository and consistently update related works and resources on GitHub (LLM4IE repository).",inform extract ie aim extract structur knowledg plain natur languag text recent gener larg languag model llm demonstr remark capabl text understand gener result numer work propos integr llm ie task base gener paradigm conduct comprehens systemat review explor llm effort ie task studi survey recent advanc field first present extens overview categor work term variou ie subtask techniqu empir analyz advanc method discov emerg trend ie task llm base thorough review conduct identifi sever insight techniqu promis research direct deserv explor futur studi maintain public repositori consist updat relat work resourc github llmie repositori,2023,97
LLMs Accelerate Annotation for Medical Information Extraction,"The unstructured nature of clinical notes within electronic health records often conceals vital patient-related information, making it challenging to access or interpret. To uncover this hidden information, specialized Natural Language Processing (NLP) models are required. However, training these models necessitates large amounts of labeled data, a process that is both time-consuming and costly when relying solely on human experts for annotation. In this paper, we propose an approach that combines Large Language Models (LLMs) with human expertise to create an efficient method for generating ground truth labels for medical text annotation. By utilizing LLMs in conjunction with human annotators, we significantly reduce the human annotation burden, enabling the rapid creation of labeled datasets. We rigorously evaluate our method on a medical information extraction task, demonstrating that our approach not only substantially cuts down on human intervention but also maintains high accuracy. The results highlight the potential of using LLMs to improve the utilization of unstructured clinical data, allowing for the swift deployment of tailored NLP solutions in healthcare.",unstructur natur clinic note within electron health record often conceal vital patientrel inform make challeng access interpret uncov hidden inform special natur languag process nlp model requir howev train model necessit larg amount label data process timeconsum costli reli sole human expert annot paper propos approach combin larg languag model llm human expertis creat effici method gener ground truth label medic text annot util llm conjunct human annot significantli reduc human annot burden enabl rapid creation label dataset rigor evalu method medic inform extract task demonstr approach substanti cut human intervent also maintain high accuraci result highlight potenti use llm improv util unstructur clinic data allow swift deploy tailor nlp solut healthcar,2023,74
An Empirical Study on Information Extraction using Large Language Models,"Human-like large language models (LLMs), especially the most powerful and popular ones in OpenAI's GPT family, have proven to be very helpful for many natural language processing (NLP) related tasks. Therefore, various attempts have been made to apply LLMs to information extraction (IE), which is a fundamental NLP task that involves extracting information from unstructured plain text. To demonstrate the latest representative progress in LLMs' information extraction ability, we assess the information extraction ability of GPT-4 (the latest version of GPT at the time of writing this paper) from four perspectives: Performance, Evaluation Criteria, Robustness, and Error Types. Our results suggest a visible performance gap between GPT-4 and state-of-the-art (SOTA) IE methods. To alleviate this problem, considering the LLMs' human-like characteristics, we propose and analyze the effects of a series of simple prompt-based methods, which can be generalized to other LLMs and NLP tasks. Rich experiments show our methods' effectiveness and some of their remaining issues in improving GPT-4's information extraction ability.",humanlik larg languag model llm especi power popular one openai gpt famili proven help mani natur languag process nlp relat task therefor variou attempt made appli llm inform extract ie fundament nlp task involv extract inform unstructur plain text demonstr latest repres progress llm inform extract abil assess inform extract abil gpt latest version gpt time write paper four perspect perform evalu criterion robust error type result suggest visibl perform gap gpt stateoftheart sota ie method allevi problem consid llm humanlik characterist propos analyz effect seri simpl promptbas method gener llm nlp task rich experi show method effect remain issu improv gpt inform extract abil,2023,68
Universal Information Extraction as Unified Semantic Matching,"The challenge of information extraction (IE) lies in the diversity of label schemas and the heterogeneity of structures.
Traditional methods require task-specific model design and rely heavily on expensive supervision, making them difficult to generalize to new schemas.
In this paper, we decouple IE into two basic abilities, structuring and conceptualizing, which are shared by different tasks and schemas.
Based on this paradigm, we propose to universally model various IE tasks with Unified Semantic Matching (USM) framework, which introduces three unified token linking operations to model the abilities of structuring and conceptualizing.
In this way, USM can jointly encode schema and input text, uniformly extract substructures in parallel, and controllably decode target structures on demand.
Empirical evaluation on 4 IE tasks shows that the proposed method achieves state-of-the-art performance under the supervised experiments and shows strong generalization ability in zero/few-shot transfer settings.",challeng inform extract ie lie diver label schema heterogen structur tradit method requir taskspecif model design reli heavili expens supervis make difficult gener new schema paper decoupl ie two basic abil structur conceptu share differ task schema base paradigm propos univers model variou ie task unifi semant match usm framework introduc three unifi token link oper model abil structur conceptu way usm jointli encod schema input text uniformli extract substructur parallel control decod target structur demand empir evalu ie task show propos method achiev stateoftheart perform supervis experi show strong gener abil zerofewshot transfer set,2023,60
A Joint Neural Model for Information Extraction with Global Features,"Most existing joint neural models for Information Extraction (IE) use local task-specific classifiers to predict labels for individual instances (e.g., trigger, relation) regardless of their interactions. For example, a victim of a die event is likely to be a victim of an attack event in the same sentence. In order to capture such cross-subtask and cross-instance inter-dependencies, we propose a joint neural framework, OneIE, that aims to extract the globally optimal IE result as a graph from an input sentence. OneIE performs end-to-end IE in four stages: (1) Encoding a given sentence as contextualized word representations; (2) Identifying entity mentions and event triggers as nodes; (3) Computing label scores for all nodes and their pairwise links using local classifiers; (4) Searching for the globally optimal graph with a beam decoder. At the decoding stage, we incorporate global features to capture the cross-subtask and cross-instance interactions. Experiments show that adding global features improves the performance of our model and achieves new state of-the-art on all subtasks. In addition, as OneIE does not use any language-specific feature, we prove it can be easily applied to new languages or trained in a multilingual manner.",exist joint neural model inform extract ie use local taskspecif classifi predict label individu instanc eg trigger relat regardless interact exampl victim die event like victim attack event sentenc order captur crosssubtask crossinst interdepend propos joint neural framework onei aim extract global optim ie result graph input sentenc onei perform endtoend ie four stage encod given sentenc contextu word represent identifi entiti mention event trigger node comput label score node pairwis link use local classifi search global optim graph beam decod decod stage incorpor global featur captur crosssubtask crossinst interact experi show ad global featur improv perform model achiev new state oftheart subtask addit onei use languagespecif featur prove easili appli new languag train multilingu manner,2020,384
Incorporating Non-local Information into Information Extraction Systems by Gibbs Sampling,"Most current statistical natural language processing models use only local features so as to permit dynamic programming in inference, but this makes them unable to fully account for the long distance structure that is prevalent in language use. We show how to solve this dilemma with Gibbs sampling, a simple Monte Carlo method used to perform approximate inference in factored probabilistic models. By using simulated annealing in place of Viterbi decoding in sequence models such as HMMs, CMMs, and CRFs, it is possible to incorporate non-local structure while preserving tractable inference. We use this technique to augment an existing CRF-based information extraction system with long-distance dependency models, enforcing label consistency and extraction template consistency constraints. This technique results in an error reduction of up to 9% over state-of-the-art systems on two established information extraction tasks.",current statist natur languag process model use local featur permit dynam program infer make unabl fulli account long distanc structur preval languag use show solv dilemma gibb sampl simpl mont carlo method use perform approxim infer factor probabilist model use simul anneal place viterbi decod sequenc model hmm cmm crf possibl incorpor nonloc structur preserv tractabl infer use techniqu augment exist crfbase inform extract system longdist depend model enforc label consist extract templat consist constraint techniqu result error reduct stateoftheart system two establish inform extract task,2005,3394
WebFormer: The Web-page Transformer for Structure Information Extraction,"Structure information extraction refers to the task of extracting structured text fields from web pages, such as extracting a product offer from a shopping page including product title, description, brand and price. It is an important research topic which has been widely studied in document understanding and web search. Recent natural language models with sequence modeling have demonstrated state-of-the-art performance on web information extraction. However, effectively serializing tokens from unstructured web pages is challenging in practice due to a variety of web layout patterns. Limited work has focused on modeling the web layout for extracting the text fields. In this paper, we introduce WebFormer, a Web-page transFormer model for structure information extraction from web documents. First, we design HTML tokens for each DOM node in the HTML by embedding representations from their neighboring tokens through graph attention. Second, we construct rich attention patterns between HTML tokens and text tokens, which leverages the web layout for effective attention weight computation. We conduct an extensive set of experiments on SWDE and Common Crawl benchmarks. Experimental results demonstrate the superior performance of the proposed approach over several state-of-the-art methods.",structur inform extract refer task extract structur text field web page extract product offer shop page includ product titl descript brand price import research topic wide studi document understand web search recent natur languag model sequenc model demonstr stateoftheart perform web inform extract howev effect serial token unstructur web page challeng practic due varieti web layout pattern limit work focus model web layout extract text field paper introduc webform webpag transform model structur inform extract web document first design html token dom node html embed represent neighbor token graph attent second construct rich attent pattern html token text token leverag web layout effect attent weight comput conduct extens set experi swde common crawl benchmark experiment result demonstr superior perform propos approach sever stateoftheart method,2022,60
Information extraction from electronic medical documents: state of the art and future research directions,,,2022,54
A Survey of Information Extraction Based on Deep Learning,"As a core task and an important link in the fields of natural language understanding and information retrieval, information extraction (IE) can structure and semanticize unstructured multi-modal information. In recent years, deep learning (DL) has attracted considerable research attention to IE tasks. Deep learning-based entity relation extraction techniques have gradually surpassed traditional feature- and kernel-function-based methods in terms of the depth of feature extraction and model accuracy. In this paper, we explain the basic concepts of IE and DL, primarily expounding on the research progress and achievements of DL technologies in the field of IE. At the level of IE tasks, it is expounded from entity relationship extraction, event extraction, and multi-modal information extraction three aspects, and creates a comparative analysis of various extraction techniques. We also summarize the prospects and development trends in DL in the field of IE as well as difficulties requiring further study. It is believed that research can be carried out in the direction of multi-model and multi-task joint extraction, information extraction based on knowledge enhancement, and information fusion based on multi-modal at the method level. At the model level, further research should be carried out in the aspects of strengthening theoretical research, model lightweight, and improving model generalization ability.",core task import link field natur languag understand inform retriev inform extract ie structur semantic unstructur multimod inform recent year deep learn dl attract consider research attent ie task deep learningbas entiti relat extract techniqu gradual surpass tradit featur kernelfunctionbas method term depth featur extract model accuraci paper explain basic concept ie dl primarili expound research progress achiev dl technolog field ie level ie task expound entiti relationship extract event extract multimod inform extract three aspect creat compar analysi variou extract techniqu also summar prospect develop trend dl field ie well difficulti requir studi believ research carri direct multimodel multitask joint extract inform extract base knowledg enhanc inform fusion base multimod method level model level research carri aspect strengthen theoret research model lightweight improv model gener abil,2022,43
MatSciBERT: A materials domain language model for text mining and information extraction,,,2021,149
Kleister: Key Information Extraction Datasets Involving Long Documents with Complex Layouts,,,2021,78
Abstract Meaning Representation Guided Graph Encoding and Decoding for Joint Information Extraction,"The tasks of Rich Semantic Parsing, such as Abstract Meaning Representation (AMR), share similar goals with Information Extraction (IE) to convert natural language texts into structured semantic representations. To take advantage of such similarity, we propose a novel AMR-guided framework for joint information extraction to discover entities, relations, and events with the help of a pre-trained AMR parser. Our framework consists of two novel components: 1) an AMR based semantic graph aggregator to let the candidate entity and event trigger nodes collect neighborhood information from AMR graph for passing message among related knowledge elements; 2) an AMR guided graph decoder to extract knowledge elements based on the order decided by the hierarchical structures in AMR. Experiments on multiple datasets have shown that the AMR graph encoder and decoder have provided significant gains and our approach has achieved new state-of-the-art performance on all IE subtasks.",task rich semant par abstract mean represent amr share similar goal inform extract ie convert natur languag text structur semant represent take advantag similar propos novel amrguid framework joint inform extract discov entiti relat event help pretrain amr parser framework consist two novel compon amr base semant graph aggreg let candid entiti event trigger node collect neighborhood inform amr graph pas messag among relat knowledg element amr guid graph decod extract knowledg element base order decid hierarch structur amr experi multipl dataset shown amr graph encod decod provid signific gain approach achiev new stateoftheart perform ie subtask,2021,77
Towards Robust Visual Information Extraction in Real World: New Dataset and Novel Solution,"Visual Information Extraction (VIE) has attracted considerable attention recently owing to its various advanced applications such as document understanding, automatic marking and intelligent education. Most existing works decoupled this problem into several independent sub-tasks of text spotting (text detection and recognition) and information extraction, which completely ignored the high correlation among them during optimization. In this paper, we propose a robust Visual Information Extraction System (VIES) towards real-world scenarios, which is an unified end-to-end trainable framework for simultaneous text detection, recognition and information extraction by taking a single document image as input and outputting the structured information. Specifically, the information extraction branch collects abundant visual and semantic representations from text spotting for multimodal feature fusion and conversely, provides higher-level semantic clues to contribute to the optimization of text spotting. Moreover, regarding the shortage of public benchmarks, we construct a fully-annotated dataset called EPHOIE (https://github.com/HCIILAB/EPHOIE), which is the first Chinese benchmark for both text spotting and visual information extraction. EPHOIE consists of 1,494 images of examination paper head with complex layouts and background, including a total of 15,771 Chinese handwritten or printed text instances. Compared with the state-of-the-art methods, our VIES shows significant superior performance on the EPHOIE dataset and achieves a 9.01% F-score gain on the widely used SROIE dataset under the end-to-end scenario.",visual inform extract vie attract consider attent recent owe variou advanc applic document understand automat mark intellig educ exist work decoupl problem sever independ subtask text spot text detect recognit inform extract complet ignor high correl among optim paper propos robust visual inform extract system vie toward realworld scenario unifi endtoend trainabl framework simultan text detect recognit inform extract take singl document imag input output structur inform specif inform extract branch collect abund visual semant represent text spot multimod featur fusion convers provid higherlevel semant clue contribut optim text spot moreov regard shortag public benchmark construct fullyannot dataset call ephoi httpsgithubcomhciilabephoi first chine benchmark text spot visual inform extract ephoi consist imag examin paper head complex layout background includ total chine handwritten print text instanc compar stateoftheart method vie show signific superior perform ephoi dataset achiev fscore gain wide use sroie dataset endtoend scenario,2021,74
SciREX: A Challenge Dataset for Document-Level Information Extraction,"Extracting information from full documents is an important problem in many domains, but most previous work focus on identifying relationships within a sentence or a paragraph. It is challenging to create a large-scale information extraction (IE) dataset at the document level since it requires an understanding of the whole document to annotate entities and their document-level relationships that usually span beyond sentences or even sections. In this paper, we introduce SciREX, a document level IE dataset that encompasses multiple IE tasks, including salient entity identification and document level N-ary relation identification from scientific articles. We annotate our dataset by integrating automatic and human annotations, leveraging existing scientific knowledge resources. We develop a neural model as a strong baseline that extends previous state-of-the-art IE models to document-level IE. Analyzing the model performance shows a significant gap between human performance and current baselines, inviting the community to use our dataset as a challenge to develop document-level IE models. Our data and code are publicly available at https://github.com/allenai/SciREX .",extract inform full document import problem mani domain previou work focu identifi relationship within sentenc paragraph challeng creat largescal inform extract ie dataset document level sinc requir understand whole document annot entiti documentlevel relationship usual span beyond sentenc even section paper introduc scirex document level ie dataset encompass multipl ie task includ salient entiti identif document level nari relat identif scientif articl annot dataset integr automat human annot leverag exist scientif knowledg resourc develop neural model strong baselin extend previou stateoftheart ie model documentlevel ie analyz model perform show signific gap human perform current baselin invit commun use dataset challeng develop documentlevel ie model data code publicli avail httpsgithubcomallenaiscirex,2020,152
Learning from Noisy Labels for Entity-Centric Information Extraction,"Recent information extraction approaches have relied on training deep neural models. However, such models can easily overfit noisy labels and suffer from performance degradation. While it is very costly to filter noisy labels in large learning resources, recent studies show that such labels take more training steps to be memorized and are more frequently forgotten than clean labels, therefore are identifiable in training. Motivated by such properties, we propose a simple co-regularization framework for entity-centric information extraction, which consists of several neural models with identical structures but different parameter initialization. These models are jointly optimized with the task-specific losses and are regularized to generate similar predictions based on an agreement loss, which prevents overfitting on noisy labels. Extensive experiments on two widely used but noisy benchmarks for information extraction, TACRED and CoNLL03, demonstrate the effectiveness of our framework. We release our code to the community for future research.",recent inform extract approach reli train deep neural model howev model easili overfit noisi label suffer perform degrad costli filter noisi label larg learn resourc recent studi show label take train step memor frequent forgotten clean label therefor identifi train motiv properti propos simpl coregular framework entitycentr inform extract consist sever neural model ident structur differ paramet initi model jointli optim taskspecif loss regular gener similar predict base agreement loss prevent overfit noisi label extens experi two wide use noisi benchmark inform extract tacr conll demonstr effect framework releas code commun futur research,2021,62
GenIE: Generative Information Extraction,"Structured and grounded representation of text is typically formalized by closed information extraction, the problem of extracting an exhaustive set of (subject, relation, object) triplets that are consistent with a predefined set of entities and relations from a knowledge base schema. Most existing works are pipelines prone to error accumulation, and all approaches are only applicable to unrealistically small numbers of entities and relations. We introduce GenIE (generative information extraction), the first end-to-end autoregressive formulation of closed information extraction. GenIE naturally exploits the language knowledge from the pre-trained transformer by autoregressively generating relations and entities in textual form. Thanks to a new bi-level constrained generation strategy, only triplets consistent with the predefined knowledge base schema are produced. Our experiments show that GenIE is state-of-the-art on closed information extraction, generalizes from fewer training data points than baselines, and scales to a previously unmanageable number of entities and relations. With this work, closed information extraction becomes practical in realistic scenarios, providing new opportunities for downstream tasks. Finally, this work paves the way towards a unified end-to-end approach to the core tasks of information extraction.",structur ground represent text typic formal close inform extract problem extract exhaust set subject relat object triplet consist predefin set entiti relat knowledg base schema exist work pipelin prone error accumul approach applic unrealist small number entiti relat introduc geni gener inform extract first endtoend autoregress formul close inform extract geni natur exploit languag knowledg pretrain transform autoregress gener relat entiti textual form thank new bilevel constrain gener strategi triplet consist predefin knowledg base schema produc experi show geni stateoftheart close inform extract gener fewer train data point baselin scale previous unmanag number entiti relat work close inform extract becom practic realist scenario provid new opportun downstream task final work pave way toward unifi endtoend approach core task inform extract,2021,59
Spatial Dependency Parsing for Semi-Structured Document Information Extraction,"Information Extraction (IE) for semi-structured document images is often approached as a sequence tagging problem by classifying each recognized input token into one of the IOB (Inside, Outside, and Beginning) categories. However, such problem setup has two inherent limitations that (1) it cannot easily handle complex spatial relationships and (2) it is not suitable for highly structured information, which are nevertheless frequently observed in real-world document images. To tackle these issues, we first formulate the IE task as spatial dependency parsing problem that focuses on the relationship among text segment nodes in the documents. Under this setup, we then propose SPADE (SPAtial DEpendency parser) that models highly complex spatial relationships and an arbitrary number of information layers in the documents in an end-to-end manner. We evaluate it on various kinds of documents such as receipts, name cards, forms, and invoices, and show that it achieves a similar or better performance compared to strong baselines including BERT-based IOB taggger, with up to 37.7% improvement.",inform extract ie semistructur document imag often approach sequenc tag problem classifi recogn input token one iob insid outsid begin categori howev problem setup two inher limit cannot easili handl complex spatial relationship suitabl highli structur inform nevertheless frequent observ realworld document imag tackl issu first formul ie task spatial depend par problem focus relationship among text segment node document setup propos spade spatial depend parser model highli complex spatial relationship arbitrari number inform layer document endtoend manner evalu variou kind document receipt name card form invoic show achiev similar better perform compar strong baselin includ bertbas iob taggger improv,2020,86
TRIE: End-to-End Text Reading and Information Extraction for Document Understanding,"Since real-world ubiquitous documents (e.g., invoices, tickets, resumes and leaflets) contain rich information, automatic document image understanding has become a hot topic. Most existing works decouple the problem into two separate tasks, (1) text reading for detecting and recognizing texts in images and (2) information extraction for analyzing and extracting key elements from previously extracted plain text.However, they mainly focus on improving information extraction task, while neglecting the fact that text reading and information extraction are mutually correlated. In this paper, we propose a unified end-to-end text reading and information extraction network, where the two tasks can reinforce each other. Specifically, the multimodal visual and textual features of text reading are fused for information extraction and in turn, the semantics in information extraction contribute to the optimization of text reading. On three real-world datasets with diverse document images (from fixed layout to variable layout, from structured text to semi-structured text), our proposed method significantly outperforms the state-of-the-art methods in both efficiency and accuracy.",sinc realworld ubiquit document eg invoic ticket resum leaflet contain rich inform automat document imag understand becom hot topic exist work decoupl problem two separ task text read detect recogn text imag inform extract analyz extract key element previous extract plain texthowev mainli focu improv inform extract task neglect fact text read inform extract mutual correl paper propos unifi endtoend text read inform extract network two task reinforc specif multimod visual textual featur text read fuse inform extract turn semant inform extract contribut optim text read three realworld dataset diver document imag fix layout variabl layout structur text semistructur text propos method significantli outperform stateoftheart method effici accuraci,2020,96
IMoJIE: Iterative Memory-Based Joint Open Information Extraction,"While traditional systems for Open Information Extraction were statistical and rule-based, recently neural models have been introduced for the task. Our work builds upon CopyAttention, a sequence generation OpenIE model (Cui et. al. 18). Our analysis reveals that CopyAttention produces a constant number of extractions per sentence, and its extracted tuples often express redundant information. We present IMoJIE, an extension to CopyAttention, which produces the next extraction conditioned on all previously extracted tuples. This approach overcomes both shortcomings of CopyAttention, resulting in a variable number of diverse extractions per sentence. We train IMoJIE on training data bootstrapped from extractions of several non-neural systems, which have been automatically filtered to reduce redundancy and noise. IMoJIE outperforms CopyAttention by about 18 F1 pts, and a BERT-based strong baseline by 2 F1 pts, establishing a new state of the art for the task.",tradit system open inform extract statist rulebas recent neural model introduc task work build upon copyattent sequenc gener openi model cui et al analysi reveal copyattent produc constant number extract per sentenc extract tupl often express redund inform present imoji extens copyattent produc next extract condit previous extract tupl approach overcom shortcom copyattent result variabl number diver extract per sentenc train imoji train data bootstrap extract sever nonneur system automat filter reduc redund nois imoji outperform copyattent f pt bertbas strong baselin f pt establish new state art task,2020,71
RESIN: A Dockerized Schema-Guided Cross-document Cross-lingual Cross-media Information Extraction and Event Tracking System,"We present a new information extraction system that can automatically construct temporal event graphs from a collection of news documents from multiple sources, multiple languages (English and Spanish for our experiment), and multiple data modalities (speech, text, image and video). The system advances state-of-the-art from two aspects: (1) extending from sentence-level event extraction to cross-document cross-lingual cross-media event extraction, coreference resolution and temporal event tracking; (2) using human curated event schema library to match and enhance the extraction output. We have made the dockerlized system publicly available for research purpose at GitHub, with a demo video.",present new inform extract system automat construct tempor event graph collect news document multipl sourc multipl languag english spanish experi multipl data modal speech text imag video system advanc stateoftheart two aspect extend sentencelevel event extract crossdocu crosslingu crossmedia event extract corefer resolut tempor event track use human curat event schema librari match enhanc extract output made dockerl system publicli avail research purpos github demo video,2021,57
LAMBERT: Layout-Aware Language Modeling for Information Extraction,,,2020,83
Named Entity Recognition and Relation Detection for Biomedical Information Extraction,"The number of scientific publications in the literature is steadily growing, containing our knowledge in the biomedical, health, and clinical sciences. Since there is currently no automatic archiving of the obtained results, much of this information remains buried in textual details not readily available for further usage or analysis. For this reason, natural language processing (NLP) and text mining methods are used for information extraction from such publications. In this paper, we review practices for Named Entity Recognition (NER) and Relation Detection (RD), allowing, e.g., to identify interactions between proteins and drugs or genes and diseases. This information can be integrated into networks to summarize large-scale details on a particular biomedical or clinical problem, which is then amenable for easy data management and further analysis. Furthermore, we survey novel deep learning methods that have recently been introduced for such tasks.",number scientif public literatur steadili grow contain knowledg biomed health clinic scienc sinc current automat archiv obtain result much inform remain buri textual detail readili avail usag analysi reason natur languag process nlp text mine method use inform extract public paper review practic name entiti recognit ner relat detect rd allow eg identifi interact protein drug gene diseas inform integr network summar largescal detail particular biomed clinic problem amen easi data manag analysi furthermor survey novel deep learn method recent introduc task,2020,117
Enriching contextualized language model from knowledge graph for biomedical information extraction,"Biomedical information extraction (BioIE) is an important task. The aim is to analyze biomedical texts and extract structured information such as named entities and semantic relations between them. In recent years, pre-trained language models have largely improved the performance of BioIE. However, they neglect to incorporate external structural knowledge, which can provide rich factual information to support the underlying understanding and reasoning for biomedical information extraction. In this paper, we first evaluate current extraction methods, including vanilla neural networks, general language models and pre-trained contextualized language models on biomedical information extraction tasks, including named entity recognition, relation extraction and event extraction. We then propose to enrich a contextualized language model by integrating a large scale of biomedical knowledge graphs (namely, BioKGLM). In order to effectively encode knowledge, we explore a three-stage training procedure and introduce different fusion strategies to facilitate knowledge injection. Experimental results on multiple tasks show that BioKGLM consistently outperforms state-of-the-art extraction models. A further analysis proves that BioKGLM can capture the underlying relations between biomedical knowledge concepts, which are crucial for BioIE.",biomed inform extract bioie import task aim analyz biomed text extract structur inform name entiti semant relat recent year pretrain languag model larg improv perform bioie howev neglect incorpor extern structur knowledg provid rich factual inform support underli understand reason biomed inform extract paper first evalu current extract method includ vanilla neural network gener languag model pretrain contextu languag model biomed inform extract task includ name entiti recognit relat extract event extract propos enrich contextu languag model integr larg scale biomed knowledg graph name biokglm order effect encod knowledg explor threestag train procedur introduc differ fusion strategi facilit knowledg inject experiment result multipl task show biokglm consist outperform stateoftheart extract model analysi prove biokglm captur underli relat biomed knowledg concept crucial bioie,2020,121
ICDAR2019 Competition on Scanned Receipt OCR and Information Extraction,"The ICDAR 2019 Challenge on ""Scanned receipts OCR and key information extraction"" (SROIE) covers important aspects related to the automated analysis of scanned receipts. The SROIE tasks play a key role in many document analysis systems and hold significant commercial potential. Although a lot of work has been published over the years on administrative document analysis, the community has advanced relatively slowly, as most datasets have been kept private. One of the key contributions of SROIE to the document analysis community is to offer a first, standardized dataset of 1000 whole scanned receipt images and annotations, as well as an evaluation procedure for such tasks. The Challenge is structured around three tasks, namely Scanned Receipt Text Localization (Task 1), Scanned Receipt OCR (Task 2) and Key Information Extraction from Scanned Receipts (Task 3). The competition opened on 10th February, 2019 and closed on 5th May, 2019. We received 29, 24 and 18 valid submissions received for the three competition tasks, respectively. This report presents the competition datasets, define the tasks and the evaluation protocols, offer detailed submission statistics, as well as an analysis of the submitted performance. While the tasks of text localization and recognition seem to be relatively easy to tackle, it is interesting to observe the variety of ideas and approaches proposed for the information extraction task. According to the submissions' performance we believe there is still margin for improving information extraction performance, although the current dataset would have to grow substantially in following editions. Given the success of the SROIE competition evidenced by the wide interest generated and the healthy number of submissions from academic, research institutes and industry over different countries, we consider that the SROIE competition can evolve into a useful resource for the community, drawing further attention and promoting research and development efforts in this field.",icdar challeng scan receipt ocr key inform extract sroie cover import aspect relat autom analysi scan receipt sroie task play key role mani document analysi system hold signific commerci potenti although lot work publish year administr document analysi commun advanc rel slowli dataset kept privat one key contribut sroie document analysi commun offer first standard dataset whole scan receipt imag annot well evalu procedur task challeng structur around three task name scan receipt text local task scan receipt ocr task key inform extract scan receipt task competit open th februari close th may receiv valid submiss receiv three competit task respect report present competit dataset defin task evalu protocol offer detail submiss statist well analysi submit perform task text local recognit seem rel easi tackl interest observ varieti idea approach propos inform extract task accord submiss perform believ still margin improv inform extract perform although current dataset would grow substanti follow edit given success sroie competit evidenc wide interest gener healthi number submiss academ research institut industri differ countri consid sroie competit evolv use resourc commun draw attent promot research develop effort field,2019,266
A general framework for information extraction using dynamic span graphs,"We introduce a general framework for several information extraction tasks that share span representations using dynamically constructed span graphs. The graphs are dynamically constructed by selecting the most confident entity spans and linking these nodes with confidence-weighted relation types and coreferences. The dynamic span graph allow coreference and relation type confidences to propagate through the graph to iteratively refine the span representations. This is unlike previous multi-task frameworks for information extraction in which the only interaction between tasks is in the shared first-layer LSTM. Our framework significantly outperforms state-of-the-art on multiple information extraction tasks across multiple datasets reflecting different domains. We further observe that the span enumeration approach is good at detecting nested span entities, with significant F1 score improvement on the ACE dataset.",introduc gener framework sever inform extract task share span represent use dynam construct span graph graph dynam construct select confid entiti span link node confidenceweight relat type corefer dynam span graph allow corefer relat type confid propag graph iter refin span represent unlik previou multitask framework inform extract interact task share firstlay lstm framework significantli outperform stateoftheart multipl inform extract task across multipl dataset reflect differ domain observ span enumer approach good detect nest span entiti signific f score improv ace dataset,2019,310
Data-driven materials research enabled by natural language processing and information extraction,"Given the emergence of data science and machine learning throughout all aspects of society, but particularly in the scientific domain, there is increased importance placed on obtaining data. Data in materials science are particularly heterogeneous, based on the significant range in materials classes that are explored and the variety of materials properties that are of interest. This leads to data that range many orders of magnitude, and these data may manifest as numerical text or image-based information, which requires quantitative interpretation. The ability to automatically consume and codify the scientific literature across domains—enabled by techniques adapted from the field of natural language processing—therefore has immense potential to unlock and generate the rich datasets necessary for data science and machine learning. This review focuses on the progress and practices of natural language processing and text mining of materials science literature and highlights opportunities for extracting additional information beyond text contained in figures and tables in articles. We discuss and provide examples for several reasons for the pursuit of natural language processing for materials, including data compilation, hypothesis development, and understanding the trends within and across fields. Current and emerging natural language processing methods along with their applications to materials science are detailed. We, then, discuss natural language processing and data challenges within the materials science domain where future directions may prove valuable.",given emerg data scienc machin learn throughout aspect societi particularli scientif domain increas import place obtain data data materi scienc particularli heterogen base signific rang materi class explor varieti materi properti interest lead data rang mani order magnitud data may manifest numer text imagebas inform requir quantit interpret abil automat consum codifi scientif literatur across domainsen techniqu adapt field natur languag processingtherefor immens potenti unlock gener rich dataset necessari data scienc machin learn review focus progress practic natur languag process text mine materi scienc literatur highlight opportun extract addit inform beyond text contain figur tabl articl discus provid exampl sever reason pursuit natur languag process materi includ data compil hypothesi develop understand trend within across field current emerg natur languag process method along applic materi scienc detail discus natur languag process data challeng within materi scienc domain futur direct may prove valuabl,2020,200
Supervised Open Information Extraction,"We present data and methods that enable a supervised learning approach to Open Information Extraction (Open IE). Central to the approach is a novel formulation of Open IE as a sequence tagging problem, addressing challenges such as encoding multiple extractions for a predicate. We also develop a bi-LSTM transducer, extending recent deep Semantic Role Labeling models to extract Open IE tuples and provide confidence scores for tuning their precision-recall tradeoff. Furthermore, we show that the recently released Question-Answer Meaning Representation dataset can be automatically converted into an Open IE corpus which significantly increases the amount of available training data. Our supervised model outperforms the existing state-of-the-art Open IE systems on benchmark datasets.",present data method enabl supervis learn approach open inform extract open ie central approach novel formul open ie sequenc tag problem address challeng encod multipl extract predic also develop bilstm transduc extend recent deep semant role label model extract open ie tupl provid confid score tune precisionrecal tradeoff furthermor show recent releas questionansw mean represent dataset automat convert open ie corpu significantli increas amount avail train data supervis model outperform exist stateoftheart open ie system benchmark dataset,2018,291
Medical Information Extraction in the Age of Deep Learning,"Summary Objectives: We survey recent developments in medical Information Extraction (IE) as reported in the literature from the past three years. Our focus is on the fundamental methodological paradigm shift from standard Machine Learning (ML) techniques to Deep Neural Networks (DNNs). We describe applications of this new paradigm concentrating on two basic IE tasks, named entity recognition and relation extraction, for two selected semantic classes—diseases and drugs (or medications)—and relations between them. Methods: For the time period from 2017 to early 2020, we searched for relevant publications from three major scientific communities: medicine and medical informatics, natural language processing, as well as neural networks and artificial intelligence. Results: In the past decade, the field of Natural Language Processing (NLP) has undergone a profound methodological shift from symbolic to distributed representations based on the paradigm of Deep Learning (DL). Meanwhile, this trend is, although with some delay, also reflected in the medical NLP community. In the reporting period, overwhelming experimental evidence has been gathered, as illustrated in this survey for medical IE, that DL-based approaches outperform non-DL ones by often large margins. Still, small-sized and access-limited corpora create intrinsic problems for data-greedy DL as do special linguistic phenomena of medical sublanguages that have to be overcome by adaptive learning strategies. Conclusions: The paradigm shift from (feature-engineered) ML to DNNs changes the fundamental methodological rules of the game for medical NLP. This change is by no means restricted to medical IE but should also deeply influence other areas of medical informatics, either NLP- or non-NLP-based.",summari object survey recent develop medic inform extract ie report literatur past three year focu fundament methodolog paradigm shift standard machin learn ml techniqu deep neural network dnn describ applic new paradigm concentr two basic ie task name entiti recognit relat extract two select semant classesdiseas drug medicationsand relat method time period earli search relev public three major scientif commun medicin medic informat natur languag process well neural network artifici intellig result past decad field natur languag process nlp undergon profound methodolog shift symbol distribut represent base paradigm deep learn dl meanwhil trend although delay also reflect medic nlp commun report period overwhelm experiment evid gather illustr survey medic ie dlbase approach outperform nondl one often larg margin still smallsiz accesslimit corpus creat intrins problem datagreedi dl special linguist phenomenon medic sublanguag overcom adapt learn strategi conclus paradigm shift featureengin ml dnn chang fundament methodolog rule game medic nlp chang mean restrict medic ie also deepli influenc area medic informat either nlp nonnlpbas,2020,69
Representation Learning for Information Extraction from Form-like Documents,"We propose a novel approach using representation learning for tackling the problem of extracting structured information from form-like document images. We propose an extraction system that uses knowledge of the types of the target fields to generate extraction candidates and a neural network architecture that learns a dense representation of each candidate based on neighboring words in the document. These learned representations are not only useful in solving the extraction task for unseen document templates from two different domains but are also interpretable, as we show using loss cases.",propos novel approach use represent learn tackl problem extract structur inform formlik document imag propos extract system use knowledg type target field gener extract candid neural network architectur learn den represent candid base neighbor word document learn represent use solv extract task unseen document templat two differ domain also interpret show use loss case,2020,103
Information extraction meets the Semantic Web: A survey,"Millennium Institute for Foundational Research on Data (IMFD) 
Comision Nacional de Investigacion Cientifica y Tecnologica (CONICYT), CONICYT FONDECYT: 1181896",millennium institut foundat research data imfd comis nacion de investigacion cientifica tecnologica conicyt conicyt fondecyt,2020,175
Open Information Extraction from the Web,"Traditionally, Information Extraction (IE) has focused on satisfying precise, narrow, pre-specified requests from small homogeneous corpora (e.g., extract the location and time of seminars from a set of announcements). Shifting to a new domain requires the user to name the target relations and to manually create new extraction rules or hand-tag new training examples. This manual labor scales linearly with the number of target relations. This paper introduces Open IE (OIE), a new extraction paradigm where the system makes a single data-driven pass over its corpus and extracts a large set of relational tuples without requiring any human input. The paper also introduces TEXTRUNNER, a fully implemented, highly scalable OIE system where the tuples are assigned a probability and indexed to support efficient extraction and exploration via user queries. We report on experiments over a 9,000,000 Web page corpus that compare TEXTRUNNER with KNOWITALL, a state-of-the-art Web IE system. TEXTRUNNER achieves an error reduction of 33% on a comparable set of extractions. Furthermore, in the amount of time it takes KNOWITALL to perform extraction for a handful of pre-specified relations, TEXTRUNNER extracts a far broader set of facts reflecting orders of magnitude more relations, discovered on the fly. We report statistics on TEXTRUNNER’s 11,000,000 highest probability tuples, and show that they contain over 1,000,000 concrete facts and over 6,500,000more abstract assertions.",tradit inform extract ie focus satisfi precis narrow prespecifi request small homogen corpus eg extract locat time seminar set announc shift new domain requir user name target relat manual creat new extract rule handtag new train exampl manual labor scale linearli number target relat paper introduc open ie oie new extract paradigm system make singl datadriven pas corpu extract larg set relat tupl without requir human input paper also introduc textrunn fulli implement highli scalabl oie system tupl assign probabl index support effici extract explor via user queri report experi web page corpu compar textrunn knowital stateoftheart web ie system textrunn achiev error reduct compar set extract furthermor amount time take knowital perform extract hand prespecifi relat textrunn extract far broader set fact reflect order magnitud relat discov fli report statist textrunn highest probabl tupl show contain concret fact abstract assert,2007,2495
Named Entity Recognition and Normalization Applied to Large-Scale Information Extraction from the Materials Science Literature,"The number of published materials science articles has increased manyfold over the past few decades. Now, a major bottleneck in the materials discovery pipeline arises in connecting new results with the previously established literature. A potential solution to this problem is to map the unstructured raw-text of published articles onto structured database entries that allows for programmatic querying. To this end, we apply text-mining with named entity recognition (NER) for large-scale information extraction from the published materials science literature. The NER model is trained to extract summary-level information from materials science documents, including: inorganic material mentions, sample descriptors, phase labels, material properties and applications, as well as any synthesis and characterization methods used. Our classifier achieves an accuracy (f1) of 87%, and is applied to information extraction from 3.27 million materials science abstracts. We extract more than 80 million materials-science-related named entities, and the content of each abstract is represented as a database entry in a structured format. We demonstrate that simple database queries can be used to answer complex ``meta-questions"" of the published literature that would have previously required laborious, manual literature searches to answer. All of our data and functionality has been made freely available (https://github.com/materialsintelligence/matscholar), and we expect these results to accelerate the pace of future materials science discovery.",number publish materi scienc articl increas manyfold past decad major bottleneck materi discoveri pipelin aris connect new result previous establish literatur potenti solut problem map unstructur rawtext publish articl onto structur databas entri allow programmat queri end appli textmin name entiti recognit ner largescal inform extract publish materi scienc literatur ner model train extract summarylevel inform materi scienc document includ inorgan materi mention sampl descriptor phase label materi properti applic well synthesi character method use classifi achiev accuraci f appli inform extract million materi scienc abstract extract million materialssciencerel name entiti content abstract repres databas entri structur format demonstr simpl databas queri use answer complex metaquest publish literatur would previous requir labori manual literatur search answer data function made freeli avail httpsgithubcommaterialsintelligencematscholar expect result acceler pace futur materi scienc discoveri,2019,217
Graph Convolution for Multimodal Information Extraction from Visually Rich Documents,"Visually rich documents (VRDs) are ubiquitous in daily business and life. Examples are purchase receipts, insurance policy documents, custom declaration forms and so on. In VRDs, visual and layout information is critical for document understanding, and texts in such documents cannot be serialized into the one-dimensional sequence without losing information. Classic information extraction models such as BiLSTM-CRF typically operate on text sequences and do not incorporate visual features. In this paper, we introduce a graph convolution based model to combine textual and visual information presented in VRDs. Graph embeddings are trained to summarize the context of a text segment in the document, and further combined with text embeddings for entity extraction. Extensive experiments have been conducted to show that our method outperforms BiLSTM-CRF baselines by significant margins, on two real-world datasets. Additionally, ablation studies are also performed to evaluate the effectiveness of each component of our model.",visual rich document vrd ubiquit daili busi life exampl purchas receipt insur polici document custom declar form vrd visual layout inform critic document understand text document cannot serial onedimension sequenc without lose inform classic inform extract model bilstmcrf typic oper text sequenc incorpor visual featur paper introduc graph convolut base model combin textual visual inform present vrd graph embed train summar context text segment document combin text embed entiti extract extens experi conduct show method outperform bilstmcrf baselin signific margin two realworld dataset addit ablat studi also perform evalu effect compon model,2019,170
Clinical information extraction applications: A literature review,,,2018,654
An analytical study of information extraction from unstructured and multidimensional big data,,,2019,149
DWIE: an entity-centric dataset for multi-task document-level information extraction,,,2020,55
Free Your CSI: A Channel State Information Extraction Platform For Modern Wi-Fi Chipsets,"Modern wireless transmission systems heavily benefit from knowing the channel response. The evaluation of Channel State Information (CSI) during the reception of a frame preamble is fundamental to properly equalizing the rest of the transmission at the receiver side. Reporting this state information back to the transmitter facilitates mechanisms such as beamforming and MIMO, thus boosting the network performance. While these features are an integral part of standards such as 802.11ac, accessing CSI data on commercial devices is either not possible, limited to outdated chipsets or very inflexible. This hinders the research and development of innovative CSI-dependent techniques including localization, object tracking, and interference evaluation. To help researchers and practitioners, we introduce the nexmon CSI Extractor Tool. It allows per-frame CSI extraction for up to four spatial streams using up to four receive chains on modern Broadcom and Cypress Wi-Fi chips with up to 80MHz bandwidth in both the 2.4 and 5GHz bands. The tool supports devices ranging from the low-cost Raspberry Pi platform, over mobile platforms such as Nexus smartphones to state-of-the-art Wi-Fi APs. We release all tools and Wi-Fi firmware patches as extensible open source project. It includes our user-friendly smartphone application to demonstrate the CSI extraction capabilities in form of a waterfall diagram.",modern wireless transmiss system heavili benefit know channel respons evalu channel state inform csi recept frame preambl fundament properli equal rest transmiss receiv side report state inform back transmitt facilit mechan beamform mimo thu boost network perform featur integr part standard ac access csi data commerci devic either possibl limit outdat chipset inflex hinder research develop innov csidepend techniqu includ local object track interfer evalu help research practition introduc nexmon csi extractor tool allow perfram csi extract four spatial stream use four receiv chain modern broadcom cypress wifi chip mhz bandwidth ghz band tool support devic rang lowcost raspberri pi platform mobil platform nexu smartphon stateoftheart wifi ap releas tool wifi firmwar patch extens open sourc project includ userfriendli smartphon applic demonstr csi extract capabl form waterfal diagram,2019,243
Span Model for Open Information Extraction on Accurate Corpus,"Open information extraction (Open IE) is a challenging task especially due to its brittle data basis. Most of Open IE systems have to be trained on automatically built corpus and evaluated on inaccurate test set. In this work, we first alleviate this difficulty from both sides of training and test sets. For the former, we propose an improved model design to more sufficiently exploit training dataset. For the latter, we present our accurately re-annotated benchmark test set (Re-OIE6) according to a series of linguistic observation and analysis. Then, we introduce a span model instead of previous adopted sequence labeling formulization for n-ary Open IE. Our newly introduced model achieves new state-of-the-art performance on both benchmark evaluation datasets.",open inform extract open ie challeng task especi due brittl data basi open ie system train automat built corpu evalu inaccur test set work first allevi difficulti side train test set former propos improv model design suffici exploit train dataset latter present accur reannot benchmark test set reoie accord seri linguist observ analysi introduc span model instead previou adopt sequenc label formul nari open ie newli introduc model achiev new stateoftheart perform benchmark evalu dataset,2019,78
Neural Open Information Extraction,"Conventional Open Information Extraction (Open IE) systems are usually built on hand-crafted patterns from other NLP tools such as syntactic parsing, yet they face problems of error propagation. In this paper, we propose a neural Open IE approach with an encoder-decoder framework. Distinct from existing methods, the neural Open IE approach learns highly confident arguments and relation tuples bootstrapped from a state-of-the-art Open IE system. An empirical study on a large benchmark dataset shows that the neural Open IE system significantly outperforms several baselines, while maintaining comparable computational efficiency.",convent open inform extract open ie system usual built handcraft pattern nlp tool syntact par yet face problem error propag paper propos neural open ie approach encoderdecod framework distinct exist method neural open ie approach learn highli confid argument relat tupl bootstrap stateoftheart open ie system empir studi larg benchmark dataset show neural open ie system significantli outperform sever baselin maintain compar comput effici,2018,154
A Survey on Open Information Extraction,"We provide a detailed overview of the various approaches that were proposed to date to solve the task of Open Information Extraction. We present the major challenges that such systems face, show the evolution of the suggested approaches over time and depict the specific issues they address. In addition, we provide a critique of the commonly applied evaluation procedures for assessing the performance of Open IE systems and highlight some directions for future work.",provid detail overview variou approach propos date solv task open inform extract present major challeng system face show evolut suggest approach time depict specif issu address addit provid critiqu commonli appli evalu procedur assess perform open ie system highlight direct futur work,2018,173
GraphIE: A Graph-Based Framework for Information Extraction,"Most modern Information Extraction (IE) systems are implemented as sequential taggers and only model local dependencies. Non-local and non-sequential context is, however, a valuable source of information to improve predictions. In this paper, we introduce GraphIE, a framework that operates over a graph representing a broad set of dependencies between textual units (i.e. words or sentences). The algorithm propagates information between connected nodes through graph convolutions, generating a richer representation that can be exploited to improve word-level predictions. Evaluation on three different tasks — namely textual, social media and visual information extraction — shows that GraphIE consistently outperforms the state-of-the-art sequence tagging model by a significant margin.",modern inform extract ie system implement sequenti tagger model local depend nonloc nonsequenti context howev valuabl sourc inform improv predict paper introduc graphi framework oper graph repres broad set depend textual unit ie word sentenc algorithm propag inform connect node graph convolut gener richer represent exploit improv wordlevel predict evalu three differ task name textual social medium visual inform extract show graphi consist outperform stateoftheart sequenc tag model signific margin,2018,109
Information extraction from scientific articles: a survey,,,2018,97
DuIE: A Large-Scale Chinese Dataset for Information Extraction,,,2019,44
Information extraction framework for Kurunthogai,,,2019,46
Twenty-five years of information extraction,"Abstract Information extraction is the process of converting unstructured text into a structured data base containing selected information from the text. It is an essential step in making the information content of the text usable for further processing. In this paper, we describe how information extraction has changed over the past 25 years, moving from hand-coded rules to neural networks, with a few stops on the way. We connect these changes to research advances in NLP and to the evaluations organized by the US Government.",abstract inform extract process convert unstructur text structur data base contain select inform text essenti step make inform content text usabl process paper describ inform extract chang past year move handcod rule neural network stop way connect chang research advanc nlp evalu organ u govern,2019,55
Leveraging Linguistic Structure For Open Domain Information Extraction,"Relation triples produced by open domain information extraction (open IE) systems are useful for question answering, inference, and other IE tasks. Traditionally these are extracted using a large set of patterns; however, this approach is brittle on out-of-domain text and long-range dependencies, and gives no insight into the substructure of the arguments. We replace this large pattern set with a few patterns for canonically structured sentences, and shift the focus to a classifier which learns to extract self-contained clauses from longer sentences. We then run natural logic inference over these short clauses to determine the maximally specific arguments for each candidate triple. We show that our approach outperforms a state-of-the-art open IE system on the end-to-end TAC-KBP 2013 Slot Filling task.",relat tripl produc open domain inform extract open ie system use question answer infer ie task tradit extract use larg set pattern howev approach brittl outofdomain text longrang depend give insight substructur argument replac larg pattern set pattern canon structur sentenc shift focu classifi learn extract selfcontain claus longer sentenc run natur logic infer short claus determin maxim specif argument candid tripl show approach outperform stateoftheart open ie system endtoend tackbp slot fill task,2015,743
Open Information Extraction from Conjunctive Sentences,"We develop CALM, a coordination analyzer that improves upon the conjuncts identified from dependency parses. It uses a language model based scoring and several linguistic constraints to search over hierarchical conjunct boundaries (for nested coordination). By splitting a conjunctive sentence around these conjuncts, CALM outputs several simple sentences. We demonstrate the value of our coordination analyzer in the end task of Open Information Extraction (Open IE). State-of-the-art Open IE systems lose substantial yield due to ineffective processing of conjunctive sentences. Our Open IE system, CALMIE, performs extraction over the simple sentences identified by CALM to obtain up to 1.8x yield with a moderate increase in precision compared to extractions from original sentences.",develop calm coordin analyz improv upon conjunct identifi depend par use languag model base score sever linguist constraint search hierarch conjunct boundari nest coordin split conjunct sentenc around conjunct calm output sever simpl sentenc demonstr valu coordin analyz end task open inform extract open ie stateoftheart open ie system lose substanti yield due ineffect process conjunct sentenc open ie system calmi perform extract simpl sentenc identifi calm obtain x yield moder increas precis compar extract origin sentenc,2018,86
OPIEC: An Open Information Extraction Corpus,"Open information extraction (OIE) systems extract relations and their arguments from natural language text in an unsupervised manner. The resulting extractions are a valuable resource for downstream tasks such as knowledge base construction, open question answering, or event schema induction. In this paper, we release, describe, and analyze an OIE corpus called OPIEC, which was extracted from the text of English Wikipedia. OPIEC complements the available OIE resources: It is the largest OIE corpus publicly available to date (over 340M triples) and contains valuable metadata such as provenance information, confidence scores, linguistic annotations, and semantic annotations including spatial and temporal information. We analyze the OPIEC corpus by comparing its content with knowledge bases such as DBpedia or YAGO, which are also based on Wikipedia. We found that most of the facts between entities present in OPIEC cannot be found in DBpedia and/or YAGO, that OIE facts often differ in the level of specificity compared to knowledge base facts, and that OIE open relations are generally highly polysemous. We believe that the OPIEC corpus is a valuable resource for future research on automated knowledge base construction.",open inform extract oie system extract relat argument natur languag text unsupervis manner result extract valuabl resourc downstream task knowledg base construct open question answer event schema induct paper releas describ analyz oie corpu call opiec extract text english wikipedia opiec complement avail oie resourc largest oie corpu publicli avail date tripl contain valuabl metadata proven inform confid score linguist annot semant annot includ spatial tempor inform analyz opiec corpu compar content knowledg base dbpedia yago also base wikipedia found fact entiti present opiec cannot found dbpedia andor yago oie fact often differ level specif compar knowledg base fact oie open relat gener highli polysem believ opiec corpu valuabl resourc futur research autom knowledg base construct,2019,37
Automatic Information Extraction from Piping and Instrumentation Diagrams,"One of the most common modes of representing engineering schematics are Piping and Instrumentation diagrams (P&IDs) that describe the layout of an engineering process flow along with the interconnected process equipment. Over the years, P&ID diagrams have been manually generated, scanned and stored as image files. These files need to be digitized for purposes of inventory management and updation, and easy reference to different components of the schematics. There are several challenging vision problems associated with digitizing real world P&ID diagrams. Real world P&IDs come in several different resolutions, and often contain noisy textual information. Extraction of instrumentation information from these diagrams involves accurate detection of symbols that frequently have minute visual differences between them. Identification of pipelines that may converge and diverge at different points in the image is a further cause for concern. Due to these reasons, to the best of our knowledge, no system has been proposed for end-to-end data extraction from P&ID diagrams. However, with the advent of deep learning and the spectacular successes it has achieved in vision, we hypothesized that it is now possible to re-examine this problem armed with the latest deep learning models. To that end, we present a novel pipeline for information extraction from P&ID sheets via a combination of traditional vision techniques and state-of-the-art deep learning models to identify and isolate pipeline codes, pipelines, inlets and outlets, and for detecting symbols. This is followed by association of the detected components with the appropriate pipeline. The extracted pipeline information is used to populate a tree-like data-structure for capturing the structure of the piping schematics. We evaluated proposed method on a real world dataset of P&ID sheets obtained from an oil firm and have obtained promising results.",one common mode repres engin schemat pipe instrument diagram pid describ layout engin process flow along interconnect process equip year pid diagram manual gener scan store imag file file need digit purpos inventori manag updat easi refer differ compon schemat sever challeng vision problem associ digit real world pid diagram real world pid come sever differ resolut often contain noisi textual inform extract instrument inform diagram involv accur detect symbol frequent minut visual differ identif pipelin may converg diverg differ point imag caus concern due reason best knowledg system propos endtoend data extract pid diagram howev advent deep learn spectacular success achiev vision hypothes possibl reexamin problem arm latest deep learn model end present novel pipelin inform extract pid sheet via combin tradit vision techniqu stateoftheart deep learn model identifi isol pipelin code pipelin inlet outlet detect symbol follow associ detect compon appropri pipelin extract pipelin inform use popul treelik datastructur captur structur pipe schemat evalu propos method real world dataset pid sheet obtain oil firm obtain promis result,2019,46
Identifying Relations for Open Information Extraction,"Open Information Extraction (IE) is the task of extracting assertions from massive corpora without requiring a pre-specified vocabulary. This paper shows that the output of state-of-the-art Open IE systems is rife with uninformative and incoherent extractions. To overcome these problems, we introduce two simple syntactic and lexical constraints on binary relations expressed by verbs. We implemented the constraints in the ReVerb Open IE system, which more than doubles the area under the precision-recall curve relative to previous extractors such as TextRunner and woepos. More than 30% of ReVerb's extractions are at precision 0.8 or higher---compared to virtually none for earlier systems. The paper concludes with a detailed analysis of ReVerb's errors, suggesting directions for future work.",open inform extract ie task extract assert massiv corpus without requir prespecifi vocabulari paper show output stateoftheart open ie system rife uninform incoher extract overcom problem introduc two simpl syntact lexic constraint binari relat express verb implement constraint reverb open ie system doubl area precisionrecal curv rel previou extractor textrunn woepo reverb extract precis highercompar virtual none earlier system paper conclud detail analysi reverb error suggest direct futur work,2011,1394
Survey of Temporal Information Extraction,"Documents contain information that can be used for various applications, such as question answering (QA) system, information retrieval (IR) system, and recommendation system. To use the information, it is necessary to develop a method of extracting such information from the documents written in a form of natural language. There are several kinds of the information (e.g., temporal information, spatial information, semantic role information), where different kinds of information will be extracted with different methods. In this paper, the existing studies about the methods of extracting the temporal information are reported and several related issues are discussed. The issues are about the task boundary of the temporal information extraction, the history of the annotation languages and shared tasks, the research issues, the applications using the temporal information, and evaluation metrics. Although the history of the tasks of temporal information extraction is not long, there have been many studies that tried various methods. This paper gives which approach is known to be the better way of extracting a particular part of the temporal information, and also provides a future research direction.",document contain inform use variou applic question answer qa system inform retriev ir system recommend system use inform necessari develop method extract inform document written form natur languag sever kind inform eg tempor inform spatial inform semant role inform differ kind inform extract differ method paper exist studi method extract tempor inform report sever relat issu discus issu task boundari tempor inform extract histori annot languag share task research issu applic use tempor inform evalu metric although histori task tempor inform extract long mani studi tri variou method paper give approach known better way extract particular part tempor inform also provid futur research direct,2019,18
MinIE: Minimizing Facts in Open Information Extraction,"The goal of Open Information Extraction (OIE) is to extract surface relations and their arguments from natural-language text in an unsupervised, domain-independent manner. In this paper, we propose MinIE, an OIE system that aims to provide useful, compact extractions with high precision and recall. MinIE approaches these goals by (1) representing information about polarity, modality, attribution, and quantities with semantic annotations instead of in the actual extraction, and (2) identifying and removing parts that are considered overly specific. We conducted an experimental study with several real-world datasets and found that MinIE achieves competitive or higher precision and recall than most prior systems, while at the same time producing shorter, semantically enriched extractions.",goal open inform extract oie extract surfac relat argument naturallanguag text unsupervis domainindepend manner paper propos mini oie system aim provid use compact extract high precis recal mini approach goal repres inform polar modal attribut quantiti semant annot instead actual extract identifi remov part consid overli specif conduct experiment studi sever realworld dataset found mini achiev competit higher precis recal prior system time produc shorter semant enrich extract,2017,148
Natural Language Processing for Information Extraction,"With rise of digital age, there is an explosion of information in the form of news, articles, social media, and so on. Much of this data lies in unstructured form and manually managing and effectively making use of it is tedious, boring and labor intensive. This explosion of information and need for more sophisticated and efficient information handling tools gives rise to Information Extraction(IE) and Information Retrieval(IR) technology. Information Extraction systems takes natural language text as input and produces structured information specified by certain criteria, that is relevant to a particular application. Various sub-tasks of IE such as Named Entity Recognition, Coreference Resolution, Named Entity Linking, Relation Extraction, Knowledge Base reasoning forms the building blocks of various high end Natural Language Processing (NLP) tasks such as Machine Translation, Question-Answering System, Natural Language Understanding, Text Summarization and Digital Assistants like Siri, Cortana and Google Now. This paper introduces Information Extraction technology, its various sub-tasks, highlights state-of-the-art research in various IE subtasks, current challenges and future research directions.",rise digit age explos inform form news articl social medium much data lie unstructur form manual manag effect make use tediou bore labor intens explos inform need sophist effici inform handl tool give rise inform extractioni inform retrievalir technolog inform extract system take natur languag text input produc structur inform specifi certain criterion relev particular applic variou subtask ie name entiti recognit corefer resolut name entiti link relat extract knowledg base reason form build block variou high end natur languag process nlp task machin translat questionansw system natur languag understand text summar digit assist like siri cortana googl paper introduc inform extract technolog variou subtask highlight stateoftheart research variou ie subtask current challeng futur research direct,2018,55
A Two-Step Resume Information Extraction Algorithm,"With the rapid growth of Internet-based recruiting, there are a great number of personal resumes among recruiting systems. To gain more attention from the recruiters, most resumes are written in diverse formats, including varying font size, font colour, and table cells. However, the diversity of format is harmful to data mining, such as resume information extraction, automatic job matching, and candidates ranking. Supervised methods and rule-based methods have been proposed to extract facts from resumes, but they strongly rely on hierarchical structure information and large amounts of labelled data, which are hard to collect in reality. In this paper, we propose a two-step resume information extraction approach. In the first step, raw text of resume is identified as different resume blocks. To achieve the goal, we design a novel feature, Writing Style, to model sentence syntax information. Besides word index and punctuation index, word lexical attribute and prediction results of classifiers are included in Writing Style. In the second step, multiple classifiers are employed to identify different attributes of fact information in resumes. Experimental results on a real-world dataset show that the algorithm is feasible and effective.",rapid growth internetbas recruit great number person resum among recruit system gain attent recruit resum written diver format includ vari font size font colour tabl cell howev diver format harm data mine resum inform extract automat job match candid rank supervis method rulebas method propos extract fact resum strongli reli hierarch structur inform larg amount label data hard collect realiti paper propos twostep resum inform extract approach first step raw text resum identifi differ resum block achiev goal design novel featur write style model sentenc syntax inform besid word index punctuat index word lexic attribut predict result classifi includ write style second step multipl classifi employ identifi differ attribut fact inform resum experiment result realworld dataset show algorithm feasibl effect,2018,47
Information extraction and knowledge graph construction from geoscience literature,,,2018,170
Domain Analysis of Information Extraction Techniques,"— In this research, we extant a short outline of Information Extraction, which is also a natural language processing domain that tries to find required information in structured, semi structured and unstructured Data. We draw a taxonomy of information extraction tasks and techniques. The other important thing is that we also extract learning methods like supervised, semi supervised and unsupervised learning and which methods are used in these types of learning. Our domain analysis consists on social media, Biomedical, chemical and unstructured data. There are different tasks included in information extraction which makes this activity more manageable as well as to easy to work in specific domain. We also detect weakness of existing techniques.",research extant short outlin inform extract also natur languag process domain tri find requir inform structur semi structur unstructur data draw taxonomi inform extract task techniqu import thing also extract learn method like supervis semi supervis unsupervis learn method use type learn domain analysi consist social medium biomed chemic unstructur data differ task includ inform extract make activ manag well easi work specif domain also detect weak exist techniqu,2018,42
Answering Complex Questions Using Open Information Extraction,"While there has been substantial progress in factoid question-answering (QA), answering complex questions remains challenging, typically requiring both a large body of knowledge and inference techniques. Open Information Extraction (Open IE) provides a way to generate semi-structured knowledge for QA, but to date such knowledge has only been used to answer simple questions with retrieval-based methods. We overcome this limitation by presenting a method for reasoning with Open IE knowledge, allowing more complex questions to be handled. Using a recently proposed support graph optimization framework for QA, we develop a new inference model for Open IE, in particular one that can work effectively with multiple short facts, noise, and the relational structure of tuples. Our model significantly outperforms a state-of-the-art structured solver on complex questions of varying difficulty, while also removing the reliance on manually curated knowledge.",substanti progress factoid questionansw qa answer complex question remain challeng typic requir larg bodi knowledg infer techniqu open inform extract open ie provid way gener semistructur knowledg qa date knowledg use answer simpl question retrievalbas method overcom limit present method reason open ie knowledg allow complex question handl use recent propos support graph optim framework qa develop new infer model open ie particular one work effect multipl short fact nois relat structur tupl model significantli outperform stateoftheart structur solver complex question vari difficulti also remov relianc manual curat knowledg,2017,111
Hierarchical attention networks for information extraction from cancer pathology reports,"Abstract Objective We explored how a deep learning (DL) approach based on hierarchical attention networks (HANs) can improve model performance for multiple information extraction tasks from unstructured cancer pathology reports compared to conventional methods that do not sufﬁciently capture syntactic and semantic contexts from free-text documents. Materials and Methods Data for our analyses were obtained from 942 deidentiﬁed pathology reports collected by the National Cancer Institute Surveillance, Epidemiology, and End Results program. The HAN was implemented for 2 information extraction tasks: (1) primary site, matched to 12 International Classification of Diseases for Oncology topography codes (7 breast, 5 lung primary sites), and (2) histological grade classiﬁcation, matched to G1–G4. Model performance metrics were compared to conventional machine learning (ML) approaches including naive Bayes, logistic regression, support vector machine, random forest, and extreme gradient boosting, and other DL models, including a recurrent neural network (RNN), a recurrent neural network with attention (RNN w/A), and a convolutional neural network. Results Our results demonstrate that for both information tasks, HAN performed signiﬁcantly better compared to the conventional ML and DL techniques. In particular, across the 2 tasks, the mean micro and macroF-scores for the HAN with pretraining were (0.852,0.708), compared to naive Bayes (0.518, 0.213), logistic regression (0.682, 0.453), support vector machine (0.634, 0.434), random forest (0.698, 0.508), extreme gradient boosting (0.696, 0.522), RNN (0.505, 0.301), RNN w/A (0.637, 0.471), and convolutional neural network (0.714, 0.460). Conclusions HAN-based DL models show promise in information abstraction tasks within unstructured clinical pathology reports.",abstract object explor deep learn dl approach base hierarch attent network han improv model perform multipl inform extract task unstructur cancer patholog report compar convent method sufcient captur syntact semant context freetext document materi method data analys obtain deidenti patholog report collect nation cancer institut surveil epidemiolog end result program han implement inform extract task primari site match intern classif diseas oncolog topographi code breast lung primari site histolog grade classic match gg model perform metric compar convent machin learn ml approach includ naiv bay logist regress support vector machin random forest extrem gradient boost dl model includ recurr neural network rnn recurr neural network attent rnn wa convolut neural network result result demonstr inform task han perform signicantli better compar convent ml dl techniqu particular across task mean micro macrofscor han pretrain compar naiv bay logist regress support vector machin random forest extrem gradient boost rnn rnn wa convolut neural network conclus hanbas dl model show promis inform abstract task within unstructur clinic patholog report,2017,120
Creating a Large Benchmark for Open Information Extraction,"Open information extraction (Open IE) was presented as an unrestricted variant of traditional information extraction. It has been gaining substantial attention, manifested by a large number of automatic Open IE extractors and downstream applications. In spite of this broad attention, the Open IE task deﬁnition has been lacking – there are no formal guidelines and no large scale gold standard annotation. Subsequently, the various implementations of Open IE resorted to small scale post-hoc evaluations, inhibiting an objective and re-producible cross-system comparison. In this work, we develop a methodology that leverages the recent QA-SRL annotation to create a ﬁrst independent and large scale Open IE annotation, 1 and use it to automatically compare the most prominent Open IE systems.",open inform extract open ie present unrestrict variant tradit inform extract gain substanti attent manifest larg number automat open ie extractor downstream applic spite broad attent open ie task denit lack formal guidelin larg scale gold standard annot subsequ variou implement open ie resort small scale posthoc evalu inhibit object reproduc crosssystem comparison work develop methodolog leverag recent qasrl annot creat rst independ larg scale open ie annot use automat compar promin open ie system,2016,140
Open Information Extraction Systems and Downstream Applications,"Open Information Extraction (Open IE) extracts textual tuples comprising relation phrases and argument phrases from within a sentence, without requiring a pre-specified relation vocabulary. In this paper we first describe a decade of our progress on building Open IE extractors, which results in our latest extractor, OPENIE4, which is computationally efficient, outputs n-ary and nested relations, and also outputs relations mediated by nouns in addition to verbs. We also identify several strengths of the Open IE paradigm, which enable it to be a useful intermediate structure for end tasks. We survey its use in both human-facing applications and downstream NLP tasks, including event schema induction, sentence similarity, text comprehension, learning word vector embeddings, and more.",open inform extract open ie extract textual tupl compris relat phrase argument phrase within sentenc without requir prespecifi relat vocabulari paper first describ decad progress build open ie extractor result latest extractor openi comput effici output nari nest relat also output relat mediat noun addit verb also identifi sever strength open ie paradigm enabl use intermedi structur end task survey use humanfac applic downstream nlp task includ event schema induct sentenc similar text comprehens learn word vector embed,2016,191
Semantic NLP-Based Information Extraction from Construction Regulatory Documents for Automated Compliance Checking,"AbstractAutomated regulatory compliance checking requires automated extraction of requirements from regulatory textual documents and their formalization in a computer-processable rule representation. Such information extraction (IE) is a challenging task that requires complex analysis and processing of text. Natural language processing (NLP) aims to enable computers to process natural language text in a human-like manner. This paper proposes a semantic, rule-based NLP approach for automated IE from construction regulatory documents. The proposed approach uses a set of pattern-matching-based IE rules and conflict resolution (CR) rules in IE. A variety of syntactic (syntax/grammar-related) and semantic (meaning/context-related) text features are used in the patterns of the IE and CR rules. Phrase structure grammar (PSG)-based phrasal tags and separation and sequencing of semantic information elements are proposed and used to reduce the number of needed patterns. An ontology is used to aid in the recognition...",abstractautom regulatori complianc check requir autom extract requir regulatori textual document formal computerprocess rule represent inform extract ie challeng task requir complex analysi process text natur languag process nlp aim enabl comput process natur languag text humanlik manner paper propos semant rulebas nlp approach autom ie construct regulatori document propos approach use set patternmatchingbas ie rule conflict resolut cr rule ie varieti syntact syntaxgrammarrel semant meaningcontextrel text featur use pattern ie cr rule phrase structur grammar psgbase phrasal tag separ sequenc semant inform element propos use reduc number need pattern ontolog use aid recognit,2016,221
Improving Information Extraction by Acquiring External Evidence with Reinforcement Learning,"Most successful information extraction systems operate with access to a large collection of documents. In this work, we explore the task of acquiring and incorporating external evidence to improve extraction accuracy in domains where the amount of training data is scarce. This process entails issuing search queries, extraction from new sources and reconciliation of extracted values, which are repeated until sufficient evidence is collected. We approach the problem using a reinforcement learning framework where our model learns to select optimal actions based on contextual information. We employ a deep Q-network, trained to optimize a reward function that reflects extraction accuracy while penalizing extra effort. Our experiments on two databases -- of shooting incidents, and food adulteration cases -- demonstrate that our system significantly outperforms traditional extractors and a competitive meta-classifier baseline.",success inform extract system oper access larg collect document work explor task acquir incorpor extern evid improv extract accuraci domain amount train data scarc process entail issu search queri extract new sourc reconcili extract valu repeat suffici evid collect approach problem use reinforc learn framework model learn select optim action base contextu inform employ deep qnetwork train optim reward function reflect extract accuraci penal extra effort experi two databas shoot incid food adulter case demonstr system significantli outperform tradit extractor competit metaclassifi baselin,2016,149
EliIE: An open-source information extraction system for clinical trial eligibility criteria,"Objective
To develop an open-source information extraction system called Eligibility Criteria Information Extraction (EliIE) for parsing and formalizing free-text clinical research eligibility criteria (EC) following Observational Medical Outcomes Partnership Common Data Model (OMOP CDM) version 5.0.


Materials and Methods
EliIE parses EC in 4 steps: (1) clinical entity and attribute recognition, (2) negation detection, (3) relation extraction, and (4) concept normalization and output structuring. Informaticians and domain experts were recruited to design an annotation guideline and generate a training corpus of annotated EC for 230 Alzheimer's clinical trials, which were represented as queries against the OMOP CDM and included 8008 entities, 3550 attributes, and 3529 relations. A sequence labeling-based method was developed for automatic entity and attribute recognition. Negation detection was supported by NegEx and a set of predefined rules. Relation extraction was achieved by a support vector machine classifier. We further performed terminology-based concept normalization and output structuring.


Results
In task-specific evaluations, the best F1 score for entity recognition was 0.79, and for relation extraction was 0.89. The accuracy of negation detection was 0.94. The overall accuracy for query formalization was 0.71 in an end-to-end evaluation.


Conclusions
This study presents EliIE, an OMOP CDM-based information extraction system for automatic structuring and formalization of free-text EC. According to our evaluation, machine learning-based EliIE outperforms existing systems and shows promise to improve.",object develop opensourc inform extract system call elig criterion inform extract elii par formal freetext clinic research elig criterion ec follow observ medic outcom partnership common data model omop cdm version materi method elii par ec step clinic entiti attribut recognit negat detect relat extract concept normal output structur informatician domain expert recruit design annot guidelin gener train corpu annot ec alzheim clinic trial repres queri omop cdm includ entiti attribut relat sequenc labelingbas method develop automat entiti attribut recognit negat detect support negex set predefin rule relat extract achiev support vector machin classifi perform terminologybas concept normal output structur result taskspecif evalu best f score entiti recognit relat extract accuraci negat detect overal accuraci queri formal endtoend evalu conclus studi present elii omop cdmbase inform extract system automat structur formal freetext ec accord evalu machin learningbas elii outperform exist system show promis improv,2017,101
Characterizing microglia activation: a spatial statistics approach to maximize information extraction,,,2017,258
Weakly supervised learning of biomedical information extraction from curated data,,,2016,209
Scientific Information Extraction with Semi-supervised Neural Tagging,"This paper addresses the problem of extracting keyphrases from scientific articles and categorizing them as corresponding to a task, process, or material. We cast the problem as sequence tagging and introduce semi-supervised methods to a neural tagging model, which builds on recent advances in named entity recognition. Since annotated training data is scarce in this domain, we introduce a graph-based semi-supervised algorithm together with a data selection scheme to leverage unannotated articles. Both inductive and transductive semi-supervised learning strategies outperform state-of-the-art information extraction performance on the 2017 SemEval Task 10 ScienceIE task.",paper address problem extract keyphras scientif articl categor correspond task process materi cast problem sequenc tag introduc semisupervis method neural tag model build recent advanc name entiti recognit sinc annot train data scarc domain introduc graphbas semisupervis algorithm togeth data select scheme leverag unannot articl induct transduct semisupervis learn strategi outperform stateoftheart inform extract perform semev task sciencei task,2017,90
Cultivated land information extraction in UAV imagery based on deep convolutional neural network and transfer learning,,,2017,77
Simple tricks for improving pattern-based information extraction from the biomedical literature,,,2010,1824
Information Extraction in Illicit Web Domains,"Extracting useful entities and attribute values from illicit domains such as human trafficking is a challenging problem with the potential for widespread social impact. Such domains employ atypical language models, have 'long tails' and suffer from the problem of concept drift. In this paper, we propose a lightweight, feature-agnostic Information Extraction (IE) paradigm specifically designed for such domains. Our approach uses raw, unlabeled text from an initial corpus, and a few (12-120) seed annotations per domain-specific attribute, to learn robust IE models for unobserved pages and websites. Empirically, we demonstrate that our approach can outperform feature-centric Conditional Random Field baselines by over 18% F-Measure on five annotated sets of real-world human trafficking datasets in both low-supervision and high-supervision settings. We also show that our approach is demonstrably robust to concept drift, and can be efficiently bootstrapped even in a serial computing environment.",extract use entiti attribut valu illicit domain human traffick challeng problem potenti widespread social impact domain employ atyp languag model long tail suffer problem concept drift paper propos lightweight featureagnost inform extract ie paradigm specif design domain approach use raw unlabel text initi corpu seed annot per domainspecif attribut learn robust ie model unobserv page websit empir demonstr approach outperform featurecentr condit random field baselin fmeasur five annot set realworld human traffick dataset lowsupervis highsupervis set also show approach demonstr robust concept drift effici bootstrap even serial comput environ,2017,35
Waterbody information extraction from remote-sensing images after disasters based on spectral information and characteristic knowledge,"ABSTRACT This article proposes a post-disaster waterbody information extraction method based on spectral information from remote-sensing images and characteristic knowledge that can resist interference from factors such as changes in water quality, waves caused by accelerated water flow, and varying water levels. The method first analyses the display characteristics of waterbodies from remote-sensing images (their spectral characteristics, geometric features, and textural features), forming a decision tree of rules that represent characteristic knowledge for waterbody information extraction. This rule set is added to the various processing stages of waterbody information extraction after disasters to construct a waterbody information extraction model. Second, an object-oriented method is used for image segmentation. A rough initial waterbody information extraction is performed based on spectral information, and then refined based on the characteristic knowledge. Third, noise is eliminated and holes are filled in the images of the refined waterbody information extraction results. Finally, the accuracy of this new waterbody information extraction method is evaluated from both qualitative and quantitative aspects. Accuracy assessments of the experimental results obtained using remote-sensing images from the Wenchuan earthquake and a 2010 flood in Pakistan show that the proposed method is both efficient and accurate at extracting post-disaster waterbody information even when the background is complex.",abstract articl propos postdisast waterbodi inform extract method base spectral inform remotesens imag characterist knowledg resist interfer factor chang water qualiti wave caus acceler water flow vari water level method first analys display characterist waterbodi remotesens imag spectral characterist geometr featur textur featur form decis tree rule repres characterist knowledg waterbodi inform extract rule set ad variou process stage waterbodi inform extract disast construct waterbodi inform extract model second objectori method use imag segment rough initi waterbodi inform extract perform base spectral inform refin base characterist knowledg third nois elimin hole fill imag refin waterbodi inform extract result final accuraci new waterbodi inform extract method evalu qualit quantit aspect accuraci assess experiment result obtain use remotesens imag wenchuan earthquak flood pakistan show propos method effici accur extract postdisast waterbodi inform even background complex,2017,37
Ontology-based automated information extraction from building energy conservation codes,,,2017,114
Snorkel: Fast Training Set Generation for Information Extraction,"State-of-the art machine learning methods such as deep learning rely on large sets of hand-labeled training data. Collecting training data is prohibitively slow and expensive, especially when technical domain expertise is required; even the largest technology companies struggle with this challenge. We address this critical bottleneck with Snorkel, a new system for quickly creating, managing, and modeling training sets. Snorkel enables users to generate large volumes of training data by writing labeling functions, which are simple functions that express heuristics and other weak supervision strategies. These user-authored labeling functions may have low accuracies and may overlap and conflict, but Snorkel automatically learns their accuracies and synthesizes their output labels. Experiments and theory show that surprisingly, by modeling the labeling process in this way, we can train high-accuracy machine learning models even using potentially lower-accuracy inputs. Snorkel is currently used in production at top technology and consulting companies, and used by researchers to extract information from electronic health records, after-action combat reports, and the scientific literature. In this demonstration, we focus on the challenging task of information extraction, a common application of Snorkel in practice. Using the task of extracting corporate employment relationships from news articles, we will demonstrate and build intuition for a radically different way of developing machine learning systems which allows us to effectively bypass the bottleneck of hand-labeling training data.",stateofth art machin learn method deep learn reli larg set handlabel train data collect train data prohibit slow expens especi technic domain expertis requir even largest technolog compani struggl challeng address critic bottleneck snorkel new system quickli creat manag model train set snorkel enabl user gener larg volum train data write label function simpl function express heurist weak supervis strategi userauthor label function may low accuraci may overlap conflict snorkel automat learn accuraci synthes output label experi theori show surprisingli model label process way train highaccuraci machin learn model even use potenti loweraccuraci input snorkel current use product top technolog consult compani use research extract inform electron health record afteract combat report scientif literatur demonstr focu challeng task inform extract common applic snorkel practic use task extract corpor employ relationship news articl demonstr build intuit radic differ way develop machin learn system allow u effect bypass bottleneck handlabel train data,2017,85
Odin’s Runes: A Rule Language for Information Extraction,"Odin is an information extraction framework that applies cascades of finite state automata over both surface text and syntactic dependency graphs. Support for syntactic patterns allow us to concisely define relations that are otherwise difficult to express in languages such as Common Pattern Specification Language (CPSL), which are currently limited to shallow linguistic features. The interaction of lexical and syntactic automata provides robustness and flexibility when writing extraction rules. This paper describes Odin’s declarative language for writing these cascaded automata.",odin inform extract framework appli cascad finit state automaton surfac text syntact depend graph support syntact pattern allow u concis defin relat otherwis difficult express languag common pattern specif languag cpsl current limit shallow linguist featur interact lexic syntact automaton provid robust flexibl write extract rule paper describ odin declar languag write cascad automaton,2016,43
Applying Information Extraction for Patent Structure Analysis,"Patent engineers are spending significant time analyzing patent claim structures to grasp the range of technology covered or to compare similar patents in the same patent family. Though patent claims are the most important section in a patent, it is hard for a human to examine them. In this paper, we propose an information-extraction-based technique to grasp the patent claim structure. We confirmed that our approach is promising through empirical evaluation of entity mention extraction and the relation extraction method. We also built a preliminary interface to visualize patent structures, compare patents, and search similar patents.",patent engin spend signific time analyz patent claim structur grasp rang technolog cover compar similar patent patent famili though patent claim import section patent hard human examin paper propos informationextractionbas techniqu grasp patent claim structur confirm approach promis empir evalu entiti mention extract relat extract method also built preliminari interfac visual patent structur compar patent search similar patent,2017,27
Learning for Biomedical Information Extraction: Methodological Review of Recent Advances,"Biomedical information extraction (BioIE) is important to many applications, including clinical decision support, integrative biology, and pharmacovigilance, and therefore it has been an active research. Unlike existing reviews covering a holistic view on BioIE, this review focuses on mainly recent advances in learning based approaches, by systematically summarizing them into different aspects of methodological development. In addition, we dive into open information extraction and deep learning, two emerging and influential techniques and envision next generation of BioIE.",biomed inform extract bioie import mani applic includ clinic decis support integr biolog pharmacovigil therefor activ research unlik exist review cover holist view bioie review focus mainli recent advanc learn base approach systemat summar differ aspect methodolog develop addit dive open inform extract deep learn two emerg influenti techniqu envis next gener bioie,2016,60
Overview of ImageCLEF 2017: Information Extraction from Images,,,2017,70
A hybrid ontology-based information extraction system,"Information Extraction is the process of automatically obtaining knowledge from plain text. Because of the ambiguity of written natural language, Information Extraction is a difficult task. Ontology-based Information Extraction (OBIE) reduces this complexity by including contextual information in the form of a domain ontology. The ontology provides guidance to the extraction process by providing concepts and relationships about the domain. However, OBIE systems have not been widely adopted because of the difficulties in deployment and maintenance. The Ontology-based Components for Information Extraction (OBCIE) architecture has been proposed as a form to encourage the adoption of OBIE by promoting reusability through modularity. In this paper, we propose two orthogonal extensions to OBCIE that allow the construction of hybrid OBIE systems with higher extraction accuracy and a new functionality. The first extension utilizes OBCIE modularity to integrate different types of implementation into one extraction system, producing a more accurate extraction. For each concept or relationship in the ontology, we can select the best implementation for extraction, or we can combine both implementations under an ensemble learning schema. The second extension is a novel ontology-based error detection mechanism. Following a heuristic approach, we can identify sentences that are logically inconsistent with the domain ontology. Because the implementation strategy for the extraction of a concept is independent of the functionality of the extraction, we can design a hybrid OBIE system with concepts utilizing different implementation strategies for extracting correct or incorrect sentences. Our evaluation shows that, in the implementation extension, our proposed method is more accurate in terms of correctness and completeness of the extraction. Moreover, our error detection method can identify incorrect statements with a high accuracy.",inform extract process automat obtain knowledg plain text ambigu written natur languag inform extract difficult task ontologybas inform extract obi reduc complex includ contextu inform form domain ontolog ontolog provid guidanc extract process provid concept relationship domain howev obi system wide adopt difficulti deploy mainten ontologybas compon inform extract obci architectur propos form encourag adopt obi promot reusabl modular paper propos two orthogon extens obci allow construct hybrid obi system higher extract accuraci new function first extens util obci modular integr differ type implement one extract system produc accur extract concept relationship ontolog select best implement extract combin implement ensembl learn schema second extens novel ontologybas error detect mechan follow heurist approach identifi sentenc logic inconsist domain ontolog implement strategi extract concept independ function extract design hybrid obi system concept util differ implement strategi extract correct incorrect sentenc evalu show implement extens propos method accur term correct complet extract moreov error detect method identifi incorrect statement high accuraci,2016,29
Information Extraction,,,2019,0
Information extraction from multi-institutional radiology reports,,,2016,93
Nested Propositions in Open Information Extraction,",",,2016,57
FEVEROUS: Fact Extraction and VERification Over Unstructured and Structured information,"Fact verification has attracted a lot of attention in the machine learning and natural language processing communities, as it is one of the key methods for detecting misinformation. Existing large-scale benchmarks for this task have focused mostly on textual sources, i.e. unstructured information, and thus ignored the wealth of information available in structured formats, such as tables. In this paper we introduce a novel dataset and benchmark, Fact Extraction and VERification Over Unstructured and Structured information (FEVEROUS), which consists of 87,026 verified claims. Each claim is annotated with evidence in the form of sentences and/or cells from tables in Wikipedia, as well as a label indicating whether this evidence supports, refutes, or does not provide enough information to reach a verdict. Furthermore, we detail our efforts to track and minimize the biases present in the dataset and could be exploited by models, e.g. being able to predict the label without using evidence. Finally, we develop a baseline for verifying claims against text and tables which predicts both the correct evidence and verdict for 18% of the claims.",fact verif attract lot attent machin learn natur languag process commun one key method detect misinform exist largescal benchmark task focus mostli textual sourc ie unstructur inform thu ignor wealth inform avail structur format tabl paper introduc novel dataset benchmark fact extract verif unstructur structur inform fever consist verifi claim claim annot evid form sentenc andor cell tabl wikipedia well label indic whether evid support refut provid enough inform reach verdict furthermor detail effort track minim bias present dataset could exploit model eg abl predict label without use evid final develop baselin verifi claim text tabl predict correct evid verdict claim,2021,160
Maximum Entropy Markov Models for Information Extraction and Segmentation,"Hidden Markov models (HMMs) are a powerful probabilistic tool for modeling sequential data, and have been applied with success to many text-related tasks, such as part-of-speech tagging, text segmentation and information extraction. In these cases, the observations are usually modeled as multinomial distributions over a discrete vocabulary, and the HMM parameters are set to maximize the likelihood of the observations. This paper presents a new Markovian sequence model, closely related to HMMs, that allows observations to be represented as arbitrary overlapping features (such as word, capitalization, formatting, part-of-speech), and defines the conditional probability of state sequences given observation sequences. It does this by using the maximum entropy framework to fit a set of exponential models that represent the probability of a state given an observation and the previous state. We present positive experimental results on the segmentation of FAQ’s.",hidden markov model hmm power probabilist tool model sequenti data appli success mani textrel task partofspeech tag text segment inform extract case observ usual model multinomi distribut discret vocabulari hmm paramet set maxim likelihood observ paper present new markovian sequenc model close relat hmm allow observ repres arbitrari overlap featur word capit format partofspeech defin condit probabl state sequenc given observ sequenc use maximum entropi framework fit set exponenti model repres probabl state given observ previou state present posit experiment result segment faq,2000,1615
Information Extraction Under Privacy Constraints,"A privacy-constrained information extraction problem is considered where for a pair of correlated discrete random variables $(X,Y)$ governed by a given joint distribution, an agent observes $Y$ and wants to convey to a potentially public user as much information about $Y$ as possible without compromising the amount of information revealed about $X$. To this end, the so-called {\em rate-privacy function} is introduced to quantify the maximal amount of information (measured in terms of mutual information) that can be extracted from $Y$ under a privacy constraint between $X$ and the extracted information, where privacy is measured using either mutual information or maximal correlation. Properties of the rate-privacy function are analyzed and information-theoretic and estimation-theoretic interpretations of it are presented for both the mutual information and maximal correlation privacy measures. It is also shown that the rate-privacy function admits a closed-form expression for a large family of joint distributions of $(X,Y)$. Finally, the rate-privacy function under the mutual information privacy measure is considered for the case where $(X,Y)$ has a joint probability density function by studying the problem where the extracted information is a uniform quantization of $Y$ corrupted by additive Gaussian noise. The asymptotic behavior of the rate-privacy function is studied as the quantization resolution grows without bound and it is observed that not all of the properties of the rate-privacy function carry over from the discrete to the continuous case.",privacyconstrain inform extract problem consid pair correl discret random variabl xy govern given joint distribut agent observ want convey potenti public user much inform possibl without compromis amount inform reveal x end socal em rateprivaci function introduc quantifi maxim amount inform measur term mutual inform extract privaci constraint x extract inform privaci measur use either mutual inform maxim correl properti rateprivaci function analyz informationtheoret estimationtheoret interpret present mutual inform maxim correl privaci measur also shown rateprivaci function admit closedform express larg famili joint distribut xy final rateprivaci function mutual inform privaci measur consid case xy joint probabl densiti function studi problem extract inform uniform quantiz corrupt addit gaussian nois asymptot behavior rateprivaci function studi quantiz resolut grow without bound observ properti rateprivaci function carri discret continu case,2015,88
Information Extraction with RapidMiner,"In this paper we present the Information Extraction (IE)- plugin for the open source Data Mining (DM) software RapidMiner 1 (Mierswa et al., 2006). The IE-plugin can be seen as an interface between natural language and IE- or DM-methods, because it converts docu- ments containing natural language texts into machine-readable form in order to extract interesting information like special entities and relations between those. The plugin is very modular and easy to use, which makes it more applicable for dierent domains and tasks.",paper present inform extract ie plugin open sourc data mine dm softwar rapidmin mierswa et al ieplugin seen interfac natur languag ie dmmethod convert docu ment contain natur languag text machineread form order extract interest inform like special entiti relat plugin modular easi use make applic dierent domain task,2015,48
"Benchmarking Clinical Speech Recognition and Information Extraction: New Data, Methods, and Evaluations","Background Over a tenth of preventable adverse events in health care are caused by failures in information flow. These failures are tangible in clinical handover; regardless of good verbal handover, from two-thirds to all of this information is lost after 3-5 shifts if notes are taken by hand, or not at all. Speech recognition and information extraction provide a way to fill out a handover form for clinical proofing and sign-off. Objective The objective of the study was to provide a recorded spoken handover, annotated verbatim transcriptions, and evaluations to support research in spoken and written natural language processing for filling out a clinical handover form. This dataset is based on synthetic patient profiles, thereby avoiding ethical and legal restrictions, while maintaining efficacy for research in speech-to-text conversion and information extraction, based on realistic clinical scenarios. We also introduce a Web app to demonstrate the system design and workflow. Methods We experiment with Dragon Medical 11.0 for speech recognition and CRF++ for information extraction. To compute features for information extraction, we also apply CoreNLP, MetaMap, and Ontoserver. Our evaluation uses cross-validation techniques to measure processing correctness. Results The data provided were a simulation of nursing handover, as recorded using a mobile device, built from simulated patient records and handover scripts, spoken by an Australian registered nurse. Speech recognition recognized 5276 of 7277 words in our 100 test documents correctly. We considered 50 mutually exclusive categories in information extraction and achieved the F1 (ie, the harmonic mean of Precision and Recall) of 0.86 in the category for irrelevant text and the macro-averaged F1 of 0.70 over the remaining 35 nonempty categories of the form in our 101 test documents. Conclusions The significance of this study hinges on opening our data, together with the related performance benchmarks and some processing software, to the research and development community for studying clinical documentation and language-processing. The data are used in the CLEFeHealth 2015 evaluation laboratory for a shared task on speech recognition.",background tenth prevent advers event health care caus failur inform flow failur tangibl clinic handov regardless good verbal handov twothird inform lost shift note taken hand speech recognit inform extract provid way fill handov form clinic proof signoff object object studi provid record spoken handov annot verbatim transcript evalu support research spoken written natur languag process fill clinic handov form dataset base synthet patient profil therebi avoid ethic legal restrict maintain efficaci research speechtotext convers inform extract base realist clinic scenario also introduc web app demonstr system design workflow method experi dragon medic speech recognit crf inform extract comput featur inform extract also appli corenlp metamap ontoserv evalu use crossvalid techniqu measur process correct result data provid simul nurs handov record use mobil devic built simul patient record handov script spoken australian regist nurs speech recognit recogn word test document correctli consid mutual exclus categori inform extract achiev f ie harmon mean precis recal categori irrelev text macroaverag f remain nonempti categori form test document conclus signific studi hing open data togeth relat perform benchmark process softwar research develop commun studi clinic document languageprocess data use clefehealth evalu laboratori share task speech recognit,2015,59
Multilingual Open Information Extraction,,,2015,53
Automated Road Information Extraction From Mobile Laser Scanning Data,"This paper presents a survey of literature about road feature extraction, giving a detailed description of a Mobile Laser Scanning (MLS) system (RIEGL VMX-450) for transportation-related applications. This paper describes the development of automated algorithms for extracting road features (road surfaces, road markings, and pavement cracks) from MLS point cloud data. The proposed road surface extraction algorithm detects road curbs from a set of profiles that are sliced along vehicle trajectory data. Based on segmented road surface points, we create Geo-Referenced Feature (GRF) images and develop two algorithms, respectively, for extracting the following: 1) road markings with high retroreflectivity and 2) cracks containing low contrast with their surroundings, low signal-to-noise ratio, and poor continuity. A comprehensive comparison illustrates satisfactory performance of the proposed algorithms and concludes that MLS is a reliable and cost-effective alternative for rapid road inspection.",paper present survey literatur road featur extract give detail descript mobil laser scan ml system riegl vmx transportationrel applic paper describ develop autom algorithm extract road featur road surfac road mark pavement crack ml point cloud data propos road surfac extract algorithm detect road curb set profil slice along vehicl trajectori data base segment road surfac point creat georeferenc featur grf imag develop two algorithm respect extract follow road mark high retroreflect crack contain low contrast surround low signaltonois ratio poor continu comprehens comparison illustr satisfactori perform propos algorithm conclud ml reliabl costeffect altern rapid road inspect,2015,126
Information Extraction,"Much of the world's knowledge is recorded in natural language text, but making effective use of it in this form poses a major challenge. Information extraction converts this knowledge to a structured form suitable for computer manipulation, opening up many possibilities for using it. In this review, the author describes the processing pipeline of information extraction, how the pipeline components are trained, and how this training can be made more efficient. He also describes some of the challenges that must be addressed for information extraction to become a more widely used technology.",much world knowledg record natur languag text make effect use form pose major challeng inform extract convert knowledg structur form suitabl comput manipul open mani possibl use review author describ process pipelin inform extract pipelin compon train train made effici also describ challeng must address inform extract becom wide use technolog,2015,24
Open Language Learning for Information Extraction,"Open Information Extraction (IE) systems extract relational tuples from text, without requiring a pre-specified vocabulary, by identifying relation phrases and associated arguments in arbitrary sentences. However, state-of-the-art Open IE systems such as ReVerb and woe share two important weaknesses -- (1) they extract only relations that are mediated by verbs, and (2) they ignore context, thus extracting tuples that are not asserted as factual. This paper presents ollie, a substantially improved Open IE system that addresses both these limitations. First, ollie achieves high yield by extracting relations mediated by nouns, adjectives, and more. Second, a context-analysis step increases precision by including contextual information from the sentence in the extractions. ollie obtains 2.7 times the area under precision-yield curve (AUC) compared to ReVerb and 1.9 times the AUC of woeparse.",open inform extract ie system extract relat tupl text without requir prespecifi vocabulari identifi relat phrase associ argument arbitrari sentenc howev stateoftheart open ie system reverb woe share two import weak extract relat mediat verb ignor context thu extract tupl assert factual paper present olli substanti improv open ie system address limit first olli achiev high yield extract relat mediat noun adject second contextanalysi step increas precis includ contextu inform sentenc extract olli obtain time area precisionyield curv auc compar reverb time auc woepars,2012,831
Information Extraction over Structured Data: Question Answering with Freebase,"Answering natural language questions using the Freebase knowledge base has recently been explored as a platform for advancing the state of the art in open domain semantic parsing. Those efforts map questions to sophisticated meaning representations that are then attempted to be matched against viable answer candidates in the knowledge base. Here we show that relatively modest information extraction techniques, when paired with a webscale corpus, can outperform these sophisticated approaches by roughly 34% relative gain.",answer natur languag question use freebas knowledg base recent explor platform advanc state art open domain semant par effort map question sophist mean represent attempt match viabl answer candid knowledg base show rel modest inform extract techniqu pair webscal corpu outperform sophist approach roughli rel gain,2014,463
YOLOv9: Learning What You Want to Learn Using Programmable Gradient Information,"Today's deep learning methods focus on how to design the most appropriate objective functions so that the prediction results of the model can be closest to the ground truth. Meanwhile, an appropriate architecture that can facilitate acquisition of enough information for prediction has to be designed. Existing methods ignore a fact that when input data undergoes layer-by-layer feature extraction and spatial transformation, large amount of information will be lost. This paper will delve into the important issues of data loss when data is transmitted through deep networks, namely information bottleneck and reversible functions. We proposed the concept of programmable gradient information (PGI) to cope with the various changes required by deep networks to achieve multiple objectives. PGI can provide complete input information for the target task to calculate objective function, so that reliable gradient information can be obtained to update network weights. In addition, a new lightweight network architecture -- Generalized Efficient Layer Aggregation Network (GELAN), based on gradient path planning is designed. GELAN's architecture confirms that PGI has gained superior results on lightweight models. We verified the proposed GELAN and PGI on MS COCO dataset based object detection. The results show that GELAN only uses conventional convolution operators to achieve better parameter utilization than the state-of-the-art methods developed based on depth-wise convolution. PGI can be used for variety of models from lightweight to large. It can be used to obtain complete information, so that train-from-scratch models can achieve better results than state-of-the-art models pre-trained using large datasets, the comparison results are shown in Figure 1. The source codes are at: https://github.com/WongKinYiu/yolov9.",today deep learn method focu design appropri object function predict result model closest ground truth meanwhil appropri architectur facilit acquisit enough inform predict design exist method ignor fact input data undergo layerbylay featur extract spatial transform larg amount inform lost paper delv import issu data loss data transmit deep network name inform bottleneck revers function propos concept programm gradient inform pgi cope variou chang requir deep network achiev multipl object pgi provid complet input inform target task calcul object function reliabl gradient inform obtain updat network weight addit new lightweight network architectur gener effici layer aggreg network gelan base gradient path plan design gelan architectur confirm pgi gain superior result lightweight model verifi propos gelan pgi m coco dataset base object detect result show gelan use convent convolut oper achiev better paramet util stateoftheart method develop base depthwis convolut pgi use varieti model lightweight larg use obtain complet inform trainfromscratch model achiev better result stateoftheart model pretrain use larg dataset comparison result shown figur sourc code httpsgithubcomwongkinyiuyolov,2024,667
Multiscale Feature Extraction and Fusion of Image and Text in VQA,,,2023,174
Knowledge-Based Weak Supervision for Information Extraction of Overlapping Relations,"Information extraction (IE) holds the promise of generating a large-scale knowledge base from the Web's natural language text. Knowledge-based weak supervision, using structured data to heuristically label a training corpus, works towards this goal by enabling the automated learning of a potentially unbounded number of relation extractors. Recently, researchers have developed multi-instance learning algorithms to combat the noisy training data that can come from heuristic labeling, but their models assume relations are disjoint --- for example they cannot extract the pair Founded(Jobs, Apple) and CEO-of(Jobs, Apple). 
 
This paper presents a novel approach for multi-instance learning with overlapping relations that combines a sentence-level extraction model with a simple, corpus-level component for aggregating the individual facts. We apply our model to learn extractors for NY Times text using weak supervision from Free-base. Experiments show that the approach runs quickly and yields surprising gains in accuracy, at both the aggregate and sentence level.",inform extract ie hold promis gener largescal knowledg base web natur languag text knowledgebas weak supervis use structur data heurist label train corpu work toward goal enabl autom learn potenti unbound number relat extractor recent research develop multiinst learn algorithm combat noisi train data come heurist label model assum relat disjoint exampl cannot extract pair foundedjob appl ceoofjob appl paper present novel approach multiinst learn overlap relat combin sentencelevel extract model simpl corpuslevel compon aggreg individu fact appli model learn extractor ny time text use weak supervis freebas experi show approach run quickli yield surpris gain accuraci aggreg sentenc level,2011,1021
Information Extraction,,,1997,1681
Hypothetical Thinking and Information Extraction in the Laboratory,"In several common-value environments (e.g., auctions or elections), players should make informational inferences from opponents' strategies under certain hypothetical events (e.g., winning the auction or being pivotal). We design a voting experiment that identifies whether subjects make these inferences and distinguishes between hypothetical thinking and information extraction. Depending on feedback, between 50 and 80 percent of subjects behave non-optimally. More importantly, these mistakes are driven by difficulty in extracting information from hypothetical, but not from actual, events. Mistakes are robust to experience and hints, and also arise in more general settings where players have no private information.",sever commonvalu environ eg auction elect player make inform infer oppon strategi certain hypothet event eg win auction pivot design vote experi identifi whether subject make infer distinguish hypothet think inform extract depend feedback percent subject behav nonoptim importantli mistak driven difficulti extract inform hypothet actual event mistak robust experi hint also aris gener set player privat inform,2014,165
